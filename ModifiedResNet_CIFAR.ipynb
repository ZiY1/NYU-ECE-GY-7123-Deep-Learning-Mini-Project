{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WASVRCayqsq5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrNe63dCiYdd"
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "To build the Modified ResNet, first pick type of Residual Blocks in each layer and the shortcut connection type.\n",
        "\n",
        "## Blocks\n",
        "\n",
        "BasicBlock class is defined as in the original ResNet Paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GS_f4EChbwZ"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # First Convolutional Layer:\n",
        "        # Kernel(Filter) Size: 3 x 3\n",
        "        # Stride: 1\n",
        "        # Padding: 1 (keep the output size equal to the input size if stride is 1)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        # Batch Normalization\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # Second Convolutional Layer:\n",
        "        # Kernel Size: 3 x 3\n",
        "        # Stride: 1\n",
        "        # Padding: 1 (keep the output size equal to the input size if stride is 1)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        # Batch Normalization\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # When the input and output dimensions do not match up,\n",
        "        # use 1x1 convolutional layer to adjust the number of channels\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.MaxPool2d(3, stride = stride, padding = 1),\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "                # nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                #           kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        # print(\"shape after first weight layer:\", out.shape)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        # print(\"shape after second weight layer:\", out.shape)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PreActBlock class is a known variation of the original Residual Block, which moves the ReLu layer and Batch normalization layer from after the shortcut addition to the very beginning of the block"
      ],
      "metadata": {
        "id": "bb9QfAB2SNPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YPp5Mww93dn"
      },
      "outputs": [],
      "source": [
        "class PreActBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    # Full Pre-activation:\n",
        "    # BatchNorm -> Relu -> Conv -> BatchNorm -> Relu -> Conv\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBlock, self).__init__()\n",
        "\n",
        "        # First Batch Normalization\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        # First Convolutional Layer:\n",
        "        # Kernel(Filter) Size: 3 x 3\n",
        "        # Stride: 1\n",
        "        # Padding: 1 (keep the output size equal to the input size if stride is 1)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "        # Second Batch Normalization\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        # Second Convolutional Layer:\n",
        "        # Kernel Size: 3 x 3\n",
        "        # Stride: 1\n",
        "        # Padding: 1 (keep the output size equal to the input size if stride is 1)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "\n",
        "\n",
        "        # When the input and output dimensions do not match up,\n",
        "        # use 1x1 convolutional layer to adjust the number of channels\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.MaxPool2d(3, stride = stride, padding = 1),\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "                # nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                #           kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        # bn-> relu -> conv\n",
        "        out = F.relu(self.bn1(x))\n",
        "        out = self.conv1(out)\n",
        "        # print(\"shape after first weight layer:\", out.shape)\n",
        "        out = F.relu(self.bn2(out))\n",
        "        out = self.conv2(out)\n",
        "        # print(\"shape after second weight layer:\", out.shape)\n",
        "        out += self.shortcut(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shortcut connection\n",
        "Between the different Residual Layers of ResNet, a Convolution layer of kernel 1x1 and stride 2 was used in the original paper to accomodate the number of different channels between layers. This down-sampling leads to unused information, so a way to avoid this is to replace the 1x1 convolution layer with a Max Pool of 3x3 and stride 2, following by a 1x1 convolution layer of stride 1.\n",
        "\n",
        "To use this modified shortcut, uncomment the following lines where ```self.shortcut``` is defined in both the BasicBlock and the PreActBlock,\n",
        "```\n",
        "                # nn.MaxPool2d(3, stride = stride, padding = 1),\n",
        "                # nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                #           kernel_size=1, stride=1, bias=False),\n",
        "```\n",
        "and comment out the following lines.\n",
        "```\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "```"
      ],
      "metadata": {
        "id": "jSee5Oi2SU0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test out the blocks."
      ],
      "metadata": {
        "id": "CRQ5uGEeTNuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt8WhQV2yIEO",
        "outputId": "fda91897-a0f8-4a68-bb28-6a32b452b6b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 64, 32, 32])\n",
            "torch.Size([1, 128, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "NUM_CHANNELS_IN = 64\n",
        "NUM_CHANNELS_OUT = 128\n",
        "test_image = torch.randn(1, NUM_CHANNELS_IN, 32, 32)\n",
        "print(test_image.shape)\n",
        "\n",
        "building_block = PreActBlock(in_planes=NUM_CHANNELS_IN, planes=NUM_CHANNELS_OUT)\n",
        "y = building_block(test_image)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input Layer Convolution\n",
        "\n",
        "Given the small size of the images in CIFAR10 dataset, it is possible that we can pass all pixels directly to the residual layers after batch normalization (assuming that each pixel itself is important, and the relative relationship to surrounding pixels can be learned from other convolution layers later on). Therefore, we can change the first 3x3 convolution layer to a 1x1 convolution layer to achieve this.\n",
        "\n",
        "Turn input convolution off: ```kernel_size=1```,```padding=0``` when defining ```self.conv1```.\n",
        "\n",
        "Turn input convolution on: ```kernel_size=3```,```padding=1``` when defining ```self.conv1```."
      ],
      "metadata": {
        "id": "tZG3fmNlSfdN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoVShXYJhsh0"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 32\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=1,\n",
        "                               stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(256*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puK_KG7ehzF4"
      },
      "outputs": [],
      "source": [
        "def ModifiedResNet():\n",
        "    return ResNet(PreActBlock, [8, 6, 4, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTFxvEhxwHkg",
        "outputId": "cc8e7130-fa8e-4d1b-935c-2fc9c47102d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [1, 32, 32, 32]              96\n",
            "       BatchNorm2d-2            [1, 32, 32, 32]              64\n",
            "       BatchNorm2d-3            [1, 32, 32, 32]              64\n",
            "            Conv2d-4            [1, 32, 32, 32]           9,216\n",
            "       BatchNorm2d-5            [1, 32, 32, 32]              64\n",
            "            Conv2d-6            [1, 32, 32, 32]           9,216\n",
            "       PreActBlock-7            [1, 32, 32, 32]               0\n",
            "       BatchNorm2d-8            [1, 32, 32, 32]              64\n",
            "            Conv2d-9            [1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-10            [1, 32, 32, 32]              64\n",
            "           Conv2d-11            [1, 32, 32, 32]           9,216\n",
            "      PreActBlock-12            [1, 32, 32, 32]               0\n",
            "      BatchNorm2d-13            [1, 32, 32, 32]              64\n",
            "           Conv2d-14            [1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-15            [1, 32, 32, 32]              64\n",
            "           Conv2d-16            [1, 32, 32, 32]           9,216\n",
            "      PreActBlock-17            [1, 32, 32, 32]               0\n",
            "      BatchNorm2d-18            [1, 32, 32, 32]              64\n",
            "           Conv2d-19            [1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-20            [1, 32, 32, 32]              64\n",
            "           Conv2d-21            [1, 32, 32, 32]           9,216\n",
            "      PreActBlock-22            [1, 32, 32, 32]               0\n",
            "      BatchNorm2d-23            [1, 32, 32, 32]              64\n",
            "           Conv2d-24            [1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-25            [1, 32, 32, 32]              64\n",
            "           Conv2d-26            [1, 32, 32, 32]           9,216\n",
            "      PreActBlock-27            [1, 32, 32, 32]               0\n",
            "      BatchNorm2d-28            [1, 32, 32, 32]              64\n",
            "           Conv2d-29            [1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-30            [1, 32, 32, 32]              64\n",
            "           Conv2d-31            [1, 32, 32, 32]           9,216\n",
            "      PreActBlock-32            [1, 32, 32, 32]               0\n",
            "      BatchNorm2d-33            [1, 32, 32, 32]              64\n",
            "           Conv2d-34            [1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-35            [1, 32, 32, 32]              64\n",
            "           Conv2d-36            [1, 32, 32, 32]           9,216\n",
            "      PreActBlock-37            [1, 32, 32, 32]               0\n",
            "      BatchNorm2d-38            [1, 32, 32, 32]              64\n",
            "           Conv2d-39            [1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-40            [1, 32, 32, 32]              64\n",
            "           Conv2d-41            [1, 32, 32, 32]           9,216\n",
            "      PreActBlock-42            [1, 32, 32, 32]               0\n",
            "      BatchNorm2d-43            [1, 32, 32, 32]              64\n",
            "           Conv2d-44            [1, 64, 16, 16]          18,432\n",
            "      BatchNorm2d-45            [1, 64, 16, 16]             128\n",
            "           Conv2d-46            [1, 64, 16, 16]          36,864\n",
            "        MaxPool2d-47            [1, 32, 16, 16]               0\n",
            "           Conv2d-48            [1, 64, 16, 16]           2,048\n",
            "      BatchNorm2d-49            [1, 64, 16, 16]             128\n",
            "      PreActBlock-50            [1, 64, 16, 16]               0\n",
            "      BatchNorm2d-51            [1, 64, 16, 16]             128\n",
            "           Conv2d-52            [1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-53            [1, 64, 16, 16]             128\n",
            "           Conv2d-54            [1, 64, 16, 16]          36,864\n",
            "      PreActBlock-55            [1, 64, 16, 16]               0\n",
            "      BatchNorm2d-56            [1, 64, 16, 16]             128\n",
            "           Conv2d-57            [1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-58            [1, 64, 16, 16]             128\n",
            "           Conv2d-59            [1, 64, 16, 16]          36,864\n",
            "      PreActBlock-60            [1, 64, 16, 16]               0\n",
            "      BatchNorm2d-61            [1, 64, 16, 16]             128\n",
            "           Conv2d-62            [1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-63            [1, 64, 16, 16]             128\n",
            "           Conv2d-64            [1, 64, 16, 16]          36,864\n",
            "      PreActBlock-65            [1, 64, 16, 16]               0\n",
            "      BatchNorm2d-66            [1, 64, 16, 16]             128\n",
            "           Conv2d-67            [1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-68            [1, 64, 16, 16]             128\n",
            "           Conv2d-69            [1, 64, 16, 16]          36,864\n",
            "      PreActBlock-70            [1, 64, 16, 16]               0\n",
            "      BatchNorm2d-71            [1, 64, 16, 16]             128\n",
            "           Conv2d-72            [1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-73            [1, 64, 16, 16]             128\n",
            "           Conv2d-74            [1, 64, 16, 16]          36,864\n",
            "      PreActBlock-75            [1, 64, 16, 16]               0\n",
            "      BatchNorm2d-76            [1, 64, 16, 16]             128\n",
            "           Conv2d-77             [1, 128, 8, 8]          73,728\n",
            "      BatchNorm2d-78             [1, 128, 8, 8]             256\n",
            "           Conv2d-79             [1, 128, 8, 8]         147,456\n",
            "        MaxPool2d-80              [1, 64, 8, 8]               0\n",
            "           Conv2d-81             [1, 128, 8, 8]           8,192\n",
            "      BatchNorm2d-82             [1, 128, 8, 8]             256\n",
            "      PreActBlock-83             [1, 128, 8, 8]               0\n",
            "      BatchNorm2d-84             [1, 128, 8, 8]             256\n",
            "           Conv2d-85             [1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-86             [1, 128, 8, 8]             256\n",
            "           Conv2d-87             [1, 128, 8, 8]         147,456\n",
            "      PreActBlock-88             [1, 128, 8, 8]               0\n",
            "      BatchNorm2d-89             [1, 128, 8, 8]             256\n",
            "           Conv2d-90             [1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-91             [1, 128, 8, 8]             256\n",
            "           Conv2d-92             [1, 128, 8, 8]         147,456\n",
            "      PreActBlock-93             [1, 128, 8, 8]               0\n",
            "      BatchNorm2d-94             [1, 128, 8, 8]             256\n",
            "           Conv2d-95             [1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-96             [1, 128, 8, 8]             256\n",
            "           Conv2d-97             [1, 128, 8, 8]         147,456\n",
            "      PreActBlock-98             [1, 128, 8, 8]               0\n",
            "      BatchNorm2d-99             [1, 128, 8, 8]             256\n",
            "          Conv2d-100             [1, 256, 4, 4]         294,912\n",
            "     BatchNorm2d-101             [1, 256, 4, 4]             512\n",
            "          Conv2d-102             [1, 256, 4, 4]         589,824\n",
            "       MaxPool2d-103             [1, 128, 4, 4]               0\n",
            "          Conv2d-104             [1, 256, 4, 4]          32,768\n",
            "     BatchNorm2d-105             [1, 256, 4, 4]             512\n",
            "     PreActBlock-106             [1, 256, 4, 4]               0\n",
            "     BatchNorm2d-107             [1, 256, 4, 4]             512\n",
            "          Conv2d-108             [1, 256, 4, 4]         589,824\n",
            "     BatchNorm2d-109             [1, 256, 4, 4]             512\n",
            "          Conv2d-110             [1, 256, 4, 4]         589,824\n",
            "     PreActBlock-111             [1, 256, 4, 4]               0\n",
            "     BatchNorm2d-112             [1, 256, 4, 4]             512\n",
            "          Conv2d-113             [1, 256, 4, 4]         589,824\n",
            "     BatchNorm2d-114             [1, 256, 4, 4]             512\n",
            "          Conv2d-115             [1, 256, 4, 4]         589,824\n",
            "     PreActBlock-116             [1, 256, 4, 4]               0\n",
            "          Linear-117                    [1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 4,975,210\n",
            "Trainable params: 4,975,210\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 16.73\n",
            "Params size (MB): 18.98\n",
            "Estimated Total Size (MB): 35.73\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "summary(ModifiedResNet().cuda(),(3,32,32),1,device=\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out if the network definition works by passing a [1,3,32,32] tensor to it."
      ],
      "metadata": {
        "id": "JxRavvVIUKa6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmD9ICZuhzHh"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    net = ModifiedResNet()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1Wje8s2h2vc",
        "outputId": "20ac02c8-08b2-4690-b704-fcc30298f9e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGXrcj3Kidlv"
      },
      "source": [
        "# Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAxF8iMCoCA7"
      },
      "outputs": [],
      "source": [
        "# Define a function to check if we are in a Jupyter notebook\n",
        "def is_notebook():\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        if 'IPKernelApp' not in get_ipython().config:  # pragma: no cover\n",
        "            return False\n",
        "    except Exception:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Define the parser\n",
        "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
        "\n",
        "# Modify args if in a Jupyter notebook\n",
        "if is_notebook():\n",
        "    args = parser.parse_args(args=[])\n",
        "else:\n",
        "    args = parser.parse_args()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If need to resume training from the saved checkpoint, uncomment the following code."
      ],
      "metadata": {
        "id": "rYur9rYRUZwB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcM5xAE7RrzL"
      },
      "outputs": [],
      "source": [
        "# args.resume = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiD8YNcnoCCL"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n",
        "In addition to the data augmentation methods used in the https://github.com/kuangliu/pytorch-cifar code, we have also tried out adding RandomPerspective() and ColorJitter() to encourage generalization. To apply these these augmentation, uncomment the two lines in the following code:\n",
        "```\n",
        "    # transforms.RandomPerspective(0.3),\n",
        "    # transforms.ColorJitter(brightness=0.5,hue=0.3),\n",
        "```"
      ],
      "metadata": {
        "id": "MOoK_ZRiUkfl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RlhCoUuoUuz",
        "outputId": "bf984f69-bee2-4251-9e94-b26091769e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomPerspective(0.3),\n",
        "    # transforms.ColorJitter(brightness=0.5,hue=0.3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=1)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=1)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "Three optimizer where tested in our experiments:\n",
        "1. SGD with no momentum (\"vanilla SGD\") with learning rate 0.1 and weight decay $5×10^{-4}$,\n",
        "2. Same SGD but with 0.9 momentum,\n",
        "3. Adam with learning rate 0.001 and weight decay $5×10^{-4}$\n",
        "\n",
        "Uncomment and comment the code section as needed."
      ],
      "metadata": {
        "id": "IKSPBMecUs5p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OOx_0FDoZy9",
        "outputId": "4a38b535-4ce1-4638-d89e-34f9b165b4f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Building model..\n",
            "==> Resuming from checkpoint..\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "print('==> Building model..')\n",
        "\n",
        "net = ModifiedResNet()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=args.lr,\n",
        "                      momentum=0.9,\n",
        "                      weight_decay=5e-4)\n",
        "# optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, last_epoch=-1, T_max=200)\n",
        "if args.resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOxHjnDsol4p"
      },
      "outputs": [],
      "source": [
        "# Training while recording histogram\n",
        "def train(epoch,acc_hist):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(batch_idx, len(trainloader), 'Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
        "                      % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    acc = 100.*correct/total\n",
        "    acc_hist.append(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWxsgNTRoouS"
      },
      "outputs": [],
      "source": [
        "# Testing while keep the histogram\n",
        "def test(epoch,acc_hist):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(batch_idx, len(testloader), 'Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
        "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    acc_hist.append(acc)\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "            'optimizer':optimizer.state_dict(),\n",
        "            'scheduler':scheduler.state_dict(),\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxArZ1dWRrzO"
      },
      "outputs": [],
      "source": [
        "train_acc_hist=[]\n",
        "test_acc_hist=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1szoRKbBowk6",
        "outputId": "6fe57755-0a19-40d6-ff86-a3a2670a35d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 97\n",
            "0 391 Loss: 0.167 | Train Acc: 95.312% (122/128)\n",
            "10 391 Loss: 0.220 | Train Acc: 92.259% (1299/1408)\n",
            "20 391 Loss: 0.219 | Train Acc: 92.485% (2486/2688)\n",
            "30 391 Loss: 0.206 | Train Acc: 93.070% (3693/3968)\n",
            "40 391 Loss: 0.204 | Train Acc: 93.178% (4890/5248)\n",
            "50 391 Loss: 0.203 | Train Acc: 93.275% (6089/6528)\n",
            "60 391 Loss: 0.197 | Train Acc: 93.430% (7295/7808)\n",
            "70 391 Loss: 0.197 | Train Acc: 93.299% (8479/9088)\n",
            "80 391 Loss: 0.195 | Train Acc: 93.374% (9681/10368)\n",
            "90 391 Loss: 0.196 | Train Acc: 93.441% (10884/11648)\n",
            "100 391 Loss: 0.197 | Train Acc: 93.348% (12068/12928)\n",
            "110 391 Loss: 0.197 | Train Acc: 93.250% (13249/14208)\n",
            "120 391 Loss: 0.195 | Train Acc: 93.317% (14453/15488)\n",
            "130 391 Loss: 0.195 | Train Acc: 93.279% (15641/16768)\n",
            "140 391 Loss: 0.197 | Train Acc: 93.201% (16821/18048)\n",
            "150 391 Loss: 0.199 | Train Acc: 93.134% (18001/19328)\n",
            "160 391 Loss: 0.201 | Train Acc: 93.066% (19179/20608)\n",
            "170 391 Loss: 0.200 | Train Acc: 93.083% (20374/21888)\n",
            "180 391 Loss: 0.203 | Train Acc: 92.973% (21540/23168)\n",
            "190 391 Loss: 0.204 | Train Acc: 92.911% (22715/24448)\n",
            "200 391 Loss: 0.203 | Train Acc: 92.914% (23905/25728)\n",
            "210 391 Loss: 0.204 | Train Acc: 92.910% (25093/27008)\n",
            "220 391 Loss: 0.204 | Train Acc: 92.880% (26274/28288)\n",
            "230 391 Loss: 0.205 | Train Acc: 92.840% (27451/29568)\n",
            "240 391 Loss: 0.206 | Train Acc: 92.836% (28638/30848)\n",
            "250 391 Loss: 0.204 | Train Acc: 92.922% (29854/32128)\n",
            "260 391 Loss: 0.205 | Train Acc: 92.894% (31034/33408)\n",
            "270 391 Loss: 0.205 | Train Acc: 92.853% (32209/34688)\n",
            "280 391 Loss: 0.206 | Train Acc: 92.855% (33398/35968)\n",
            "290 391 Loss: 0.206 | Train Acc: 92.824% (34575/37248)\n",
            "300 391 Loss: 0.206 | Train Acc: 92.842% (35770/38528)\n",
            "310 391 Loss: 0.207 | Train Acc: 92.805% (36944/39808)\n",
            "320 391 Loss: 0.207 | Train Acc: 92.835% (38144/41088)\n",
            "330 391 Loss: 0.208 | Train Acc: 92.801% (39318/42368)\n",
            "340 391 Loss: 0.208 | Train Acc: 92.799% (40505/43648)\n",
            "350 391 Loss: 0.207 | Train Acc: 92.815% (41700/44928)\n",
            "360 391 Loss: 0.207 | Train Acc: 92.830% (42895/46208)\n",
            "370 391 Loss: 0.208 | Train Acc: 92.813% (44075/47488)\n",
            "380 391 Loss: 0.208 | Train Acc: 92.805% (45259/48768)\n",
            "390 391 Loss: 0.209 | Train Acc: 92.752% (46376/50000)\n",
            "0 100 Loss: 0.358 | Test Acc: 91.000% (91/100)\n",
            "10 100 Loss: 0.373 | Test Acc: 87.182% (959/1100)\n",
            "20 100 Loss: 0.414 | Test Acc: 86.429% (1815/2100)\n",
            "30 100 Loss: 0.426 | Test Acc: 86.355% (2677/3100)\n",
            "40 100 Loss: 0.420 | Test Acc: 86.439% (3544/4100)\n",
            "50 100 Loss: 0.429 | Test Acc: 86.098% (4391/5100)\n",
            "60 100 Loss: 0.435 | Test Acc: 85.902% (5240/6100)\n",
            "70 100 Loss: 0.443 | Test Acc: 85.859% (6096/7100)\n",
            "80 100 Loss: 0.439 | Test Acc: 85.975% (6964/8100)\n",
            "90 100 Loss: 0.434 | Test Acc: 86.132% (7838/9100)\n",
            "\n",
            "Epoch: 98\n",
            "0 391 Loss: 0.157 | Train Acc: 94.531% (121/128)\n",
            "10 391 Loss: 0.222 | Train Acc: 92.827% (1307/1408)\n",
            "20 391 Loss: 0.208 | Train Acc: 92.746% (2493/2688)\n",
            "30 391 Loss: 0.208 | Train Acc: 92.893% (3686/3968)\n",
            "40 391 Loss: 0.210 | Train Acc: 92.893% (4875/5248)\n",
            "50 391 Loss: 0.207 | Train Acc: 92.999% (6071/6528)\n",
            "60 391 Loss: 0.203 | Train Acc: 93.084% (7268/7808)\n",
            "70 391 Loss: 0.203 | Train Acc: 93.123% (8463/9088)\n",
            "80 391 Loss: 0.203 | Train Acc: 93.065% (9649/10368)\n",
            "90 391 Loss: 0.203 | Train Acc: 93.020% (10835/11648)\n",
            "100 391 Loss: 0.202 | Train Acc: 93.116% (12038/12928)\n",
            "110 391 Loss: 0.204 | Train Acc: 93.018% (13216/14208)\n",
            "120 391 Loss: 0.206 | Train Acc: 92.988% (14402/15488)\n",
            "130 391 Loss: 0.206 | Train Acc: 93.016% (15597/16768)\n",
            "140 391 Loss: 0.204 | Train Acc: 93.085% (16800/18048)\n",
            "150 391 Loss: 0.204 | Train Acc: 93.072% (17989/19328)\n",
            "160 391 Loss: 0.204 | Train Acc: 93.012% (19168/20608)\n",
            "170 391 Loss: 0.204 | Train Acc: 93.010% (20358/21888)\n",
            "180 391 Loss: 0.206 | Train Acc: 92.943% (21533/23168)\n",
            "190 391 Loss: 0.205 | Train Acc: 92.981% (22732/24448)\n",
            "200 391 Loss: 0.205 | Train Acc: 92.980% (23922/25728)\n",
            "210 391 Loss: 0.205 | Train Acc: 92.987% (25114/27008)\n",
            "220 391 Loss: 0.206 | Train Acc: 92.944% (26292/28288)\n",
            "230 391 Loss: 0.207 | Train Acc: 92.928% (27477/29568)\n",
            "240 391 Loss: 0.207 | Train Acc: 92.946% (28672/30848)\n",
            "250 391 Loss: 0.207 | Train Acc: 92.941% (29860/32128)\n",
            "260 391 Loss: 0.207 | Train Acc: 92.942% (31050/33408)\n",
            "270 391 Loss: 0.206 | Train Acc: 92.960% (32246/34688)\n",
            "280 391 Loss: 0.207 | Train Acc: 92.949% (33432/35968)\n",
            "290 391 Loss: 0.208 | Train Acc: 92.910% (34607/37248)\n",
            "300 391 Loss: 0.209 | Train Acc: 92.881% (35785/38528)\n",
            "310 391 Loss: 0.210 | Train Acc: 92.828% (36953/39808)\n",
            "320 391 Loss: 0.210 | Train Acc: 92.815% (38136/41088)\n",
            "330 391 Loss: 0.210 | Train Acc: 92.832% (39331/42368)\n",
            "340 391 Loss: 0.209 | Train Acc: 92.836% (40521/43648)\n",
            "350 391 Loss: 0.210 | Train Acc: 92.826% (41705/44928)\n",
            "360 391 Loss: 0.209 | Train Acc: 92.837% (42898/46208)\n",
            "370 391 Loss: 0.208 | Train Acc: 92.845% (44090/47488)\n",
            "380 391 Loss: 0.208 | Train Acc: 92.842% (45277/48768)\n",
            "390 391 Loss: 0.208 | Train Acc: 92.828% (46414/50000)\n",
            "0 100 Loss: 0.444 | Test Acc: 87.000% (87/100)\n",
            "10 100 Loss: 0.360 | Test Acc: 88.818% (977/1100)\n",
            "20 100 Loss: 0.385 | Test Acc: 88.048% (1849/2100)\n",
            "30 100 Loss: 0.394 | Test Acc: 88.032% (2729/3100)\n",
            "40 100 Loss: 0.396 | Test Acc: 87.829% (3601/4100)\n",
            "50 100 Loss: 0.385 | Test Acc: 88.196% (4498/5100)\n",
            "60 100 Loss: 0.385 | Test Acc: 88.115% (5375/6100)\n",
            "70 100 Loss: 0.387 | Test Acc: 87.915% (6242/7100)\n",
            "80 100 Loss: 0.384 | Test Acc: 87.951% (7124/8100)\n",
            "90 100 Loss: 0.387 | Test Acc: 87.956% (8004/9100)\n",
            "\n",
            "Epoch: 99\n",
            "0 391 Loss: 0.223 | Train Acc: 92.188% (118/128)\n",
            "10 391 Loss: 0.179 | Train Acc: 93.750% (1320/1408)\n",
            "20 391 Loss: 0.186 | Train Acc: 93.527% (2514/2688)\n",
            "30 391 Loss: 0.192 | Train Acc: 93.221% (3699/3968)\n",
            "40 391 Loss: 0.188 | Train Acc: 93.579% (4911/5248)\n",
            "50 391 Loss: 0.194 | Train Acc: 93.229% (6086/6528)\n",
            "60 391 Loss: 0.193 | Train Acc: 93.327% (7287/7808)\n",
            "70 391 Loss: 0.191 | Train Acc: 93.398% (8488/9088)\n",
            "80 391 Loss: 0.193 | Train Acc: 93.277% (9671/10368)\n",
            "90 391 Loss: 0.198 | Train Acc: 93.089% (10843/11648)\n",
            "100 391 Loss: 0.201 | Train Acc: 93.015% (12025/12928)\n",
            "110 391 Loss: 0.202 | Train Acc: 92.955% (13207/14208)\n",
            "120 391 Loss: 0.203 | Train Acc: 92.943% (14395/15488)\n",
            "130 391 Loss: 0.204 | Train Acc: 92.885% (15575/16768)\n",
            "140 391 Loss: 0.202 | Train Acc: 92.974% (16780/18048)\n",
            "150 391 Loss: 0.204 | Train Acc: 92.922% (17960/19328)\n",
            "160 391 Loss: 0.203 | Train Acc: 92.978% (19161/20608)\n",
            "170 391 Loss: 0.204 | Train Acc: 93.010% (20358/21888)\n",
            "180 391 Loss: 0.203 | Train Acc: 93.016% (21550/23168)\n",
            "190 391 Loss: 0.204 | Train Acc: 93.001% (22737/24448)\n",
            "200 391 Loss: 0.203 | Train Acc: 93.015% (23931/25728)\n",
            "210 391 Loss: 0.204 | Train Acc: 93.021% (25123/27008)\n",
            "220 391 Loss: 0.203 | Train Acc: 93.043% (26320/28288)\n",
            "230 391 Loss: 0.203 | Train Acc: 93.047% (27512/29568)\n",
            "240 391 Loss: 0.202 | Train Acc: 93.082% (28714/30848)\n",
            "250 391 Loss: 0.202 | Train Acc: 93.090% (29908/32128)\n",
            "260 391 Loss: 0.202 | Train Acc: 93.068% (31092/33408)\n",
            "270 391 Loss: 0.203 | Train Acc: 93.055% (32279/34688)\n",
            "280 391 Loss: 0.204 | Train Acc: 93.033% (33462/35968)\n",
            "290 391 Loss: 0.204 | Train Acc: 93.022% (34649/37248)\n",
            "300 391 Loss: 0.204 | Train Acc: 93.005% (35833/38528)\n",
            "310 391 Loss: 0.204 | Train Acc: 92.989% (37017/39808)\n",
            "320 391 Loss: 0.205 | Train Acc: 92.971% (38200/41088)\n",
            "330 391 Loss: 0.204 | Train Acc: 92.978% (39393/42368)\n",
            "340 391 Loss: 0.204 | Train Acc: 92.976% (40582/43648)\n",
            "350 391 Loss: 0.204 | Train Acc: 92.967% (41768/44928)\n",
            "360 391 Loss: 0.205 | Train Acc: 92.945% (42948/46208)\n",
            "370 391 Loss: 0.205 | Train Acc: 92.939% (44135/47488)\n",
            "380 391 Loss: 0.206 | Train Acc: 92.913% (45312/48768)\n",
            "390 391 Loss: 0.207 | Train Acc: 92.882% (46441/50000)\n",
            "0 100 Loss: 0.311 | Test Acc: 91.000% (91/100)\n",
            "10 100 Loss: 0.334 | Test Acc: 89.455% (984/1100)\n",
            "20 100 Loss: 0.344 | Test Acc: 88.952% (1868/2100)\n",
            "30 100 Loss: 0.345 | Test Acc: 89.194% (2765/3100)\n",
            "40 100 Loss: 0.340 | Test Acc: 89.024% (3650/4100)\n",
            "50 100 Loss: 0.339 | Test Acc: 89.118% (4545/5100)\n",
            "60 100 Loss: 0.338 | Test Acc: 88.934% (5425/6100)\n",
            "70 100 Loss: 0.335 | Test Acc: 89.141% (6329/7100)\n",
            "80 100 Loss: 0.335 | Test Acc: 89.173% (7223/8100)\n",
            "90 100 Loss: 0.341 | Test Acc: 89.000% (8099/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 100\n",
            "0 391 Loss: 0.157 | Train Acc: 95.312% (122/128)\n",
            "10 391 Loss: 0.195 | Train Acc: 94.034% (1324/1408)\n",
            "20 391 Loss: 0.192 | Train Acc: 93.638% (2517/2688)\n",
            "30 391 Loss: 0.184 | Train Acc: 93.725% (3719/3968)\n",
            "40 391 Loss: 0.178 | Train Acc: 94.036% (4935/5248)\n",
            "50 391 Loss: 0.176 | Train Acc: 94.102% (6143/6528)\n",
            "60 391 Loss: 0.178 | Train Acc: 93.929% (7334/7808)\n",
            "70 391 Loss: 0.177 | Train Acc: 93.926% (8536/9088)\n",
            "80 391 Loss: 0.179 | Train Acc: 93.856% (9731/10368)\n",
            "90 391 Loss: 0.180 | Train Acc: 93.827% (10929/11648)\n",
            "100 391 Loss: 0.180 | Train Acc: 93.820% (12129/12928)\n",
            "110 391 Loss: 0.184 | Train Acc: 93.651% (13306/14208)\n",
            "120 391 Loss: 0.185 | Train Acc: 93.673% (14508/15488)\n",
            "130 391 Loss: 0.186 | Train Acc: 93.684% (15709/16768)\n",
            "140 391 Loss: 0.188 | Train Acc: 93.556% (16885/18048)\n",
            "150 391 Loss: 0.189 | Train Acc: 93.517% (18075/19328)\n",
            "160 391 Loss: 0.190 | Train Acc: 93.469% (19262/20608)\n",
            "170 391 Loss: 0.191 | Train Acc: 93.444% (20453/21888)\n",
            "180 391 Loss: 0.192 | Train Acc: 93.418% (21643/23168)\n",
            "190 391 Loss: 0.193 | Train Acc: 93.386% (22831/24448)\n",
            "200 391 Loss: 0.192 | Train Acc: 93.412% (24033/25728)\n",
            "210 391 Loss: 0.192 | Train Acc: 93.435% (25235/27008)\n",
            "220 391 Loss: 0.190 | Train Acc: 93.520% (26455/28288)\n",
            "230 391 Loss: 0.190 | Train Acc: 93.506% (27648/29568)\n",
            "240 391 Loss: 0.190 | Train Acc: 93.497% (28842/30848)\n",
            "250 391 Loss: 0.190 | Train Acc: 93.517% (30045/32128)\n",
            "260 391 Loss: 0.191 | Train Acc: 93.502% (31237/33408)\n",
            "270 391 Loss: 0.192 | Train Acc: 93.459% (32419/34688)\n",
            "280 391 Loss: 0.192 | Train Acc: 93.447% (33611/35968)\n",
            "290 391 Loss: 0.192 | Train Acc: 93.436% (34803/37248)\n",
            "300 391 Loss: 0.193 | Train Acc: 93.418% (35992/38528)\n",
            "310 391 Loss: 0.195 | Train Acc: 93.356% (37163/39808)\n",
            "320 391 Loss: 0.195 | Train Acc: 93.324% (38345/41088)\n",
            "330 391 Loss: 0.196 | Train Acc: 93.280% (39521/42368)\n",
            "340 391 Loss: 0.196 | Train Acc: 93.283% (40716/43648)\n",
            "350 391 Loss: 0.196 | Train Acc: 93.267% (41903/44928)\n",
            "360 391 Loss: 0.197 | Train Acc: 93.252% (43090/46208)\n",
            "370 391 Loss: 0.197 | Train Acc: 93.251% (44283/47488)\n",
            "380 391 Loss: 0.199 | Train Acc: 93.188% (45446/48768)\n",
            "390 391 Loss: 0.199 | Train Acc: 93.166% (46583/50000)\n",
            "0 100 Loss: 0.412 | Test Acc: 85.000% (85/100)\n",
            "10 100 Loss: 0.351 | Test Acc: 88.545% (974/1100)\n",
            "20 100 Loss: 0.381 | Test Acc: 88.000% (1848/2100)\n",
            "30 100 Loss: 0.406 | Test Acc: 87.065% (2699/3100)\n",
            "40 100 Loss: 0.403 | Test Acc: 87.024% (3568/4100)\n",
            "50 100 Loss: 0.390 | Test Acc: 87.353% (4455/5100)\n",
            "60 100 Loss: 0.389 | Test Acc: 87.361% (5329/6100)\n",
            "70 100 Loss: 0.385 | Test Acc: 87.634% (6222/7100)\n",
            "80 100 Loss: 0.385 | Test Acc: 87.716% (7105/8100)\n",
            "90 100 Loss: 0.387 | Test Acc: 87.593% (7971/9100)\n",
            "\n",
            "Epoch: 101\n",
            "0 391 Loss: 0.150 | Train Acc: 95.312% (122/128)\n",
            "10 391 Loss: 0.192 | Train Acc: 93.395% (1315/1408)\n",
            "20 391 Loss: 0.186 | Train Acc: 93.378% (2510/2688)\n",
            "30 391 Loss: 0.189 | Train Acc: 93.674% (3717/3968)\n",
            "40 391 Loss: 0.189 | Train Acc: 93.636% (4914/5248)\n",
            "50 391 Loss: 0.192 | Train Acc: 93.505% (6104/6528)\n",
            "60 391 Loss: 0.199 | Train Acc: 93.212% (7278/7808)\n",
            "70 391 Loss: 0.200 | Train Acc: 93.156% (8466/9088)\n",
            "80 391 Loss: 0.202 | Train Acc: 93.075% (9650/10368)\n",
            "90 391 Loss: 0.201 | Train Acc: 93.046% (10838/11648)\n",
            "100 391 Loss: 0.200 | Train Acc: 93.108% (12037/12928)\n",
            "110 391 Loss: 0.199 | Train Acc: 93.180% (13239/14208)\n",
            "120 391 Loss: 0.198 | Train Acc: 93.253% (14443/15488)\n",
            "130 391 Loss: 0.198 | Train Acc: 93.267% (15639/16768)\n",
            "140 391 Loss: 0.195 | Train Acc: 93.312% (16841/18048)\n",
            "150 391 Loss: 0.193 | Train Acc: 93.414% (18055/19328)\n",
            "160 391 Loss: 0.193 | Train Acc: 93.439% (19256/20608)\n",
            "170 391 Loss: 0.193 | Train Acc: 93.416% (20447/21888)\n",
            "180 391 Loss: 0.193 | Train Acc: 93.422% (21644/23168)\n",
            "190 391 Loss: 0.194 | Train Acc: 93.382% (22830/24448)\n",
            "200 391 Loss: 0.195 | Train Acc: 93.346% (24016/25728)\n",
            "210 391 Loss: 0.197 | Train Acc: 93.228% (25179/27008)\n",
            "220 391 Loss: 0.197 | Train Acc: 93.216% (26369/28288)\n",
            "230 391 Loss: 0.198 | Train Acc: 93.172% (27549/29568)\n",
            "240 391 Loss: 0.198 | Train Acc: 93.154% (28736/30848)\n",
            "250 391 Loss: 0.198 | Train Acc: 93.127% (29920/32128)\n",
            "260 391 Loss: 0.198 | Train Acc: 93.100% (31103/33408)\n",
            "270 391 Loss: 0.198 | Train Acc: 93.110% (32298/34688)\n",
            "280 391 Loss: 0.199 | Train Acc: 93.086% (33481/35968)\n",
            "290 391 Loss: 0.200 | Train Acc: 93.068% (34666/37248)\n",
            "300 391 Loss: 0.200 | Train Acc: 93.067% (35857/38528)\n",
            "310 391 Loss: 0.199 | Train Acc: 93.074% (37051/39808)\n",
            "320 391 Loss: 0.199 | Train Acc: 93.069% (38240/41088)\n",
            "330 391 Loss: 0.199 | Train Acc: 93.099% (39444/42368)\n",
            "340 391 Loss: 0.198 | Train Acc: 93.115% (40643/43648)\n",
            "350 391 Loss: 0.199 | Train Acc: 93.082% (41820/44928)\n",
            "360 391 Loss: 0.198 | Train Acc: 93.081% (43011/46208)\n",
            "370 391 Loss: 0.199 | Train Acc: 93.047% (44186/47488)\n",
            "380 391 Loss: 0.200 | Train Acc: 93.020% (45364/48768)\n",
            "390 391 Loss: 0.201 | Train Acc: 93.006% (46503/50000)\n",
            "0 100 Loss: 0.293 | Test Acc: 93.000% (93/100)\n",
            "10 100 Loss: 0.339 | Test Acc: 89.000% (979/1100)\n",
            "20 100 Loss: 0.339 | Test Acc: 88.857% (1866/2100)\n",
            "30 100 Loss: 0.351 | Test Acc: 88.742% (2751/3100)\n",
            "40 100 Loss: 0.346 | Test Acc: 88.951% (3647/4100)\n",
            "50 100 Loss: 0.359 | Test Acc: 88.627% (4520/5100)\n",
            "60 100 Loss: 0.360 | Test Acc: 88.426% (5394/6100)\n",
            "70 100 Loss: 0.359 | Test Acc: 88.563% (6288/7100)\n",
            "80 100 Loss: 0.364 | Test Acc: 88.568% (7174/8100)\n",
            "90 100 Loss: 0.367 | Test Acc: 88.473% (8051/9100)\n",
            "\n",
            "Epoch: 102\n",
            "0 391 Loss: 0.165 | Train Acc: 95.312% (122/128)\n",
            "10 391 Loss: 0.195 | Train Acc: 93.608% (1318/1408)\n",
            "20 391 Loss: 0.192 | Train Acc: 93.601% (2516/2688)\n",
            "30 391 Loss: 0.197 | Train Acc: 93.473% (3709/3968)\n",
            "40 391 Loss: 0.188 | Train Acc: 93.769% (4921/5248)\n",
            "50 391 Loss: 0.184 | Train Acc: 93.781% (6122/6528)\n",
            "60 391 Loss: 0.182 | Train Acc: 93.788% (7323/7808)\n",
            "70 391 Loss: 0.184 | Train Acc: 93.629% (8509/9088)\n",
            "80 391 Loss: 0.188 | Train Acc: 93.480% (9692/10368)\n",
            "90 391 Loss: 0.190 | Train Acc: 93.381% (10877/11648)\n",
            "100 391 Loss: 0.191 | Train Acc: 93.379% (12072/12928)\n",
            "110 391 Loss: 0.190 | Train Acc: 93.384% (13268/14208)\n",
            "120 391 Loss: 0.191 | Train Acc: 93.304% (14451/15488)\n",
            "130 391 Loss: 0.194 | Train Acc: 93.249% (15636/16768)\n",
            "140 391 Loss: 0.193 | Train Acc: 93.301% (16839/18048)\n",
            "150 391 Loss: 0.195 | Train Acc: 93.227% (18019/19328)\n",
            "160 391 Loss: 0.196 | Train Acc: 93.221% (19211/20608)\n",
            "170 391 Loss: 0.196 | Train Acc: 93.211% (20402/21888)\n",
            "180 391 Loss: 0.197 | Train Acc: 93.128% (21576/23168)\n",
            "190 391 Loss: 0.197 | Train Acc: 93.145% (22772/24448)\n",
            "200 391 Loss: 0.196 | Train Acc: 93.159% (23968/25728)\n",
            "210 391 Loss: 0.196 | Train Acc: 93.187% (25168/27008)\n",
            "220 391 Loss: 0.195 | Train Acc: 93.163% (26354/28288)\n",
            "230 391 Loss: 0.194 | Train Acc: 93.158% (27545/29568)\n",
            "240 391 Loss: 0.194 | Train Acc: 93.170% (28741/30848)\n",
            "250 391 Loss: 0.194 | Train Acc: 93.205% (29945/32128)\n",
            "260 391 Loss: 0.194 | Train Acc: 93.214% (31141/33408)\n",
            "270 391 Loss: 0.196 | Train Acc: 93.173% (32320/34688)\n",
            "280 391 Loss: 0.195 | Train Acc: 93.205% (33524/35968)\n",
            "290 391 Loss: 0.194 | Train Acc: 93.240% (34730/37248)\n",
            "300 391 Loss: 0.194 | Train Acc: 93.246% (35926/38528)\n",
            "310 391 Loss: 0.194 | Train Acc: 93.258% (37124/39808)\n",
            "320 391 Loss: 0.193 | Train Acc: 93.271% (38323/41088)\n",
            "330 391 Loss: 0.194 | Train Acc: 93.257% (39511/42368)\n",
            "340 391 Loss: 0.193 | Train Acc: 93.273% (40712/43648)\n",
            "350 391 Loss: 0.194 | Train Acc: 93.249% (41895/44928)\n",
            "360 391 Loss: 0.195 | Train Acc: 93.220% (43075/46208)\n",
            "370 391 Loss: 0.195 | Train Acc: 93.228% (44272/47488)\n",
            "380 391 Loss: 0.196 | Train Acc: 93.174% (45439/48768)\n",
            "390 391 Loss: 0.197 | Train Acc: 93.144% (46572/50000)\n",
            "0 100 Loss: 0.462 | Test Acc: 83.000% (83/100)\n",
            "10 100 Loss: 0.523 | Test Acc: 83.364% (917/1100)\n",
            "20 100 Loss: 0.533 | Test Acc: 83.333% (1750/2100)\n",
            "30 100 Loss: 0.538 | Test Acc: 83.968% (2603/3100)\n",
            "40 100 Loss: 0.538 | Test Acc: 83.805% (3436/4100)\n",
            "50 100 Loss: 0.537 | Test Acc: 83.804% (4274/5100)\n",
            "60 100 Loss: 0.532 | Test Acc: 83.803% (5112/6100)\n",
            "70 100 Loss: 0.531 | Test Acc: 83.887% (5956/7100)\n",
            "80 100 Loss: 0.533 | Test Acc: 83.802% (6788/8100)\n",
            "90 100 Loss: 0.534 | Test Acc: 83.747% (7621/9100)\n",
            "\n",
            "Epoch: 103\n",
            "0 391 Loss: 0.095 | Train Acc: 96.875% (124/128)\n",
            "10 391 Loss: 0.194 | Train Acc: 93.324% (1314/1408)\n",
            "20 391 Loss: 0.178 | Train Acc: 93.899% (2524/2688)\n",
            "30 391 Loss: 0.176 | Train Acc: 94.052% (3732/3968)\n",
            "40 391 Loss: 0.176 | Train Acc: 94.055% (4936/5248)\n",
            "50 391 Loss: 0.176 | Train Acc: 93.995% (6136/6528)\n",
            "60 391 Loss: 0.173 | Train Acc: 94.032% (7342/7808)\n",
            "70 391 Loss: 0.170 | Train Acc: 94.168% (8558/9088)\n",
            "80 391 Loss: 0.173 | Train Acc: 94.001% (9746/10368)\n",
            "90 391 Loss: 0.173 | Train Acc: 94.008% (10950/11648)\n",
            "100 391 Loss: 0.175 | Train Acc: 93.851% (12133/12928)\n",
            "110 391 Loss: 0.178 | Train Acc: 93.771% (13323/14208)\n",
            "120 391 Loss: 0.179 | Train Acc: 93.705% (14513/15488)\n",
            "130 391 Loss: 0.178 | Train Acc: 93.720% (15715/16768)\n",
            "140 391 Loss: 0.180 | Train Acc: 93.722% (16915/18048)\n",
            "150 391 Loss: 0.180 | Train Acc: 93.740% (18118/19328)\n",
            "160 391 Loss: 0.181 | Train Acc: 93.726% (19315/20608)\n",
            "170 391 Loss: 0.183 | Train Acc: 93.681% (20505/21888)\n",
            "180 391 Loss: 0.183 | Train Acc: 93.621% (21690/23168)\n",
            "190 391 Loss: 0.185 | Train Acc: 93.550% (22871/24448)\n",
            "200 391 Loss: 0.186 | Train Acc: 93.509% (24058/25728)\n",
            "210 391 Loss: 0.186 | Train Acc: 93.513% (25256/27008)\n",
            "220 391 Loss: 0.187 | Train Acc: 93.499% (26449/28288)\n",
            "230 391 Loss: 0.186 | Train Acc: 93.493% (27644/29568)\n",
            "240 391 Loss: 0.187 | Train Acc: 93.478% (28836/30848)\n",
            "250 391 Loss: 0.189 | Train Acc: 93.423% (30015/32128)\n",
            "260 391 Loss: 0.188 | Train Acc: 93.445% (31218/33408)\n",
            "270 391 Loss: 0.189 | Train Acc: 93.433% (32410/34688)\n",
            "280 391 Loss: 0.189 | Train Acc: 93.458% (33615/35968)\n",
            "290 391 Loss: 0.192 | Train Acc: 93.353% (34772/37248)\n",
            "300 391 Loss: 0.193 | Train Acc: 93.309% (35950/38528)\n",
            "310 391 Loss: 0.194 | Train Acc: 93.270% (37129/39808)\n",
            "320 391 Loss: 0.195 | Train Acc: 93.254% (38316/41088)\n",
            "330 391 Loss: 0.194 | Train Acc: 93.250% (39508/42368)\n",
            "340 391 Loss: 0.194 | Train Acc: 93.273% (40712/43648)\n",
            "350 391 Loss: 0.194 | Train Acc: 93.265% (41902/44928)\n",
            "360 391 Loss: 0.195 | Train Acc: 93.218% (43074/46208)\n",
            "370 391 Loss: 0.195 | Train Acc: 93.228% (44272/47488)\n",
            "380 391 Loss: 0.195 | Train Acc: 93.235% (45469/48768)\n",
            "390 391 Loss: 0.196 | Train Acc: 93.212% (46606/50000)\n",
            "0 100 Loss: 0.317 | Test Acc: 89.000% (89/100)\n",
            "10 100 Loss: 0.333 | Test Acc: 89.545% (985/1100)\n",
            "20 100 Loss: 0.356 | Test Acc: 88.952% (1868/2100)\n",
            "30 100 Loss: 0.361 | Test Acc: 89.290% (2768/3100)\n",
            "40 100 Loss: 0.347 | Test Acc: 89.439% (3667/4100)\n",
            "50 100 Loss: 0.346 | Test Acc: 89.275% (4553/5100)\n",
            "60 100 Loss: 0.345 | Test Acc: 89.311% (5448/6100)\n",
            "70 100 Loss: 0.341 | Test Acc: 89.394% (6347/7100)\n",
            "80 100 Loss: 0.341 | Test Acc: 89.457% (7246/8100)\n",
            "90 100 Loss: 0.336 | Test Acc: 89.560% (8150/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 104\n",
            "0 391 Loss: 0.174 | Train Acc: 94.531% (121/128)\n",
            "10 391 Loss: 0.178 | Train Acc: 93.963% (1323/1408)\n",
            "20 391 Loss: 0.184 | Train Acc: 93.824% (2522/2688)\n",
            "30 391 Loss: 0.179 | Train Acc: 94.027% (3731/3968)\n",
            "40 391 Loss: 0.177 | Train Acc: 94.036% (4935/5248)\n",
            "50 391 Loss: 0.173 | Train Acc: 94.225% (6151/6528)\n",
            "60 391 Loss: 0.173 | Train Acc: 94.249% (7359/7808)\n",
            "70 391 Loss: 0.174 | Train Acc: 94.179% (8559/9088)\n",
            "80 391 Loss: 0.175 | Train Acc: 94.145% (9761/10368)\n",
            "90 391 Loss: 0.175 | Train Acc: 94.205% (10973/11648)\n",
            "100 391 Loss: 0.171 | Train Acc: 94.330% (12195/12928)\n",
            "110 391 Loss: 0.173 | Train Acc: 94.207% (13385/14208)\n",
            "120 391 Loss: 0.172 | Train Acc: 94.273% (14601/15488)\n",
            "130 391 Loss: 0.172 | Train Acc: 94.263% (15806/16768)\n",
            "140 391 Loss: 0.174 | Train Acc: 94.210% (17003/18048)\n",
            "150 391 Loss: 0.175 | Train Acc: 94.154% (18198/19328)\n",
            "160 391 Loss: 0.178 | Train Acc: 93.993% (19370/20608)\n",
            "170 391 Loss: 0.182 | Train Acc: 93.914% (20556/21888)\n",
            "180 391 Loss: 0.184 | Train Acc: 93.879% (21750/23168)\n",
            "190 391 Loss: 0.185 | Train Acc: 93.836% (22941/24448)\n",
            "200 391 Loss: 0.186 | Train Acc: 93.812% (24136/25728)\n",
            "210 391 Loss: 0.186 | Train Acc: 93.846% (25346/27008)\n",
            "220 391 Loss: 0.184 | Train Acc: 93.930% (26571/28288)\n",
            "230 391 Loss: 0.183 | Train Acc: 93.926% (27772/29568)\n",
            "240 391 Loss: 0.183 | Train Acc: 93.906% (28968/30848)\n",
            "250 391 Loss: 0.184 | Train Acc: 93.865% (30157/32128)\n",
            "260 391 Loss: 0.184 | Train Acc: 93.861% (31357/33408)\n",
            "270 391 Loss: 0.184 | Train Acc: 93.865% (32560/34688)\n",
            "280 391 Loss: 0.183 | Train Acc: 93.892% (33771/35968)\n",
            "290 391 Loss: 0.183 | Train Acc: 93.876% (34967/37248)\n",
            "300 391 Loss: 0.183 | Train Acc: 93.888% (36173/38528)\n",
            "310 391 Loss: 0.182 | Train Acc: 93.903% (37381/39808)\n",
            "320 391 Loss: 0.183 | Train Acc: 93.901% (38582/41088)\n",
            "330 391 Loss: 0.183 | Train Acc: 93.906% (39786/42368)\n",
            "340 391 Loss: 0.183 | Train Acc: 93.892% (40982/43648)\n",
            "350 391 Loss: 0.184 | Train Acc: 93.875% (42176/44928)\n",
            "360 391 Loss: 0.184 | Train Acc: 93.858% (43370/46208)\n",
            "370 391 Loss: 0.183 | Train Acc: 93.853% (44569/47488)\n",
            "380 391 Loss: 0.184 | Train Acc: 93.832% (45760/48768)\n",
            "390 391 Loss: 0.185 | Train Acc: 93.786% (46893/50000)\n",
            "0 100 Loss: 0.427 | Test Acc: 88.000% (88/100)\n",
            "10 100 Loss: 0.524 | Test Acc: 85.455% (940/1100)\n",
            "20 100 Loss: 0.514 | Test Acc: 84.905% (1783/2100)\n",
            "30 100 Loss: 0.514 | Test Acc: 85.065% (2637/3100)\n",
            "40 100 Loss: 0.503 | Test Acc: 85.146% (3491/4100)\n",
            "50 100 Loss: 0.495 | Test Acc: 85.353% (4353/5100)\n",
            "60 100 Loss: 0.494 | Test Acc: 85.262% (5201/6100)\n",
            "70 100 Loss: 0.485 | Test Acc: 85.549% (6074/7100)\n",
            "80 100 Loss: 0.482 | Test Acc: 85.407% (6918/8100)\n",
            "90 100 Loss: 0.481 | Test Acc: 85.286% (7761/9100)\n",
            "\n",
            "Epoch: 105\n",
            "0 391 Loss: 0.251 | Train Acc: 92.969% (119/128)\n",
            "10 391 Loss: 0.188 | Train Acc: 94.105% (1325/1408)\n",
            "20 391 Loss: 0.178 | Train Acc: 94.271% (2534/2688)\n",
            "30 391 Loss: 0.178 | Train Acc: 94.229% (3739/3968)\n",
            "40 391 Loss: 0.172 | Train Acc: 94.245% (4946/5248)\n",
            "50 391 Loss: 0.173 | Train Acc: 94.194% (6149/6528)\n",
            "60 391 Loss: 0.173 | Train Acc: 94.185% (7354/7808)\n",
            "70 391 Loss: 0.176 | Train Acc: 94.058% (8548/9088)\n",
            "80 391 Loss: 0.173 | Train Acc: 94.097% (9756/10368)\n",
            "90 391 Loss: 0.176 | Train Acc: 93.939% (10942/11648)\n",
            "100 391 Loss: 0.179 | Train Acc: 93.820% (12129/12928)\n",
            "110 391 Loss: 0.177 | Train Acc: 93.912% (13343/14208)\n",
            "120 391 Loss: 0.175 | Train Acc: 93.937% (14549/15488)\n",
            "130 391 Loss: 0.177 | Train Acc: 93.887% (15743/16768)\n",
            "140 391 Loss: 0.179 | Train Acc: 93.805% (16930/18048)\n",
            "150 391 Loss: 0.178 | Train Acc: 93.771% (18124/19328)\n",
            "160 391 Loss: 0.180 | Train Acc: 93.731% (19316/20608)\n",
            "170 391 Loss: 0.181 | Train Acc: 93.732% (20516/21888)\n",
            "180 391 Loss: 0.181 | Train Acc: 93.728% (21715/23168)\n",
            "190 391 Loss: 0.181 | Train Acc: 93.721% (22913/24448)\n",
            "200 391 Loss: 0.181 | Train Acc: 93.684% (24103/25728)\n",
            "210 391 Loss: 0.181 | Train Acc: 93.720% (25312/27008)\n",
            "220 391 Loss: 0.181 | Train Acc: 93.718% (26511/28288)\n",
            "230 391 Loss: 0.182 | Train Acc: 93.676% (27698/29568)\n",
            "240 391 Loss: 0.184 | Train Acc: 93.611% (28877/30848)\n",
            "250 391 Loss: 0.184 | Train Acc: 93.597% (30071/32128)\n",
            "260 391 Loss: 0.186 | Train Acc: 93.537% (31249/33408)\n",
            "270 391 Loss: 0.186 | Train Acc: 93.557% (32453/34688)\n",
            "280 391 Loss: 0.186 | Train Acc: 93.553% (33649/35968)\n",
            "290 391 Loss: 0.185 | Train Acc: 93.586% (34859/37248)\n",
            "300 391 Loss: 0.185 | Train Acc: 93.599% (36062/38528)\n",
            "310 391 Loss: 0.185 | Train Acc: 93.569% (37248/39808)\n",
            "320 391 Loss: 0.185 | Train Acc: 93.575% (38448/41088)\n",
            "330 391 Loss: 0.185 | Train Acc: 93.590% (39652/42368)\n",
            "340 391 Loss: 0.185 | Train Acc: 93.539% (40828/43648)\n",
            "350 391 Loss: 0.187 | Train Acc: 93.507% (42011/44928)\n",
            "360 391 Loss: 0.187 | Train Acc: 93.484% (43197/46208)\n",
            "370 391 Loss: 0.189 | Train Acc: 93.449% (44377/47488)\n",
            "380 391 Loss: 0.190 | Train Acc: 93.430% (45564/48768)\n",
            "390 391 Loss: 0.190 | Train Acc: 93.418% (46709/50000)\n",
            "0 100 Loss: 0.261 | Test Acc: 91.000% (91/100)\n",
            "10 100 Loss: 0.317 | Test Acc: 88.909% (978/1100)\n",
            "20 100 Loss: 0.342 | Test Acc: 88.571% (1860/2100)\n",
            "30 100 Loss: 0.356 | Test Acc: 88.000% (2728/3100)\n",
            "40 100 Loss: 0.355 | Test Acc: 88.122% (3613/4100)\n",
            "50 100 Loss: 0.346 | Test Acc: 88.569% (4517/5100)\n",
            "60 100 Loss: 0.346 | Test Acc: 88.639% (5407/6100)\n",
            "70 100 Loss: 0.348 | Test Acc: 88.563% (6288/7100)\n",
            "80 100 Loss: 0.343 | Test Acc: 88.556% (7173/8100)\n",
            "90 100 Loss: 0.342 | Test Acc: 88.571% (8060/9100)\n",
            "\n",
            "Epoch: 106\n",
            "0 391 Loss: 0.160 | Train Acc: 93.750% (120/128)\n",
            "10 391 Loss: 0.165 | Train Acc: 94.176% (1326/1408)\n",
            "20 391 Loss: 0.176 | Train Acc: 93.564% (2515/2688)\n",
            "30 391 Loss: 0.174 | Train Acc: 93.649% (3716/3968)\n",
            "40 391 Loss: 0.170 | Train Acc: 93.883% (4927/5248)\n",
            "50 391 Loss: 0.165 | Train Acc: 94.087% (6142/6528)\n",
            "60 391 Loss: 0.166 | Train Acc: 94.134% (7350/7808)\n",
            "70 391 Loss: 0.169 | Train Acc: 94.080% (8550/9088)\n",
            "80 391 Loss: 0.167 | Train Acc: 94.232% (9770/10368)\n",
            "90 391 Loss: 0.170 | Train Acc: 94.171% (10969/11648)\n",
            "100 391 Loss: 0.170 | Train Acc: 94.206% (12179/12928)\n",
            "110 391 Loss: 0.173 | Train Acc: 94.123% (13373/14208)\n",
            "120 391 Loss: 0.175 | Train Acc: 94.021% (14562/15488)\n",
            "130 391 Loss: 0.177 | Train Acc: 93.953% (15754/16768)\n",
            "140 391 Loss: 0.180 | Train Acc: 93.911% (16949/18048)\n",
            "150 391 Loss: 0.181 | Train Acc: 93.848% (18139/19328)\n",
            "160 391 Loss: 0.183 | Train Acc: 93.765% (19323/20608)\n",
            "170 391 Loss: 0.184 | Train Acc: 93.700% (20509/21888)\n",
            "180 391 Loss: 0.185 | Train Acc: 93.685% (21705/23168)\n",
            "190 391 Loss: 0.184 | Train Acc: 93.660% (22898/24448)\n",
            "200 391 Loss: 0.184 | Train Acc: 93.645% (24093/25728)\n",
            "210 391 Loss: 0.186 | Train Acc: 93.546% (25265/27008)\n",
            "220 391 Loss: 0.187 | Train Acc: 93.584% (26473/28288)\n",
            "230 391 Loss: 0.187 | Train Acc: 93.598% (27675/29568)\n",
            "240 391 Loss: 0.187 | Train Acc: 93.559% (28861/30848)\n",
            "250 391 Loss: 0.188 | Train Acc: 93.551% (30056/32128)\n",
            "260 391 Loss: 0.188 | Train Acc: 93.531% (31247/33408)\n",
            "270 391 Loss: 0.188 | Train Acc: 93.537% (32446/34688)\n",
            "280 391 Loss: 0.188 | Train Acc: 93.528% (33640/35968)\n",
            "290 391 Loss: 0.187 | Train Acc: 93.543% (34843/37248)\n",
            "300 391 Loss: 0.188 | Train Acc: 93.480% (36016/38528)\n",
            "310 391 Loss: 0.189 | Train Acc: 93.474% (37210/39808)\n",
            "320 391 Loss: 0.189 | Train Acc: 93.409% (38380/41088)\n",
            "330 391 Loss: 0.190 | Train Acc: 93.408% (39575/42368)\n",
            "340 391 Loss: 0.190 | Train Acc: 93.365% (40752/43648)\n",
            "350 391 Loss: 0.189 | Train Acc: 93.378% (41953/44928)\n",
            "360 391 Loss: 0.189 | Train Acc: 93.369% (43144/46208)\n",
            "370 391 Loss: 0.189 | Train Acc: 93.384% (44346/47488)\n",
            "380 391 Loss: 0.189 | Train Acc: 93.403% (45551/48768)\n",
            "390 391 Loss: 0.190 | Train Acc: 93.388% (46694/50000)\n",
            "0 100 Loss: 0.473 | Test Acc: 87.000% (87/100)\n",
            "10 100 Loss: 0.490 | Test Acc: 86.182% (948/1100)\n",
            "20 100 Loss: 0.493 | Test Acc: 85.810% (1802/2100)\n",
            "30 100 Loss: 0.517 | Test Acc: 85.452% (2649/3100)\n",
            "40 100 Loss: 0.517 | Test Acc: 85.341% (3499/4100)\n",
            "50 100 Loss: 0.512 | Test Acc: 85.196% (4345/5100)\n",
            "60 100 Loss: 0.515 | Test Acc: 85.148% (5194/6100)\n",
            "70 100 Loss: 0.510 | Test Acc: 85.141% (6045/7100)\n",
            "80 100 Loss: 0.501 | Test Acc: 85.383% (6916/8100)\n",
            "90 100 Loss: 0.496 | Test Acc: 85.462% (7777/9100)\n",
            "\n",
            "Epoch: 107\n",
            "0 391 Loss: 0.092 | Train Acc: 97.656% (125/128)\n",
            "10 391 Loss: 0.176 | Train Acc: 94.247% (1327/1408)\n",
            "20 391 Loss: 0.171 | Train Acc: 93.973% (2526/2688)\n",
            "30 391 Loss: 0.176 | Train Acc: 93.800% (3722/3968)\n",
            "40 391 Loss: 0.169 | Train Acc: 94.188% (4943/5248)\n",
            "50 391 Loss: 0.167 | Train Acc: 94.347% (6159/6528)\n",
            "60 391 Loss: 0.169 | Train Acc: 94.237% (7358/7808)\n",
            "70 391 Loss: 0.166 | Train Acc: 94.267% (8567/9088)\n",
            "80 391 Loss: 0.164 | Train Acc: 94.338% (9781/10368)\n",
            "90 391 Loss: 0.163 | Train Acc: 94.325% (10987/11648)\n",
            "100 391 Loss: 0.165 | Train Acc: 94.245% (12184/12928)\n",
            "110 391 Loss: 0.166 | Train Acc: 94.151% (13377/14208)\n",
            "120 391 Loss: 0.167 | Train Acc: 94.137% (14580/15488)\n",
            "130 391 Loss: 0.169 | Train Acc: 94.060% (15772/16768)\n",
            "140 391 Loss: 0.170 | Train Acc: 93.977% (16961/18048)\n",
            "150 391 Loss: 0.170 | Train Acc: 93.957% (18160/19328)\n",
            "160 391 Loss: 0.170 | Train Acc: 93.983% (19368/20608)\n",
            "170 391 Loss: 0.171 | Train Acc: 93.928% (20559/21888)\n",
            "180 391 Loss: 0.172 | Train Acc: 93.888% (21752/23168)\n",
            "190 391 Loss: 0.173 | Train Acc: 93.865% (22948/24448)\n",
            "200 391 Loss: 0.173 | Train Acc: 93.859% (24148/25728)\n",
            "210 391 Loss: 0.174 | Train Acc: 93.865% (25351/27008)\n",
            "220 391 Loss: 0.174 | Train Acc: 93.849% (26548/28288)\n",
            "230 391 Loss: 0.175 | Train Acc: 93.794% (27733/29568)\n",
            "240 391 Loss: 0.176 | Train Acc: 93.789% (28932/30848)\n",
            "250 391 Loss: 0.177 | Train Acc: 93.725% (30112/32128)\n",
            "260 391 Loss: 0.177 | Train Acc: 93.726% (31312/33408)\n",
            "270 391 Loss: 0.178 | Train Acc: 93.710% (32506/34688)\n",
            "280 391 Loss: 0.178 | Train Acc: 93.708% (33705/35968)\n",
            "290 391 Loss: 0.177 | Train Acc: 93.715% (34907/37248)\n",
            "300 391 Loss: 0.179 | Train Acc: 93.698% (36100/38528)\n",
            "310 391 Loss: 0.179 | Train Acc: 93.675% (37290/39808)\n",
            "320 391 Loss: 0.181 | Train Acc: 93.614% (38464/41088)\n",
            "330 391 Loss: 0.182 | Train Acc: 93.580% (39648/42368)\n",
            "340 391 Loss: 0.183 | Train Acc: 93.555% (40835/43648)\n",
            "350 391 Loss: 0.183 | Train Acc: 93.561% (42035/44928)\n",
            "360 391 Loss: 0.183 | Train Acc: 93.527% (43217/46208)\n",
            "370 391 Loss: 0.183 | Train Acc: 93.565% (44432/47488)\n",
            "380 391 Loss: 0.183 | Train Acc: 93.557% (45626/48768)\n",
            "390 391 Loss: 0.183 | Train Acc: 93.532% (46766/50000)\n",
            "0 100 Loss: 0.214 | Test Acc: 93.000% (93/100)\n",
            "10 100 Loss: 0.265 | Test Acc: 91.455% (1006/1100)\n",
            "20 100 Loss: 0.290 | Test Acc: 90.381% (1898/2100)\n",
            "30 100 Loss: 0.293 | Test Acc: 90.161% (2795/3100)\n",
            "40 100 Loss: 0.297 | Test Acc: 90.366% (3705/4100)\n",
            "50 100 Loss: 0.297 | Test Acc: 90.451% (4613/5100)\n",
            "60 100 Loss: 0.293 | Test Acc: 90.639% (5529/6100)\n",
            "70 100 Loss: 0.291 | Test Acc: 90.761% (6444/7100)\n",
            "80 100 Loss: 0.286 | Test Acc: 90.852% (7359/8100)\n",
            "90 100 Loss: 0.290 | Test Acc: 90.648% (8249/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 108\n",
            "0 391 Loss: 0.199 | Train Acc: 91.406% (117/128)\n",
            "10 391 Loss: 0.166 | Train Acc: 94.389% (1329/1408)\n",
            "20 391 Loss: 0.173 | Train Acc: 94.196% (2532/2688)\n",
            "30 391 Loss: 0.176 | Train Acc: 94.153% (3736/3968)\n",
            "40 391 Loss: 0.170 | Train Acc: 94.207% (4944/5248)\n",
            "50 391 Loss: 0.169 | Train Acc: 94.225% (6151/6528)\n",
            "60 391 Loss: 0.169 | Train Acc: 94.262% (7360/7808)\n",
            "70 391 Loss: 0.167 | Train Acc: 94.267% (8567/9088)\n",
            "80 391 Loss: 0.164 | Train Acc: 94.377% (9785/10368)\n",
            "90 391 Loss: 0.164 | Train Acc: 94.377% (10993/11648)\n",
            "100 391 Loss: 0.164 | Train Acc: 94.330% (12195/12928)\n",
            "110 391 Loss: 0.164 | Train Acc: 94.320% (13401/14208)\n",
            "120 391 Loss: 0.164 | Train Acc: 94.325% (14609/15488)\n",
            "130 391 Loss: 0.166 | Train Acc: 94.293% (15811/16768)\n",
            "140 391 Loss: 0.165 | Train Acc: 94.348% (17028/18048)\n",
            "150 391 Loss: 0.167 | Train Acc: 94.278% (18222/19328)\n",
            "160 391 Loss: 0.168 | Train Acc: 94.260% (19425/20608)\n",
            "170 391 Loss: 0.171 | Train Acc: 94.170% (20612/21888)\n",
            "180 391 Loss: 0.172 | Train Acc: 94.104% (21802/23168)\n",
            "190 391 Loss: 0.173 | Train Acc: 94.069% (22998/24448)\n",
            "200 391 Loss: 0.173 | Train Acc: 94.057% (24199/25728)\n",
            "210 391 Loss: 0.173 | Train Acc: 94.057% (25403/27008)\n",
            "220 391 Loss: 0.175 | Train Acc: 93.990% (26588/28288)\n",
            "230 391 Loss: 0.176 | Train Acc: 93.956% (27781/29568)\n",
            "240 391 Loss: 0.176 | Train Acc: 93.928% (28975/30848)\n",
            "250 391 Loss: 0.175 | Train Acc: 93.949% (30184/32128)\n",
            "260 391 Loss: 0.175 | Train Acc: 93.948% (31386/33408)\n",
            "270 391 Loss: 0.174 | Train Acc: 93.966% (32595/34688)\n",
            "280 391 Loss: 0.173 | Train Acc: 93.970% (33799/35968)\n",
            "290 391 Loss: 0.173 | Train Acc: 93.978% (35005/37248)\n",
            "300 391 Loss: 0.174 | Train Acc: 93.939% (36193/38528)\n",
            "310 391 Loss: 0.174 | Train Acc: 93.946% (37398/39808)\n",
            "320 391 Loss: 0.174 | Train Acc: 93.947% (38601/41088)\n",
            "330 391 Loss: 0.174 | Train Acc: 93.941% (39801/42368)\n",
            "340 391 Loss: 0.174 | Train Acc: 93.952% (41008/43648)\n",
            "350 391 Loss: 0.174 | Train Acc: 93.953% (42211/44928)\n",
            "360 391 Loss: 0.174 | Train Acc: 93.960% (43417/46208)\n",
            "370 391 Loss: 0.174 | Train Acc: 93.931% (44606/47488)\n",
            "380 391 Loss: 0.175 | Train Acc: 93.904% (45795/48768)\n",
            "390 391 Loss: 0.176 | Train Acc: 93.872% (46936/50000)\n",
            "0 100 Loss: 0.291 | Test Acc: 90.000% (90/100)\n",
            "10 100 Loss: 0.346 | Test Acc: 88.091% (969/1100)\n",
            "20 100 Loss: 0.385 | Test Acc: 87.667% (1841/2100)\n",
            "30 100 Loss: 0.401 | Test Acc: 87.452% (2711/3100)\n",
            "40 100 Loss: 0.389 | Test Acc: 87.780% (3599/4100)\n",
            "50 100 Loss: 0.381 | Test Acc: 88.157% (4496/5100)\n",
            "60 100 Loss: 0.379 | Test Acc: 88.148% (5377/6100)\n",
            "70 100 Loss: 0.375 | Test Acc: 88.225% (6264/7100)\n",
            "80 100 Loss: 0.371 | Test Acc: 88.420% (7162/8100)\n",
            "90 100 Loss: 0.368 | Test Acc: 88.538% (8057/9100)\n",
            "\n",
            "Epoch: 109\n",
            "0 391 Loss: 0.233 | Train Acc: 92.188% (118/128)\n",
            "10 391 Loss: 0.191 | Train Acc: 93.821% (1321/1408)\n",
            "20 391 Loss: 0.177 | Train Acc: 93.973% (2526/2688)\n",
            "30 391 Loss: 0.173 | Train Acc: 94.002% (3730/3968)\n",
            "40 391 Loss: 0.170 | Train Acc: 94.169% (4942/5248)\n",
            "50 391 Loss: 0.170 | Train Acc: 94.164% (6147/6528)\n",
            "60 391 Loss: 0.171 | Train Acc: 94.121% (7349/7808)\n",
            "70 391 Loss: 0.173 | Train Acc: 94.113% (8553/9088)\n",
            "80 391 Loss: 0.174 | Train Acc: 94.059% (9752/10368)\n",
            "90 391 Loss: 0.173 | Train Acc: 94.025% (10952/11648)\n",
            "100 391 Loss: 0.171 | Train Acc: 94.129% (12169/12928)\n",
            "110 391 Loss: 0.176 | Train Acc: 93.982% (13353/14208)\n",
            "120 391 Loss: 0.175 | Train Acc: 93.995% (14558/15488)\n",
            "130 391 Loss: 0.174 | Train Acc: 94.030% (15767/16768)\n",
            "140 391 Loss: 0.173 | Train Acc: 94.071% (16978/18048)\n",
            "150 391 Loss: 0.173 | Train Acc: 94.050% (18178/19328)\n",
            "160 391 Loss: 0.173 | Train Acc: 94.017% (19375/20608)\n",
            "170 391 Loss: 0.174 | Train Acc: 93.992% (20573/21888)\n",
            "180 391 Loss: 0.175 | Train Acc: 93.944% (21765/23168)\n",
            "190 391 Loss: 0.175 | Train Acc: 93.942% (22967/24448)\n",
            "200 391 Loss: 0.176 | Train Acc: 93.929% (24166/25728)\n",
            "210 391 Loss: 0.175 | Train Acc: 93.957% (25376/27008)\n",
            "220 391 Loss: 0.175 | Train Acc: 93.948% (26576/28288)\n",
            "230 391 Loss: 0.174 | Train Acc: 93.963% (27783/29568)\n",
            "240 391 Loss: 0.174 | Train Acc: 93.967% (28987/30848)\n",
            "250 391 Loss: 0.174 | Train Acc: 93.959% (30187/32128)\n",
            "260 391 Loss: 0.175 | Train Acc: 93.945% (31385/33408)\n",
            "270 391 Loss: 0.175 | Train Acc: 93.923% (32580/34688)\n",
            "280 391 Loss: 0.175 | Train Acc: 93.939% (33788/35968)\n",
            "290 391 Loss: 0.176 | Train Acc: 93.884% (34970/37248)\n",
            "300 391 Loss: 0.176 | Train Acc: 93.895% (36176/38528)\n",
            "310 391 Loss: 0.176 | Train Acc: 93.876% (37370/39808)\n",
            "320 391 Loss: 0.177 | Train Acc: 93.842% (38558/41088)\n",
            "330 391 Loss: 0.177 | Train Acc: 93.851% (39763/42368)\n",
            "340 391 Loss: 0.178 | Train Acc: 93.796% (40940/43648)\n",
            "350 391 Loss: 0.178 | Train Acc: 93.783% (42135/44928)\n",
            "360 391 Loss: 0.179 | Train Acc: 93.752% (43321/46208)\n",
            "370 391 Loss: 0.179 | Train Acc: 93.725% (44508/47488)\n",
            "380 391 Loss: 0.180 | Train Acc: 93.711% (45701/48768)\n",
            "390 391 Loss: 0.180 | Train Acc: 93.716% (46858/50000)\n",
            "0 100 Loss: 0.315 | Test Acc: 86.000% (86/100)\n",
            "10 100 Loss: 0.378 | Test Acc: 88.545% (974/1100)\n",
            "20 100 Loss: 0.381 | Test Acc: 87.524% (1838/2100)\n",
            "30 100 Loss: 0.392 | Test Acc: 87.581% (2715/3100)\n",
            "40 100 Loss: 0.396 | Test Acc: 87.415% (3584/4100)\n",
            "50 100 Loss: 0.393 | Test Acc: 87.608% (4468/5100)\n",
            "60 100 Loss: 0.386 | Test Acc: 87.639% (5346/6100)\n",
            "70 100 Loss: 0.387 | Test Acc: 87.648% (6223/7100)\n",
            "80 100 Loss: 0.383 | Test Acc: 87.840% (7115/8100)\n",
            "90 100 Loss: 0.381 | Test Acc: 87.879% (7997/9100)\n",
            "\n",
            "Epoch: 110\n",
            "0 391 Loss: 0.099 | Train Acc: 97.656% (125/128)\n",
            "10 391 Loss: 0.167 | Train Acc: 94.034% (1324/1408)\n",
            "20 391 Loss: 0.162 | Train Acc: 94.122% (2530/2688)\n",
            "30 391 Loss: 0.163 | Train Acc: 94.027% (3731/3968)\n",
            "40 391 Loss: 0.166 | Train Acc: 93.979% (4932/5248)\n",
            "50 391 Loss: 0.167 | Train Acc: 93.995% (6136/6528)\n",
            "60 391 Loss: 0.164 | Train Acc: 94.032% (7342/7808)\n",
            "70 391 Loss: 0.163 | Train Acc: 94.113% (8553/9088)\n",
            "80 391 Loss: 0.162 | Train Acc: 94.174% (9764/10368)\n",
            "90 391 Loss: 0.158 | Train Acc: 94.342% (10989/11648)\n",
            "100 391 Loss: 0.157 | Train Acc: 94.377% (12201/12928)\n",
            "110 391 Loss: 0.155 | Train Acc: 94.440% (13418/14208)\n",
            "120 391 Loss: 0.155 | Train Acc: 94.402% (14621/15488)\n",
            "130 391 Loss: 0.156 | Train Acc: 94.412% (15831/16768)\n",
            "140 391 Loss: 0.158 | Train Acc: 94.376% (17033/18048)\n",
            "150 391 Loss: 0.161 | Train Acc: 94.273% (18221/19328)\n",
            "160 391 Loss: 0.164 | Train Acc: 94.196% (19412/20608)\n",
            "170 391 Loss: 0.165 | Train Acc: 94.088% (20594/21888)\n",
            "180 391 Loss: 0.166 | Train Acc: 94.095% (21800/23168)\n",
            "190 391 Loss: 0.166 | Train Acc: 94.094% (23004/24448)\n",
            "200 391 Loss: 0.166 | Train Acc: 94.088% (24207/25728)\n",
            "210 391 Loss: 0.166 | Train Acc: 94.076% (25408/27008)\n",
            "220 391 Loss: 0.166 | Train Acc: 94.089% (26616/28288)\n",
            "230 391 Loss: 0.167 | Train Acc: 94.071% (27815/29568)\n",
            "240 391 Loss: 0.167 | Train Acc: 94.087% (29024/30848)\n",
            "250 391 Loss: 0.169 | Train Acc: 94.055% (30218/32128)\n",
            "260 391 Loss: 0.169 | Train Acc: 94.046% (31419/33408)\n",
            "270 391 Loss: 0.170 | Train Acc: 94.027% (32616/34688)\n",
            "280 391 Loss: 0.171 | Train Acc: 94.014% (33815/35968)\n",
            "290 391 Loss: 0.172 | Train Acc: 93.965% (35000/37248)\n",
            "300 391 Loss: 0.173 | Train Acc: 93.945% (36195/38528)\n",
            "310 391 Loss: 0.173 | Train Acc: 93.956% (37402/39808)\n",
            "320 391 Loss: 0.173 | Train Acc: 93.969% (38610/41088)\n",
            "330 391 Loss: 0.173 | Train Acc: 93.979% (39817/42368)\n",
            "340 391 Loss: 0.172 | Train Acc: 93.995% (41027/43648)\n",
            "350 391 Loss: 0.172 | Train Acc: 93.990% (42228/44928)\n",
            "360 391 Loss: 0.171 | Train Acc: 94.016% (43443/46208)\n",
            "370 391 Loss: 0.172 | Train Acc: 93.992% (44635/47488)\n",
            "380 391 Loss: 0.173 | Train Acc: 93.980% (45832/48768)\n",
            "390 391 Loss: 0.173 | Train Acc: 93.962% (46981/50000)\n",
            "0 100 Loss: 0.464 | Test Acc: 85.000% (85/100)\n",
            "10 100 Loss: 0.374 | Test Acc: 87.545% (963/1100)\n",
            "20 100 Loss: 0.400 | Test Acc: 87.714% (1842/2100)\n",
            "30 100 Loss: 0.419 | Test Acc: 87.806% (2722/3100)\n",
            "40 100 Loss: 0.415 | Test Acc: 87.707% (3596/4100)\n",
            "50 100 Loss: 0.403 | Test Acc: 87.843% (4480/5100)\n",
            "60 100 Loss: 0.396 | Test Acc: 87.869% (5360/6100)\n",
            "70 100 Loss: 0.390 | Test Acc: 88.070% (6253/7100)\n",
            "80 100 Loss: 0.385 | Test Acc: 88.259% (7149/8100)\n",
            "90 100 Loss: 0.382 | Test Acc: 88.286% (8034/9100)\n",
            "\n",
            "Epoch: 111\n",
            "0 391 Loss: 0.096 | Train Acc: 96.875% (124/128)\n",
            "10 391 Loss: 0.162 | Train Acc: 94.531% (1331/1408)\n",
            "20 391 Loss: 0.162 | Train Acc: 94.457% (2539/2688)\n",
            "30 391 Loss: 0.156 | Train Acc: 94.708% (3758/3968)\n",
            "40 391 Loss: 0.155 | Train Acc: 94.684% (4969/5248)\n",
            "50 391 Loss: 0.151 | Train Acc: 94.853% (6192/6528)\n",
            "60 391 Loss: 0.152 | Train Acc: 94.762% (7399/7808)\n",
            "70 391 Loss: 0.150 | Train Acc: 94.817% (8617/9088)\n",
            "80 391 Loss: 0.150 | Train Acc: 94.830% (9832/10368)\n",
            "90 391 Loss: 0.154 | Train Acc: 94.703% (11031/11648)\n",
            "100 391 Loss: 0.155 | Train Acc: 94.725% (12246/12928)\n",
            "110 391 Loss: 0.158 | Train Acc: 94.637% (13446/14208)\n",
            "120 391 Loss: 0.161 | Train Acc: 94.551% (14644/15488)\n",
            "130 391 Loss: 0.160 | Train Acc: 94.567% (15857/16768)\n",
            "140 391 Loss: 0.160 | Train Acc: 94.570% (17068/18048)\n",
            "150 391 Loss: 0.160 | Train Acc: 94.511% (18267/19328)\n",
            "160 391 Loss: 0.161 | Train Acc: 94.449% (19464/20608)\n",
            "170 391 Loss: 0.162 | Train Acc: 94.390% (20660/21888)\n",
            "180 391 Loss: 0.163 | Train Acc: 94.359% (21861/23168)\n",
            "190 391 Loss: 0.165 | Train Acc: 94.261% (23045/24448)\n",
            "200 391 Loss: 0.164 | Train Acc: 94.279% (24256/25728)\n",
            "210 391 Loss: 0.164 | Train Acc: 94.250% (25455/27008)\n",
            "220 391 Loss: 0.166 | Train Acc: 94.185% (26643/28288)\n",
            "230 391 Loss: 0.167 | Train Acc: 94.159% (27841/29568)\n",
            "240 391 Loss: 0.166 | Train Acc: 94.181% (29053/30848)\n",
            "250 391 Loss: 0.167 | Train Acc: 94.164% (30253/32128)\n",
            "260 391 Loss: 0.166 | Train Acc: 94.151% (31454/33408)\n",
            "270 391 Loss: 0.168 | Train Acc: 94.102% (32642/34688)\n",
            "280 391 Loss: 0.169 | Train Acc: 94.053% (33829/35968)\n",
            "290 391 Loss: 0.169 | Train Acc: 94.053% (35033/37248)\n",
            "300 391 Loss: 0.169 | Train Acc: 94.098% (36254/38528)\n",
            "310 391 Loss: 0.169 | Train Acc: 94.094% (37457/39808)\n",
            "320 391 Loss: 0.169 | Train Acc: 94.096% (38662/41088)\n",
            "330 391 Loss: 0.170 | Train Acc: 94.085% (39862/42368)\n",
            "340 391 Loss: 0.171 | Train Acc: 94.043% (41048/43648)\n",
            "350 391 Loss: 0.171 | Train Acc: 94.028% (42245/44928)\n",
            "360 391 Loss: 0.172 | Train Acc: 94.003% (43437/46208)\n",
            "370 391 Loss: 0.172 | Train Acc: 93.984% (44631/47488)\n",
            "380 391 Loss: 0.172 | Train Acc: 94.000% (45842/48768)\n",
            "390 391 Loss: 0.172 | Train Acc: 93.996% (46998/50000)\n",
            "0 100 Loss: 0.346 | Test Acc: 89.000% (89/100)\n",
            "10 100 Loss: 0.353 | Test Acc: 88.545% (974/1100)\n",
            "20 100 Loss: 0.383 | Test Acc: 88.143% (1851/2100)\n",
            "30 100 Loss: 0.374 | Test Acc: 88.097% (2731/3100)\n",
            "40 100 Loss: 0.365 | Test Acc: 88.390% (3624/4100)\n",
            "50 100 Loss: 0.365 | Test Acc: 88.549% (4516/5100)\n",
            "60 100 Loss: 0.360 | Test Acc: 88.836% (5419/6100)\n",
            "70 100 Loss: 0.355 | Test Acc: 88.859% (6309/7100)\n",
            "80 100 Loss: 0.349 | Test Acc: 88.864% (7198/8100)\n",
            "90 100 Loss: 0.349 | Test Acc: 88.813% (8082/9100)\n",
            "\n",
            "Epoch: 112\n",
            "0 391 Loss: 0.081 | Train Acc: 96.094% (123/128)\n",
            "10 391 Loss: 0.148 | Train Acc: 94.531% (1331/1408)\n",
            "20 391 Loss: 0.141 | Train Acc: 94.829% (2549/2688)\n",
            "30 391 Loss: 0.149 | Train Acc: 94.582% (3753/3968)\n",
            "40 391 Loss: 0.147 | Train Acc: 94.722% (4971/5248)\n",
            "50 391 Loss: 0.148 | Train Acc: 94.838% (6191/6528)\n",
            "60 391 Loss: 0.150 | Train Acc: 94.800% (7402/7808)\n",
            "70 391 Loss: 0.153 | Train Acc: 94.729% (8609/9088)\n",
            "80 391 Loss: 0.155 | Train Acc: 94.676% (9816/10368)\n",
            "90 391 Loss: 0.156 | Train Acc: 94.617% (11021/11648)\n",
            "100 391 Loss: 0.156 | Train Acc: 94.670% (12239/12928)\n",
            "110 391 Loss: 0.159 | Train Acc: 94.573% (13437/14208)\n",
            "120 391 Loss: 0.162 | Train Acc: 94.473% (14632/15488)\n",
            "130 391 Loss: 0.162 | Train Acc: 94.466% (15840/16768)\n",
            "140 391 Loss: 0.163 | Train Acc: 94.387% (17035/18048)\n",
            "150 391 Loss: 0.163 | Train Acc: 94.350% (18236/19328)\n",
            "160 391 Loss: 0.162 | Train Acc: 94.357% (19445/20608)\n",
            "170 391 Loss: 0.162 | Train Acc: 94.330% (20647/21888)\n",
            "180 391 Loss: 0.165 | Train Acc: 94.277% (21842/23168)\n",
            "190 391 Loss: 0.165 | Train Acc: 94.261% (23045/24448)\n",
            "200 391 Loss: 0.166 | Train Acc: 94.213% (24239/25728)\n",
            "210 391 Loss: 0.166 | Train Acc: 94.205% (25443/27008)\n",
            "220 391 Loss: 0.165 | Train Acc: 94.277% (26669/28288)\n",
            "230 391 Loss: 0.165 | Train Acc: 94.281% (27877/29568)\n",
            "240 391 Loss: 0.165 | Train Acc: 94.278% (29083/30848)\n",
            "250 391 Loss: 0.166 | Train Acc: 94.264% (30285/32128)\n",
            "260 391 Loss: 0.167 | Train Acc: 94.226% (31479/33408)\n",
            "270 391 Loss: 0.167 | Train Acc: 94.226% (32685/34688)\n",
            "280 391 Loss: 0.168 | Train Acc: 94.203% (33883/35968)\n",
            "290 391 Loss: 0.169 | Train Acc: 94.171% (35077/37248)\n",
            "300 391 Loss: 0.171 | Train Acc: 94.137% (36269/38528)\n",
            "310 391 Loss: 0.171 | Train Acc: 94.119% (37467/39808)\n",
            "320 391 Loss: 0.171 | Train Acc: 94.113% (38669/41088)\n",
            "330 391 Loss: 0.172 | Train Acc: 94.078% (39859/42368)\n",
            "340 391 Loss: 0.172 | Train Acc: 94.091% (41069/43648)\n",
            "350 391 Loss: 0.171 | Train Acc: 94.106% (42280/44928)\n",
            "360 391 Loss: 0.172 | Train Acc: 94.072% (43469/46208)\n",
            "370 391 Loss: 0.173 | Train Acc: 94.045% (44660/47488)\n",
            "380 391 Loss: 0.173 | Train Acc: 94.072% (45877/48768)\n",
            "390 391 Loss: 0.173 | Train Acc: 94.062% (47031/50000)\n",
            "0 100 Loss: 0.431 | Test Acc: 87.000% (87/100)\n",
            "10 100 Loss: 0.321 | Test Acc: 89.000% (979/1100)\n",
            "20 100 Loss: 0.358 | Test Acc: 88.810% (1865/2100)\n",
            "30 100 Loss: 0.357 | Test Acc: 89.129% (2763/3100)\n",
            "40 100 Loss: 0.348 | Test Acc: 89.244% (3659/4100)\n",
            "50 100 Loss: 0.341 | Test Acc: 89.255% (4552/5100)\n",
            "60 100 Loss: 0.336 | Test Acc: 89.295% (5447/6100)\n",
            "70 100 Loss: 0.338 | Test Acc: 89.296% (6340/7100)\n",
            "80 100 Loss: 0.329 | Test Acc: 89.642% (7261/8100)\n",
            "90 100 Loss: 0.325 | Test Acc: 89.769% (8169/9100)\n",
            "\n",
            "Epoch: 113\n",
            "0 391 Loss: 0.150 | Train Acc: 96.094% (123/128)\n",
            "10 391 Loss: 0.165 | Train Acc: 93.750% (1320/1408)\n",
            "20 391 Loss: 0.167 | Train Acc: 93.862% (2523/2688)\n",
            "30 391 Loss: 0.158 | Train Acc: 94.582% (3753/3968)\n",
            "40 391 Loss: 0.153 | Train Acc: 94.703% (4970/5248)\n",
            "50 391 Loss: 0.154 | Train Acc: 94.730% (6184/6528)\n",
            "60 391 Loss: 0.153 | Train Acc: 94.749% (7398/7808)\n",
            "70 391 Loss: 0.151 | Train Acc: 94.850% (8620/9088)\n",
            "80 391 Loss: 0.151 | Train Acc: 94.850% (9834/10368)\n",
            "90 391 Loss: 0.149 | Train Acc: 94.875% (11051/11648)\n",
            "100 391 Loss: 0.151 | Train Acc: 94.848% (12262/12928)\n",
            "110 391 Loss: 0.150 | Train Acc: 94.890% (13482/14208)\n",
            "120 391 Loss: 0.150 | Train Acc: 94.886% (14696/15488)\n",
            "130 391 Loss: 0.151 | Train Acc: 94.823% (15900/16768)\n",
            "140 391 Loss: 0.151 | Train Acc: 94.758% (17102/18048)\n",
            "150 391 Loss: 0.150 | Train Acc: 94.759% (18315/19328)\n",
            "160 391 Loss: 0.151 | Train Acc: 94.750% (19526/20608)\n",
            "170 391 Loss: 0.151 | Train Acc: 94.723% (20733/21888)\n",
            "180 391 Loss: 0.152 | Train Acc: 94.687% (21937/23168)\n",
            "190 391 Loss: 0.152 | Train Acc: 94.691% (23150/24448)\n",
            "200 391 Loss: 0.155 | Train Acc: 94.597% (24338/25728)\n",
            "210 391 Loss: 0.155 | Train Acc: 94.602% (25550/27008)\n",
            "220 391 Loss: 0.155 | Train Acc: 94.591% (26758/28288)\n",
            "230 391 Loss: 0.155 | Train Acc: 94.572% (27963/29568)\n",
            "240 391 Loss: 0.155 | Train Acc: 94.573% (29174/30848)\n",
            "250 391 Loss: 0.155 | Train Acc: 94.600% (30393/32128)\n",
            "260 391 Loss: 0.155 | Train Acc: 94.591% (31601/33408)\n",
            "270 391 Loss: 0.157 | Train Acc: 94.557% (32800/34688)\n",
            "280 391 Loss: 0.157 | Train Acc: 94.579% (34018/35968)\n",
            "290 391 Loss: 0.158 | Train Acc: 94.542% (35215/37248)\n",
            "300 391 Loss: 0.158 | Train Acc: 94.539% (36424/38528)\n",
            "310 391 Loss: 0.158 | Train Acc: 94.564% (37644/39808)\n",
            "320 391 Loss: 0.158 | Train Acc: 94.565% (38855/41088)\n",
            "330 391 Loss: 0.159 | Train Acc: 94.534% (40052/42368)\n",
            "340 391 Loss: 0.159 | Train Acc: 94.511% (41252/43648)\n",
            "350 391 Loss: 0.161 | Train Acc: 94.471% (42444/44928)\n",
            "360 391 Loss: 0.162 | Train Acc: 94.421% (43630/46208)\n",
            "370 391 Loss: 0.163 | Train Acc: 94.378% (44818/47488)\n",
            "380 391 Loss: 0.163 | Train Acc: 94.373% (46024/48768)\n",
            "390 391 Loss: 0.163 | Train Acc: 94.372% (47186/50000)\n",
            "0 100 Loss: 0.521 | Test Acc: 84.000% (84/100)\n",
            "10 100 Loss: 0.456 | Test Acc: 85.727% (943/1100)\n",
            "20 100 Loss: 0.450 | Test Acc: 86.286% (1812/2100)\n",
            "30 100 Loss: 0.439 | Test Acc: 86.290% (2675/3100)\n",
            "40 100 Loss: 0.419 | Test Acc: 86.683% (3554/4100)\n",
            "50 100 Loss: 0.410 | Test Acc: 86.941% (4434/5100)\n",
            "60 100 Loss: 0.410 | Test Acc: 86.934% (5303/6100)\n",
            "70 100 Loss: 0.403 | Test Acc: 87.169% (6189/7100)\n",
            "80 100 Loss: 0.400 | Test Acc: 87.235% (7066/8100)\n",
            "90 100 Loss: 0.397 | Test Acc: 87.385% (7952/9100)\n",
            "\n",
            "Epoch: 114\n",
            "0 391 Loss: 0.146 | Train Acc: 95.312% (122/128)\n",
            "10 391 Loss: 0.140 | Train Acc: 95.099% (1339/1408)\n",
            "20 391 Loss: 0.138 | Train Acc: 95.089% (2556/2688)\n",
            "30 391 Loss: 0.137 | Train Acc: 95.060% (3772/3968)\n",
            "40 391 Loss: 0.136 | Train Acc: 95.370% (5005/5248)\n",
            "50 391 Loss: 0.137 | Train Acc: 95.221% (6216/6528)\n",
            "60 391 Loss: 0.137 | Train Acc: 95.184% (7432/7808)\n",
            "70 391 Loss: 0.139 | Train Acc: 95.235% (8655/9088)\n",
            "80 391 Loss: 0.137 | Train Acc: 95.322% (9883/10368)\n",
            "90 391 Loss: 0.140 | Train Acc: 95.312% (11102/11648)\n",
            "100 391 Loss: 0.141 | Train Acc: 95.196% (12307/12928)\n",
            "110 391 Loss: 0.142 | Train Acc: 95.137% (13517/14208)\n",
            "120 391 Loss: 0.144 | Train Acc: 95.054% (14722/15488)\n",
            "130 391 Loss: 0.147 | Train Acc: 94.937% (15919/16768)\n",
            "140 391 Loss: 0.147 | Train Acc: 94.947% (17136/18048)\n",
            "150 391 Loss: 0.148 | Train Acc: 94.945% (18351/19328)\n",
            "160 391 Loss: 0.150 | Train Acc: 94.885% (19554/20608)\n",
            "170 391 Loss: 0.150 | Train Acc: 94.860% (20763/21888)\n",
            "180 391 Loss: 0.151 | Train Acc: 94.816% (21967/23168)\n",
            "190 391 Loss: 0.150 | Train Acc: 94.842% (23187/24448)\n",
            "200 391 Loss: 0.152 | Train Acc: 94.780% (24385/25728)\n",
            "210 391 Loss: 0.153 | Train Acc: 94.724% (25583/27008)\n",
            "220 391 Loss: 0.154 | Train Acc: 94.683% (26784/28288)\n",
            "230 391 Loss: 0.155 | Train Acc: 94.694% (27999/29568)\n",
            "240 391 Loss: 0.154 | Train Acc: 94.713% (29217/30848)\n",
            "250 391 Loss: 0.154 | Train Acc: 94.687% (30421/32128)\n",
            "260 391 Loss: 0.154 | Train Acc: 94.693% (31635/33408)\n",
            "270 391 Loss: 0.154 | Train Acc: 94.693% (32847/34688)\n",
            "280 391 Loss: 0.153 | Train Acc: 94.673% (34052/35968)\n",
            "290 391 Loss: 0.153 | Train Acc: 94.668% (35262/37248)\n",
            "300 391 Loss: 0.154 | Train Acc: 94.656% (36469/38528)\n",
            "310 391 Loss: 0.155 | Train Acc: 94.624% (37668/39808)\n",
            "320 391 Loss: 0.156 | Train Acc: 94.560% (38853/41088)\n",
            "330 391 Loss: 0.159 | Train Acc: 94.479% (40029/42368)\n",
            "340 391 Loss: 0.160 | Train Acc: 94.444% (41223/43648)\n",
            "350 391 Loss: 0.161 | Train Acc: 94.404% (42414/44928)\n",
            "360 391 Loss: 0.162 | Train Acc: 94.375% (43609/46208)\n",
            "370 391 Loss: 0.162 | Train Acc: 94.378% (44818/47488)\n",
            "380 391 Loss: 0.162 | Train Acc: 94.388% (46031/48768)\n",
            "390 391 Loss: 0.163 | Train Acc: 94.350% (47175/50000)\n",
            "0 100 Loss: 0.343 | Test Acc: 88.000% (88/100)\n",
            "10 100 Loss: 0.332 | Test Acc: 89.091% (980/1100)\n",
            "20 100 Loss: 0.355 | Test Acc: 88.381% (1856/2100)\n",
            "30 100 Loss: 0.358 | Test Acc: 89.097% (2762/3100)\n",
            "40 100 Loss: 0.351 | Test Acc: 89.561% (3672/4100)\n",
            "50 100 Loss: 0.351 | Test Acc: 89.529% (4566/5100)\n",
            "60 100 Loss: 0.346 | Test Acc: 89.672% (5470/6100)\n",
            "70 100 Loss: 0.346 | Test Acc: 89.662% (6366/7100)\n",
            "80 100 Loss: 0.341 | Test Acc: 89.704% (7266/8100)\n",
            "90 100 Loss: 0.338 | Test Acc: 89.780% (8170/9100)\n",
            "\n",
            "Epoch: 115\n",
            "0 391 Loss: 0.117 | Train Acc: 98.438% (126/128)\n",
            "10 391 Loss: 0.141 | Train Acc: 95.241% (1341/1408)\n",
            "20 391 Loss: 0.164 | Train Acc: 94.606% (2543/2688)\n",
            "30 391 Loss: 0.158 | Train Acc: 94.758% (3760/3968)\n",
            "40 391 Loss: 0.151 | Train Acc: 94.798% (4975/5248)\n",
            "50 391 Loss: 0.147 | Train Acc: 94.899% (6195/6528)\n",
            "60 391 Loss: 0.148 | Train Acc: 94.864% (7407/7808)\n",
            "70 391 Loss: 0.145 | Train Acc: 94.982% (8632/9088)\n",
            "80 391 Loss: 0.147 | Train Acc: 94.907% (9840/10368)\n",
            "90 391 Loss: 0.147 | Train Acc: 94.892% (11053/11648)\n",
            "100 391 Loss: 0.148 | Train Acc: 94.856% (12263/12928)\n",
            "110 391 Loss: 0.149 | Train Acc: 94.827% (13473/14208)\n",
            "120 391 Loss: 0.148 | Train Acc: 94.873% (14694/15488)\n",
            "130 391 Loss: 0.147 | Train Acc: 94.931% (15918/16768)\n",
            "140 391 Loss: 0.146 | Train Acc: 94.958% (17138/18048)\n",
            "150 391 Loss: 0.146 | Train Acc: 94.950% (18352/19328)\n",
            "160 391 Loss: 0.146 | Train Acc: 94.919% (19561/20608)\n",
            "170 391 Loss: 0.148 | Train Acc: 94.851% (20761/21888)\n",
            "180 391 Loss: 0.150 | Train Acc: 94.825% (21969/23168)\n",
            "190 391 Loss: 0.152 | Train Acc: 94.764% (23168/24448)\n",
            "200 391 Loss: 0.156 | Train Acc: 94.660% (24354/25728)\n",
            "210 391 Loss: 0.157 | Train Acc: 94.594% (25548/27008)\n",
            "220 391 Loss: 0.157 | Train Acc: 94.595% (26759/28288)\n",
            "230 391 Loss: 0.157 | Train Acc: 94.612% (27975/29568)\n",
            "240 391 Loss: 0.157 | Train Acc: 94.612% (29186/30848)\n",
            "250 391 Loss: 0.157 | Train Acc: 94.625% (30401/32128)\n",
            "260 391 Loss: 0.157 | Train Acc: 94.627% (31613/33408)\n",
            "270 391 Loss: 0.158 | Train Acc: 94.586% (32810/34688)\n",
            "280 391 Loss: 0.158 | Train Acc: 94.581% (34019/35968)\n",
            "290 391 Loss: 0.158 | Train Acc: 94.563% (35223/37248)\n",
            "300 391 Loss: 0.157 | Train Acc: 94.570% (36436/38528)\n",
            "310 391 Loss: 0.157 | Train Acc: 94.581% (37651/39808)\n",
            "320 391 Loss: 0.157 | Train Acc: 94.558% (38852/41088)\n",
            "330 391 Loss: 0.158 | Train Acc: 94.527% (40049/42368)\n",
            "340 391 Loss: 0.157 | Train Acc: 94.559% (41273/43648)\n",
            "350 391 Loss: 0.157 | Train Acc: 94.556% (42482/44928)\n",
            "360 391 Loss: 0.158 | Train Acc: 94.549% (43689/46208)\n",
            "370 391 Loss: 0.158 | Train Acc: 94.538% (44894/47488)\n",
            "380 391 Loss: 0.159 | Train Acc: 94.505% (46088/48768)\n",
            "390 391 Loss: 0.159 | Train Acc: 94.496% (47248/50000)\n",
            "0 100 Loss: 0.489 | Test Acc: 82.000% (82/100)\n",
            "10 100 Loss: 0.475 | Test Acc: 86.000% (946/1100)\n",
            "20 100 Loss: 0.454 | Test Acc: 86.190% (1810/2100)\n",
            "30 100 Loss: 0.449 | Test Acc: 86.258% (2674/3100)\n",
            "40 100 Loss: 0.435 | Test Acc: 86.463% (3545/4100)\n",
            "50 100 Loss: 0.421 | Test Acc: 86.667% (4420/5100)\n",
            "60 100 Loss: 0.419 | Test Acc: 86.721% (5290/6100)\n",
            "70 100 Loss: 0.416 | Test Acc: 86.732% (6158/7100)\n",
            "80 100 Loss: 0.410 | Test Acc: 86.827% (7033/8100)\n",
            "90 100 Loss: 0.409 | Test Acc: 86.890% (7907/9100)\n",
            "\n",
            "Epoch: 116\n",
            "0 391 Loss: 0.099 | Train Acc: 96.875% (124/128)\n",
            "10 391 Loss: 0.143 | Train Acc: 95.312% (1342/1408)\n",
            "20 391 Loss: 0.144 | Train Acc: 95.052% (2555/2688)\n",
            "30 391 Loss: 0.137 | Train Acc: 95.363% (3784/3968)\n",
            "40 391 Loss: 0.134 | Train Acc: 95.370% (5005/5248)\n",
            "50 391 Loss: 0.134 | Train Acc: 95.328% (6223/6528)\n",
            "60 391 Loss: 0.134 | Train Acc: 95.428% (7451/7808)\n",
            "70 391 Loss: 0.135 | Train Acc: 95.346% (8665/9088)\n",
            "80 391 Loss: 0.135 | Train Acc: 95.341% (9885/10368)\n",
            "90 391 Loss: 0.137 | Train Acc: 95.304% (11101/11648)\n",
            "100 391 Loss: 0.138 | Train Acc: 95.235% (12312/12928)\n",
            "110 391 Loss: 0.139 | Train Acc: 95.193% (13525/14208)\n",
            "120 391 Loss: 0.142 | Train Acc: 95.125% (14733/15488)\n",
            "130 391 Loss: 0.142 | Train Acc: 95.110% (15948/16768)\n",
            "140 391 Loss: 0.142 | Train Acc: 95.074% (17159/18048)\n",
            "150 391 Loss: 0.141 | Train Acc: 95.075% (18376/19328)\n",
            "160 391 Loss: 0.142 | Train Acc: 95.075% (19593/20608)\n",
            "170 391 Loss: 0.141 | Train Acc: 95.107% (20817/21888)\n",
            "180 391 Loss: 0.141 | Train Acc: 95.084% (22029/23168)\n",
            "190 391 Loss: 0.143 | Train Acc: 94.989% (23223/24448)\n",
            "200 391 Loss: 0.145 | Train Acc: 94.900% (24416/25728)\n",
            "210 391 Loss: 0.148 | Train Acc: 94.802% (25604/27008)\n",
            "220 391 Loss: 0.147 | Train Acc: 94.828% (26825/28288)\n",
            "230 391 Loss: 0.148 | Train Acc: 94.802% (28031/29568)\n",
            "240 391 Loss: 0.148 | Train Acc: 94.774% (29236/30848)\n",
            "250 391 Loss: 0.149 | Train Acc: 94.746% (30440/32128)\n",
            "260 391 Loss: 0.150 | Train Acc: 94.738% (31650/33408)\n",
            "270 391 Loss: 0.151 | Train Acc: 94.696% (32848/34688)\n",
            "280 391 Loss: 0.152 | Train Acc: 94.687% (34057/35968)\n",
            "290 391 Loss: 0.152 | Train Acc: 94.649% (35255/37248)\n",
            "300 391 Loss: 0.154 | Train Acc: 94.614% (36453/38528)\n",
            "310 391 Loss: 0.155 | Train Acc: 94.576% (37649/39808)\n",
            "320 391 Loss: 0.158 | Train Acc: 94.487% (38823/41088)\n",
            "330 391 Loss: 0.158 | Train Acc: 94.463% (40022/42368)\n",
            "340 391 Loss: 0.160 | Train Acc: 94.421% (41213/43648)\n",
            "350 391 Loss: 0.161 | Train Acc: 94.378% (42402/44928)\n",
            "360 391 Loss: 0.161 | Train Acc: 94.399% (43620/46208)\n",
            "370 391 Loss: 0.160 | Train Acc: 94.390% (44824/47488)\n",
            "380 391 Loss: 0.162 | Train Acc: 94.351% (46013/48768)\n",
            "390 391 Loss: 0.162 | Train Acc: 94.354% (47177/50000)\n",
            "0 100 Loss: 0.356 | Test Acc: 90.000% (90/100)\n",
            "10 100 Loss: 0.344 | Test Acc: 89.273% (982/1100)\n",
            "20 100 Loss: 0.328 | Test Acc: 89.810% (1886/2100)\n",
            "30 100 Loss: 0.340 | Test Acc: 89.516% (2775/3100)\n",
            "40 100 Loss: 0.337 | Test Acc: 89.341% (3663/4100)\n",
            "50 100 Loss: 0.324 | Test Acc: 89.627% (4571/5100)\n",
            "60 100 Loss: 0.319 | Test Acc: 89.754% (5475/6100)\n",
            "70 100 Loss: 0.315 | Test Acc: 89.930% (6385/7100)\n",
            "80 100 Loss: 0.308 | Test Acc: 90.099% (7298/8100)\n",
            "90 100 Loss: 0.311 | Test Acc: 90.088% (8198/9100)\n",
            "\n",
            "Epoch: 117\n",
            "0 391 Loss: 0.166 | Train Acc: 95.312% (122/128)\n",
            "10 391 Loss: 0.138 | Train Acc: 95.810% (1349/1408)\n",
            "20 391 Loss: 0.137 | Train Acc: 95.722% (2573/2688)\n",
            "30 391 Loss: 0.129 | Train Acc: 95.968% (3808/3968)\n",
            "40 391 Loss: 0.126 | Train Acc: 96.018% (5039/5248)\n",
            "50 391 Loss: 0.125 | Train Acc: 96.002% (6267/6528)\n",
            "60 391 Loss: 0.128 | Train Acc: 95.710% (7473/7808)\n",
            "70 391 Loss: 0.128 | Train Acc: 95.731% (8700/9088)\n",
            "80 391 Loss: 0.131 | Train Acc: 95.592% (9911/10368)\n",
            "90 391 Loss: 0.132 | Train Acc: 95.570% (11132/11648)\n",
            "100 391 Loss: 0.132 | Train Acc: 95.560% (12354/12928)\n",
            "110 391 Loss: 0.133 | Train Acc: 95.524% (13572/14208)\n",
            "120 391 Loss: 0.132 | Train Acc: 95.551% (14799/15488)\n",
            "130 391 Loss: 0.133 | Train Acc: 95.485% (16011/16768)\n",
            "140 391 Loss: 0.134 | Train Acc: 95.479% (17232/18048)\n",
            "150 391 Loss: 0.135 | Train Acc: 95.442% (18447/19328)\n",
            "160 391 Loss: 0.136 | Train Acc: 95.390% (19658/20608)\n",
            "170 391 Loss: 0.137 | Train Acc: 95.331% (20866/21888)\n",
            "180 391 Loss: 0.137 | Train Acc: 95.282% (22075/23168)\n",
            "190 391 Loss: 0.138 | Train Acc: 95.259% (23289/24448)\n",
            "200 391 Loss: 0.138 | Train Acc: 95.289% (24516/25728)\n",
            "210 391 Loss: 0.138 | Train Acc: 95.272% (25731/27008)\n",
            "220 391 Loss: 0.139 | Train Acc: 95.231% (26939/28288)\n",
            "230 391 Loss: 0.139 | Train Acc: 95.241% (28161/29568)\n",
            "240 391 Loss: 0.139 | Train Acc: 95.241% (29380/30848)\n",
            "250 391 Loss: 0.139 | Train Acc: 95.232% (30596/32128)\n",
            "260 391 Loss: 0.140 | Train Acc: 95.196% (31803/33408)\n",
            "270 391 Loss: 0.140 | Train Acc: 95.209% (33026/34688)\n",
            "280 391 Loss: 0.142 | Train Acc: 95.146% (34222/35968)\n",
            "290 391 Loss: 0.143 | Train Acc: 95.122% (35431/37248)\n",
            "300 391 Loss: 0.143 | Train Acc: 95.113% (36645/38528)\n",
            "310 391 Loss: 0.144 | Train Acc: 95.101% (37858/39808)\n",
            "320 391 Loss: 0.144 | Train Acc: 95.089% (39070/41088)\n",
            "330 391 Loss: 0.145 | Train Acc: 95.067% (40278/42368)\n",
            "340 391 Loss: 0.145 | Train Acc: 95.051% (41488/43648)\n",
            "350 391 Loss: 0.146 | Train Acc: 95.041% (42700/44928)\n",
            "360 391 Loss: 0.147 | Train Acc: 95.018% (43906/46208)\n",
            "370 391 Loss: 0.148 | Train Acc: 94.999% (45113/47488)\n",
            "380 391 Loss: 0.148 | Train Acc: 94.995% (46327/48768)\n",
            "390 391 Loss: 0.148 | Train Acc: 94.980% (47490/50000)\n",
            "0 100 Loss: 0.277 | Test Acc: 90.000% (90/100)\n",
            "10 100 Loss: 0.307 | Test Acc: 89.909% (989/1100)\n",
            "20 100 Loss: 0.347 | Test Acc: 89.143% (1872/2100)\n",
            "30 100 Loss: 0.361 | Test Acc: 89.161% (2764/3100)\n",
            "40 100 Loss: 0.360 | Test Acc: 89.220% (3658/4100)\n",
            "50 100 Loss: 0.361 | Test Acc: 89.216% (4550/5100)\n",
            "60 100 Loss: 0.359 | Test Acc: 89.148% (5438/6100)\n",
            "70 100 Loss: 0.363 | Test Acc: 89.014% (6320/7100)\n",
            "80 100 Loss: 0.358 | Test Acc: 89.284% (7232/8100)\n",
            "90 100 Loss: 0.358 | Test Acc: 89.132% (8111/9100)\n",
            "\n",
            "Epoch: 118\n",
            "0 391 Loss: 0.157 | Train Acc: 94.531% (121/128)\n",
            "10 391 Loss: 0.136 | Train Acc: 95.241% (1341/1408)\n",
            "20 391 Loss: 0.150 | Train Acc: 94.866% (2550/2688)\n",
            "30 391 Loss: 0.145 | Train Acc: 95.086% (3773/3968)\n",
            "40 391 Loss: 0.141 | Train Acc: 95.122% (4992/5248)\n",
            "50 391 Loss: 0.141 | Train Acc: 95.098% (6208/6528)\n",
            "60 391 Loss: 0.142 | Train Acc: 95.044% (7421/7808)\n",
            "70 391 Loss: 0.139 | Train Acc: 95.191% (8651/9088)\n",
            "80 391 Loss: 0.137 | Train Acc: 95.245% (9875/10368)\n",
            "90 391 Loss: 0.138 | Train Acc: 95.184% (11087/11648)\n",
            "100 391 Loss: 0.138 | Train Acc: 95.212% (12309/12928)\n",
            "110 391 Loss: 0.140 | Train Acc: 95.101% (13512/14208)\n",
            "120 391 Loss: 0.140 | Train Acc: 95.087% (14727/15488)\n",
            "130 391 Loss: 0.143 | Train Acc: 94.979% (15926/16768)\n",
            "140 391 Loss: 0.143 | Train Acc: 94.952% (17137/18048)\n",
            "150 391 Loss: 0.143 | Train Acc: 94.971% (18356/19328)\n",
            "160 391 Loss: 0.144 | Train Acc: 94.934% (19564/20608)\n",
            "170 391 Loss: 0.145 | Train Acc: 94.929% (20778/21888)\n",
            "180 391 Loss: 0.145 | Train Acc: 94.898% (21986/23168)\n",
            "190 391 Loss: 0.145 | Train Acc: 94.916% (23205/24448)\n",
            "200 391 Loss: 0.146 | Train Acc: 94.897% (24415/25728)\n",
            "210 391 Loss: 0.147 | Train Acc: 94.872% (25623/27008)\n",
            "220 391 Loss: 0.147 | Train Acc: 94.892% (26843/28288)\n",
            "230 391 Loss: 0.147 | Train Acc: 94.880% (28054/29568)\n",
            "240 391 Loss: 0.147 | Train Acc: 94.920% (29281/30848)\n",
            "250 391 Loss: 0.147 | Train Acc: 94.905% (30491/32128)\n",
            "260 391 Loss: 0.146 | Train Acc: 94.938% (31717/33408)\n",
            "270 391 Loss: 0.146 | Train Acc: 94.912% (32923/34688)\n",
            "280 391 Loss: 0.146 | Train Acc: 94.887% (34129/35968)\n",
            "290 391 Loss: 0.147 | Train Acc: 94.880% (35341/37248)\n",
            "300 391 Loss: 0.148 | Train Acc: 94.819% (36532/38528)\n",
            "310 391 Loss: 0.148 | Train Acc: 94.833% (37751/39808)\n",
            "320 391 Loss: 0.149 | Train Acc: 94.806% (38954/41088)\n",
            "330 391 Loss: 0.150 | Train Acc: 94.772% (40153/42368)\n",
            "340 391 Loss: 0.151 | Train Acc: 94.737% (41351/43648)\n",
            "350 391 Loss: 0.151 | Train Acc: 94.740% (42565/44928)\n",
            "360 391 Loss: 0.151 | Train Acc: 94.737% (43776/46208)\n",
            "370 391 Loss: 0.150 | Train Acc: 94.757% (44998/47488)\n",
            "380 391 Loss: 0.150 | Train Acc: 94.747% (46206/48768)\n",
            "390 391 Loss: 0.151 | Train Acc: 94.742% (47371/50000)\n",
            "0 100 Loss: 0.465 | Test Acc: 89.000% (89/100)\n",
            "10 100 Loss: 0.391 | Test Acc: 88.273% (971/1100)\n",
            "20 100 Loss: 0.400 | Test Acc: 88.000% (1848/2100)\n",
            "30 100 Loss: 0.398 | Test Acc: 88.161% (2733/3100)\n",
            "40 100 Loss: 0.400 | Test Acc: 88.195% (3616/4100)\n",
            "50 100 Loss: 0.392 | Test Acc: 88.137% (4495/5100)\n",
            "60 100 Loss: 0.383 | Test Acc: 88.164% (5378/6100)\n",
            "70 100 Loss: 0.377 | Test Acc: 88.254% (6266/7100)\n",
            "80 100 Loss: 0.372 | Test Acc: 88.346% (7156/8100)\n",
            "90 100 Loss: 0.368 | Test Acc: 88.451% (8049/9100)\n",
            "\n",
            "Epoch: 119\n",
            "0 391 Loss: 0.150 | Train Acc: 95.312% (122/128)\n",
            "10 391 Loss: 0.129 | Train Acc: 95.384% (1343/1408)\n",
            "20 391 Loss: 0.125 | Train Acc: 95.499% (2567/2688)\n",
            "30 391 Loss: 0.125 | Train Acc: 95.766% (3800/3968)\n",
            "40 391 Loss: 0.127 | Train Acc: 95.713% (5023/5248)\n",
            "50 391 Loss: 0.132 | Train Acc: 95.680% (6246/6528)\n",
            "60 391 Loss: 0.130 | Train Acc: 95.697% (7472/7808)\n",
            "70 391 Loss: 0.133 | Train Acc: 95.599% (8688/9088)\n",
            "80 391 Loss: 0.133 | Train Acc: 95.573% (9909/10368)\n",
            "90 391 Loss: 0.132 | Train Acc: 95.510% (11125/11648)\n",
            "100 391 Loss: 0.137 | Train Acc: 95.336% (12325/12928)\n",
            "110 391 Loss: 0.139 | Train Acc: 95.277% (13537/14208)\n",
            "120 391 Loss: 0.139 | Train Acc: 95.287% (14758/15488)\n",
            "130 391 Loss: 0.139 | Train Acc: 95.193% (15962/16768)\n",
            "140 391 Loss: 0.141 | Train Acc: 95.135% (17170/18048)\n",
            "150 391 Loss: 0.140 | Train Acc: 95.173% (18395/19328)\n",
            "160 391 Loss: 0.140 | Train Acc: 95.138% (19606/20608)\n",
            "170 391 Loss: 0.141 | Train Acc: 95.107% (20817/21888)\n",
            "180 391 Loss: 0.143 | Train Acc: 95.002% (22010/23168)\n",
            "190 391 Loss: 0.144 | Train Acc: 94.969% (23218/24448)\n",
            "200 391 Loss: 0.145 | Train Acc: 94.924% (24422/25728)\n",
            "210 391 Loss: 0.144 | Train Acc: 94.920% (25636/27008)\n",
            "220 391 Loss: 0.145 | Train Acc: 94.895% (26844/28288)\n",
            "230 391 Loss: 0.146 | Train Acc: 94.859% (28048/29568)\n",
            "240 391 Loss: 0.146 | Train Acc: 94.878% (29268/30848)\n",
            "250 391 Loss: 0.147 | Train Acc: 94.864% (30478/32128)\n",
            "260 391 Loss: 0.146 | Train Acc: 94.881% (31698/33408)\n",
            "270 391 Loss: 0.146 | Train Acc: 94.877% (32911/34688)\n",
            "280 391 Loss: 0.146 | Train Acc: 94.865% (34121/35968)\n",
            "290 391 Loss: 0.148 | Train Acc: 94.824% (35320/37248)\n",
            "300 391 Loss: 0.148 | Train Acc: 94.783% (36518/38528)\n",
            "310 391 Loss: 0.149 | Train Acc: 94.772% (37727/39808)\n",
            "320 391 Loss: 0.149 | Train Acc: 94.767% (38938/41088)\n",
            "330 391 Loss: 0.149 | Train Acc: 94.746% (40142/42368)\n",
            "340 391 Loss: 0.149 | Train Acc: 94.753% (41358/43648)\n",
            "350 391 Loss: 0.150 | Train Acc: 94.725% (42558/44928)\n",
            "360 391 Loss: 0.150 | Train Acc: 94.704% (43761/46208)\n",
            "370 391 Loss: 0.151 | Train Acc: 94.691% (44967/47488)\n",
            "380 391 Loss: 0.151 | Train Acc: 94.675% (46171/48768)\n",
            "390 391 Loss: 0.151 | Train Acc: 94.670% (47335/50000)\n",
            "0 100 Loss: 0.352 | Test Acc: 92.000% (92/100)\n",
            "10 100 Loss: 0.287 | Test Acc: 91.545% (1007/1100)\n",
            "20 100 Loss: 0.309 | Test Acc: 90.381% (1898/2100)\n",
            "30 100 Loss: 0.324 | Test Acc: 90.065% (2792/3100)\n",
            "40 100 Loss: 0.330 | Test Acc: 89.927% (3687/4100)\n",
            "50 100 Loss: 0.322 | Test Acc: 89.941% (4587/5100)\n",
            "60 100 Loss: 0.323 | Test Acc: 89.672% (5470/6100)\n",
            "70 100 Loss: 0.317 | Test Acc: 89.746% (6372/7100)\n",
            "80 100 Loss: 0.309 | Test Acc: 89.914% (7283/8100)\n",
            "90 100 Loss: 0.313 | Test Acc: 89.879% (8179/9100)\n",
            "\n",
            "Epoch: 120\n",
            "0 391 Loss: 0.185 | Train Acc: 92.969% (119/128)\n",
            "10 391 Loss: 0.147 | Train Acc: 95.028% (1338/1408)\n",
            "20 391 Loss: 0.141 | Train Acc: 95.089% (2556/2688)\n",
            "30 391 Loss: 0.146 | Train Acc: 94.934% (3767/3968)\n",
            "40 391 Loss: 0.141 | Train Acc: 94.931% (4982/5248)\n",
            "50 391 Loss: 0.139 | Train Acc: 95.083% (6207/6528)\n",
            "60 391 Loss: 0.142 | Train Acc: 95.005% (7418/7808)\n",
            "70 391 Loss: 0.144 | Train Acc: 94.960% (8630/9088)\n",
            "80 391 Loss: 0.141 | Train Acc: 95.052% (9855/10368)\n",
            "90 391 Loss: 0.143 | Train Acc: 95.003% (11066/11648)\n",
            "100 391 Loss: 0.143 | Train Acc: 95.011% (12283/12928)\n",
            "110 391 Loss: 0.142 | Train Acc: 95.059% (13506/14208)\n",
            "120 391 Loss: 0.141 | Train Acc: 95.061% (14723/15488)\n",
            "130 391 Loss: 0.140 | Train Acc: 95.080% (15943/16768)\n",
            "140 391 Loss: 0.139 | Train Acc: 95.141% (17171/18048)\n",
            "150 391 Loss: 0.139 | Train Acc: 95.199% (18400/19328)\n",
            "160 391 Loss: 0.139 | Train Acc: 95.225% (19624/20608)\n",
            "170 391 Loss: 0.137 | Train Acc: 95.290% (20857/21888)\n",
            "180 391 Loss: 0.138 | Train Acc: 95.274% (22073/23168)\n",
            "190 391 Loss: 0.139 | Train Acc: 95.239% (23284/24448)\n",
            "200 391 Loss: 0.140 | Train Acc: 95.215% (24497/25728)\n",
            "210 391 Loss: 0.141 | Train Acc: 95.209% (25714/27008)\n",
            "220 391 Loss: 0.140 | Train Acc: 95.228% (26938/28288)\n",
            "230 391 Loss: 0.142 | Train Acc: 95.181% (28143/29568)\n",
            "240 391 Loss: 0.142 | Train Acc: 95.160% (29355/30848)\n",
            "250 391 Loss: 0.142 | Train Acc: 95.163% (30574/32128)\n",
            "260 391 Loss: 0.142 | Train Acc: 95.172% (31795/33408)\n",
            "270 391 Loss: 0.142 | Train Acc: 95.165% (33011/34688)\n",
            "280 391 Loss: 0.142 | Train Acc: 95.160% (34227/35968)\n",
            "290 391 Loss: 0.142 | Train Acc: 95.170% (35449/37248)\n",
            "300 391 Loss: 0.142 | Train Acc: 95.162% (36664/38528)\n",
            "310 391 Loss: 0.142 | Train Acc: 95.162% (37882/39808)\n",
            "320 391 Loss: 0.143 | Train Acc: 95.135% (39089/41088)\n",
            "330 391 Loss: 0.143 | Train Acc: 95.145% (40311/42368)\n",
            "340 391 Loss: 0.143 | Train Acc: 95.120% (41518/43648)\n",
            "350 391 Loss: 0.144 | Train Acc: 95.114% (42733/44928)\n",
            "360 391 Loss: 0.144 | Train Acc: 95.100% (43944/46208)\n",
            "370 391 Loss: 0.145 | Train Acc: 95.068% (45146/47488)\n",
            "380 391 Loss: 0.146 | Train Acc: 95.023% (46341/48768)\n",
            "390 391 Loss: 0.147 | Train Acc: 94.992% (47496/50000)\n",
            "0 100 Loss: 0.293 | Test Acc: 91.000% (91/100)\n",
            "10 100 Loss: 0.356 | Test Acc: 89.182% (981/1100)\n",
            "20 100 Loss: 0.365 | Test Acc: 88.286% (1854/2100)\n",
            "30 100 Loss: 0.375 | Test Acc: 88.323% (2738/3100)\n",
            "40 100 Loss: 0.365 | Test Acc: 88.927% (3646/4100)\n",
            "50 100 Loss: 0.353 | Test Acc: 89.157% (4547/5100)\n",
            "60 100 Loss: 0.347 | Test Acc: 89.230% (5443/6100)\n",
            "70 100 Loss: 0.345 | Test Acc: 89.099% (6326/7100)\n",
            "80 100 Loss: 0.338 | Test Acc: 89.259% (7230/8100)\n",
            "90 100 Loss: 0.340 | Test Acc: 89.110% (8109/9100)\n",
            "\n",
            "Epoch: 121\n",
            "0 391 Loss: 0.200 | Train Acc: 92.188% (118/128)\n",
            "10 391 Loss: 0.151 | Train Acc: 94.673% (1333/1408)\n",
            "20 391 Loss: 0.143 | Train Acc: 94.792% (2548/2688)\n",
            "30 391 Loss: 0.131 | Train Acc: 95.237% (3779/3968)\n",
            "40 391 Loss: 0.127 | Train Acc: 95.560% (5015/5248)\n",
            "50 391 Loss: 0.127 | Train Acc: 95.496% (6234/6528)\n",
            "60 391 Loss: 0.125 | Train Acc: 95.530% (7459/7808)\n",
            "70 391 Loss: 0.124 | Train Acc: 95.599% (8688/9088)\n",
            "80 391 Loss: 0.124 | Train Acc: 95.592% (9911/10368)\n",
            "90 391 Loss: 0.122 | Train Acc: 95.639% (11140/11648)\n",
            "100 391 Loss: 0.124 | Train Acc: 95.537% (12351/12928)\n",
            "110 391 Loss: 0.122 | Train Acc: 95.594% (13582/14208)\n",
            "120 391 Loss: 0.123 | Train Acc: 95.590% (14805/15488)\n",
            "130 391 Loss: 0.126 | Train Acc: 95.468% (16008/16768)\n",
            "140 391 Loss: 0.127 | Train Acc: 95.440% (17225/18048)\n",
            "150 391 Loss: 0.128 | Train Acc: 95.390% (18437/19328)\n",
            "160 391 Loss: 0.129 | Train Acc: 95.351% (19650/20608)\n",
            "170 391 Loss: 0.129 | Train Acc: 95.408% (20883/21888)\n",
            "180 391 Loss: 0.129 | Train Acc: 95.395% (22101/23168)\n",
            "190 391 Loss: 0.129 | Train Acc: 95.407% (23325/24448)\n",
            "200 391 Loss: 0.129 | Train Acc: 95.410% (24547/25728)\n",
            "210 391 Loss: 0.130 | Train Acc: 95.416% (25770/27008)\n",
            "220 391 Loss: 0.131 | Train Acc: 95.366% (26977/28288)\n",
            "230 391 Loss: 0.130 | Train Acc: 95.377% (28201/29568)\n",
            "240 391 Loss: 0.130 | Train Acc: 95.381% (29423/30848)\n",
            "250 391 Loss: 0.130 | Train Acc: 95.372% (30641/32128)\n",
            "260 391 Loss: 0.132 | Train Acc: 95.315% (31843/33408)\n",
            "270 391 Loss: 0.133 | Train Acc: 95.266% (33046/34688)\n",
            "280 391 Loss: 0.134 | Train Acc: 95.237% (34255/35968)\n",
            "290 391 Loss: 0.136 | Train Acc: 95.192% (35457/37248)\n",
            "300 391 Loss: 0.137 | Train Acc: 95.170% (36667/38528)\n",
            "310 391 Loss: 0.138 | Train Acc: 95.157% (37880/39808)\n",
            "320 391 Loss: 0.137 | Train Acc: 95.164% (39101/41088)\n",
            "330 391 Loss: 0.137 | Train Acc: 95.166% (40320/42368)\n",
            "340 391 Loss: 0.138 | Train Acc: 95.136% (41525/43648)\n",
            "350 391 Loss: 0.138 | Train Acc: 95.117% (42734/44928)\n",
            "360 391 Loss: 0.138 | Train Acc: 95.111% (43949/46208)\n",
            "370 391 Loss: 0.139 | Train Acc: 95.102% (45162/47488)\n",
            "380 391 Loss: 0.139 | Train Acc: 95.101% (46379/48768)\n",
            "390 391 Loss: 0.140 | Train Acc: 95.072% (47536/50000)\n",
            "0 100 Loss: 0.471 | Test Acc: 85.000% (85/100)\n",
            "10 100 Loss: 0.357 | Test Acc: 88.091% (969/1100)\n",
            "20 100 Loss: 0.360 | Test Acc: 88.952% (1868/2100)\n",
            "30 100 Loss: 0.378 | Test Acc: 88.581% (2746/3100)\n",
            "40 100 Loss: 0.371 | Test Acc: 88.683% (3636/4100)\n",
            "50 100 Loss: 0.365 | Test Acc: 88.902% (4534/5100)\n",
            "60 100 Loss: 0.364 | Test Acc: 88.820% (5418/6100)\n",
            "70 100 Loss: 0.360 | Test Acc: 89.056% (6323/7100)\n",
            "80 100 Loss: 0.357 | Test Acc: 89.099% (7217/8100)\n",
            "90 100 Loss: 0.357 | Test Acc: 89.066% (8105/9100)\n",
            "\n",
            "Epoch: 122\n",
            "0 391 Loss: 0.145 | Train Acc: 96.094% (123/128)\n",
            "10 391 Loss: 0.144 | Train Acc: 95.312% (1342/1408)\n",
            "20 391 Loss: 0.125 | Train Acc: 95.833% (2576/2688)\n",
            "30 391 Loss: 0.123 | Train Acc: 95.842% (3803/3968)\n",
            "40 391 Loss: 0.122 | Train Acc: 95.941% (5035/5248)\n",
            "50 391 Loss: 0.126 | Train Acc: 95.833% (6256/6528)\n",
            "60 391 Loss: 0.122 | Train Acc: 95.966% (7493/7808)\n",
            "70 391 Loss: 0.121 | Train Acc: 95.984% (8723/9088)\n",
            "80 391 Loss: 0.122 | Train Acc: 95.901% (9943/10368)\n",
            "90 391 Loss: 0.120 | Train Acc: 95.888% (11169/11648)\n",
            "100 391 Loss: 0.122 | Train Acc: 95.808% (12386/12928)\n",
            "110 391 Loss: 0.121 | Train Acc: 95.861% (13620/14208)\n",
            "120 391 Loss: 0.122 | Train Acc: 95.810% (14839/15488)\n",
            "130 391 Loss: 0.124 | Train Acc: 95.706% (16048/16768)\n",
            "140 391 Loss: 0.126 | Train Acc: 95.673% (17267/18048)\n",
            "150 391 Loss: 0.128 | Train Acc: 95.597% (18477/19328)\n",
            "160 391 Loss: 0.131 | Train Acc: 95.526% (19686/20608)\n",
            "170 391 Loss: 0.132 | Train Acc: 95.440% (20890/21888)\n",
            "180 391 Loss: 0.134 | Train Acc: 95.347% (22090/23168)\n",
            "190 391 Loss: 0.136 | Train Acc: 95.284% (23295/24448)\n",
            "200 391 Loss: 0.138 | Train Acc: 95.196% (24492/25728)\n",
            "210 391 Loss: 0.139 | Train Acc: 95.194% (25710/27008)\n",
            "220 391 Loss: 0.139 | Train Acc: 95.182% (26925/28288)\n",
            "230 391 Loss: 0.140 | Train Acc: 95.174% (28141/29568)\n",
            "240 391 Loss: 0.139 | Train Acc: 95.196% (29366/30848)\n",
            "250 391 Loss: 0.139 | Train Acc: 95.228% (30595/32128)\n",
            "260 391 Loss: 0.139 | Train Acc: 95.232% (31815/33408)\n",
            "270 391 Loss: 0.139 | Train Acc: 95.217% (33029/34688)\n",
            "280 391 Loss: 0.140 | Train Acc: 95.176% (34233/35968)\n",
            "290 391 Loss: 0.140 | Train Acc: 95.197% (35459/37248)\n",
            "300 391 Loss: 0.139 | Train Acc: 95.211% (36683/38528)\n",
            "310 391 Loss: 0.140 | Train Acc: 95.194% (37895/39808)\n",
            "320 391 Loss: 0.140 | Train Acc: 95.181% (39108/41088)\n",
            "330 391 Loss: 0.140 | Train Acc: 95.185% (40328/42368)\n",
            "340 391 Loss: 0.141 | Train Acc: 95.138% (41526/43648)\n",
            "350 391 Loss: 0.140 | Train Acc: 95.139% (42744/44928)\n",
            "360 391 Loss: 0.140 | Train Acc: 95.157% (43970/46208)\n",
            "370 391 Loss: 0.141 | Train Acc: 95.152% (45186/47488)\n",
            "380 391 Loss: 0.141 | Train Acc: 95.134% (46395/48768)\n",
            "390 391 Loss: 0.141 | Train Acc: 95.118% (47559/50000)\n",
            "0 100 Loss: 0.236 | Test Acc: 93.000% (93/100)\n",
            "10 100 Loss: 0.385 | Test Acc: 88.818% (977/1100)\n",
            "20 100 Loss: 0.384 | Test Acc: 88.667% (1862/2100)\n",
            "30 100 Loss: 0.388 | Test Acc: 88.839% (2754/3100)\n",
            "40 100 Loss: 0.383 | Test Acc: 88.732% (3638/4100)\n",
            "50 100 Loss: 0.378 | Test Acc: 88.980% (4538/5100)\n",
            "60 100 Loss: 0.371 | Test Acc: 89.049% (5432/6100)\n",
            "70 100 Loss: 0.363 | Test Acc: 89.254% (6337/7100)\n",
            "80 100 Loss: 0.359 | Test Acc: 89.272% (7231/8100)\n",
            "90 100 Loss: 0.358 | Test Acc: 89.330% (8129/9100)\n",
            "\n",
            "Epoch: 123\n",
            "0 391 Loss: 0.184 | Train Acc: 93.750% (120/128)\n",
            "10 391 Loss: 0.130 | Train Acc: 95.099% (1339/1408)\n",
            "20 391 Loss: 0.129 | Train Acc: 95.275% (2561/2688)\n",
            "30 391 Loss: 0.136 | Train Acc: 95.287% (3781/3968)\n",
            "40 391 Loss: 0.140 | Train Acc: 95.103% (4991/5248)\n",
            "50 391 Loss: 0.133 | Train Acc: 95.389% (6227/6528)\n",
            "60 391 Loss: 0.131 | Train Acc: 95.466% (7454/7808)\n",
            "70 391 Loss: 0.130 | Train Acc: 95.544% (8683/9088)\n",
            "80 391 Loss: 0.127 | Train Acc: 95.708% (9923/10368)\n",
            "90 391 Loss: 0.127 | Train Acc: 95.707% (11148/11648)\n",
            "100 391 Loss: 0.126 | Train Acc: 95.707% (12373/12928)\n",
            "110 391 Loss: 0.128 | Train Acc: 95.622% (13586/14208)\n",
            "120 391 Loss: 0.130 | Train Acc: 95.577% (14803/15488)\n",
            "130 391 Loss: 0.128 | Train Acc: 95.635% (16036/16768)\n",
            "140 391 Loss: 0.128 | Train Acc: 95.650% (17263/18048)\n",
            "150 391 Loss: 0.128 | Train Acc: 95.628% (18483/19328)\n",
            "160 391 Loss: 0.128 | Train Acc: 95.647% (19711/20608)\n",
            "170 391 Loss: 0.127 | Train Acc: 95.628% (20931/21888)\n",
            "180 391 Loss: 0.130 | Train Acc: 95.571% (22142/23168)\n",
            "190 391 Loss: 0.129 | Train Acc: 95.615% (23376/24448)\n",
            "200 391 Loss: 0.130 | Train Acc: 95.553% (24584/25728)\n",
            "210 391 Loss: 0.131 | Train Acc: 95.509% (25795/27008)\n",
            "220 391 Loss: 0.131 | Train Acc: 95.482% (27010/28288)\n",
            "230 391 Loss: 0.131 | Train Acc: 95.492% (28235/29568)\n",
            "240 391 Loss: 0.131 | Train Acc: 95.484% (29455/30848)\n",
            "250 391 Loss: 0.133 | Train Acc: 95.425% (30658/32128)\n",
            "260 391 Loss: 0.133 | Train Acc: 95.420% (31878/33408)\n",
            "270 391 Loss: 0.132 | Train Acc: 95.442% (33107/34688)\n",
            "280 391 Loss: 0.134 | Train Acc: 95.365% (34301/35968)\n",
            "290 391 Loss: 0.135 | Train Acc: 95.315% (35503/37248)\n",
            "300 391 Loss: 0.136 | Train Acc: 95.268% (36705/38528)\n",
            "310 391 Loss: 0.137 | Train Acc: 95.255% (37919/39808)\n",
            "320 391 Loss: 0.138 | Train Acc: 95.230% (39128/41088)\n",
            "330 391 Loss: 0.139 | Train Acc: 95.187% (40329/42368)\n",
            "340 391 Loss: 0.139 | Train Acc: 95.177% (41543/43648)\n",
            "350 391 Loss: 0.139 | Train Acc: 95.154% (42751/44928)\n",
            "360 391 Loss: 0.140 | Train Acc: 95.148% (43966/46208)\n",
            "370 391 Loss: 0.140 | Train Acc: 95.140% (45180/47488)\n",
            "380 391 Loss: 0.139 | Train Acc: 95.142% (46399/48768)\n",
            "390 391 Loss: 0.139 | Train Acc: 95.160% (47580/50000)\n",
            "0 100 Loss: 0.337 | Test Acc: 86.000% (86/100)\n",
            "10 100 Loss: 0.354 | Test Acc: 88.364% (972/1100)\n",
            "20 100 Loss: 0.331 | Test Acc: 89.190% (1873/2100)\n",
            "30 100 Loss: 0.334 | Test Acc: 89.548% (2776/3100)\n",
            "40 100 Loss: 0.335 | Test Acc: 89.878% (3685/4100)\n",
            "50 100 Loss: 0.326 | Test Acc: 90.137% (4597/5100)\n",
            "60 100 Loss: 0.323 | Test Acc: 90.197% (5502/6100)\n",
            "70 100 Loss: 0.320 | Test Acc: 90.225% (6406/7100)\n",
            "80 100 Loss: 0.316 | Test Acc: 90.346% (7318/8100)\n",
            "90 100 Loss: 0.312 | Test Acc: 90.462% (8232/9100)\n",
            "\n",
            "Epoch: 124\n",
            "0 391 Loss: 0.172 | Train Acc: 92.969% (119/128)\n",
            "10 391 Loss: 0.113 | Train Acc: 96.236% (1355/1408)\n",
            "20 391 Loss: 0.121 | Train Acc: 95.833% (2576/2688)\n",
            "30 391 Loss: 0.117 | Train Acc: 95.993% (3809/3968)\n",
            "40 391 Loss: 0.116 | Train Acc: 95.998% (5038/5248)\n",
            "50 391 Loss: 0.114 | Train Acc: 96.078% (6272/6528)\n",
            "60 391 Loss: 0.117 | Train Acc: 96.017% (7497/7808)\n",
            "70 391 Loss: 0.120 | Train Acc: 96.006% (8725/9088)\n",
            "80 391 Loss: 0.121 | Train Acc: 95.910% (9944/10368)\n",
            "90 391 Loss: 0.122 | Train Acc: 95.776% (11156/11648)\n",
            "100 391 Loss: 0.121 | Train Acc: 95.808% (12386/12928)\n",
            "110 391 Loss: 0.122 | Train Acc: 95.756% (13605/14208)\n",
            "120 391 Loss: 0.123 | Train Acc: 95.687% (14820/15488)\n",
            "130 391 Loss: 0.123 | Train Acc: 95.670% (16042/16768)\n",
            "140 391 Loss: 0.123 | Train Acc: 95.656% (17264/18048)\n",
            "150 391 Loss: 0.123 | Train Acc: 95.649% (18487/19328)\n",
            "160 391 Loss: 0.123 | Train Acc: 95.647% (19711/20608)\n",
            "170 391 Loss: 0.125 | Train Acc: 95.582% (20921/21888)\n",
            "180 391 Loss: 0.126 | Train Acc: 95.537% (22134/23168)\n",
            "190 391 Loss: 0.127 | Train Acc: 95.509% (23350/24448)\n",
            "200 391 Loss: 0.127 | Train Acc: 95.491% (24568/25728)\n",
            "210 391 Loss: 0.128 | Train Acc: 95.479% (25787/27008)\n",
            "220 391 Loss: 0.127 | Train Acc: 95.525% (27022/28288)\n",
            "230 391 Loss: 0.128 | Train Acc: 95.488% (28234/29568)\n",
            "240 391 Loss: 0.128 | Train Acc: 95.458% (29447/30848)\n",
            "250 391 Loss: 0.129 | Train Acc: 95.446% (30665/32128)\n",
            "260 391 Loss: 0.130 | Train Acc: 95.411% (31875/33408)\n",
            "270 391 Loss: 0.130 | Train Acc: 95.405% (33094/34688)\n",
            "280 391 Loss: 0.130 | Train Acc: 95.410% (34317/35968)\n",
            "290 391 Loss: 0.130 | Train Acc: 95.420% (35542/37248)\n",
            "300 391 Loss: 0.131 | Train Acc: 95.383% (36749/38528)\n",
            "310 391 Loss: 0.131 | Train Acc: 95.368% (37964/39808)\n",
            "320 391 Loss: 0.131 | Train Acc: 95.361% (39182/41088)\n",
            "330 391 Loss: 0.132 | Train Acc: 95.364% (40404/42368)\n",
            "340 391 Loss: 0.133 | Train Acc: 95.322% (41606/43648)\n",
            "350 391 Loss: 0.134 | Train Acc: 95.264% (42800/44928)\n",
            "360 391 Loss: 0.135 | Train Acc: 95.256% (44016/46208)\n",
            "370 391 Loss: 0.135 | Train Acc: 95.239% (45227/47488)\n",
            "380 391 Loss: 0.135 | Train Acc: 95.261% (46457/48768)\n",
            "390 391 Loss: 0.135 | Train Acc: 95.274% (47637/50000)\n",
            "0 100 Loss: 0.321 | Test Acc: 93.000% (93/100)\n",
            "10 100 Loss: 0.307 | Test Acc: 90.909% (1000/1100)\n",
            "20 100 Loss: 0.333 | Test Acc: 90.571% (1902/2100)\n",
            "30 100 Loss: 0.340 | Test Acc: 90.710% (2812/3100)\n",
            "40 100 Loss: 0.340 | Test Acc: 90.756% (3721/4100)\n",
            "50 100 Loss: 0.333 | Test Acc: 90.843% (4633/5100)\n",
            "60 100 Loss: 0.330 | Test Acc: 90.689% (5532/6100)\n",
            "70 100 Loss: 0.323 | Test Acc: 90.704% (6440/7100)\n",
            "80 100 Loss: 0.322 | Test Acc: 90.654% (7343/8100)\n",
            "90 100 Loss: 0.322 | Test Acc: 90.582% (8243/9100)\n",
            "\n",
            "Epoch: 125\n",
            "0 391 Loss: 0.186 | Train Acc: 93.750% (120/128)\n",
            "10 391 Loss: 0.139 | Train Acc: 95.384% (1343/1408)\n",
            "20 391 Loss: 0.133 | Train Acc: 95.685% (2572/2688)\n",
            "30 391 Loss: 0.128 | Train Acc: 95.791% (3801/3968)\n",
            "40 391 Loss: 0.126 | Train Acc: 95.694% (5022/5248)\n",
            "50 391 Loss: 0.120 | Train Acc: 95.864% (6258/6528)\n",
            "60 391 Loss: 0.120 | Train Acc: 95.838% (7483/7808)\n",
            "70 391 Loss: 0.117 | Train Acc: 95.907% (8716/9088)\n",
            "80 391 Loss: 0.117 | Train Acc: 95.949% (9948/10368)\n",
            "90 391 Loss: 0.119 | Train Acc: 95.853% (11165/11648)\n",
            "100 391 Loss: 0.117 | Train Acc: 95.908% (12399/12928)\n",
            "110 391 Loss: 0.117 | Train Acc: 95.883% (13623/14208)\n",
            "120 391 Loss: 0.118 | Train Acc: 95.868% (14848/15488)\n",
            "130 391 Loss: 0.119 | Train Acc: 95.855% (16073/16768)\n",
            "140 391 Loss: 0.120 | Train Acc: 95.811% (17292/18048)\n",
            "150 391 Loss: 0.122 | Train Acc: 95.747% (18506/19328)\n",
            "160 391 Loss: 0.123 | Train Acc: 95.691% (19720/20608)\n",
            "170 391 Loss: 0.124 | Train Acc: 95.673% (20941/21888)\n",
            "180 391 Loss: 0.125 | Train Acc: 95.662% (22163/23168)\n",
            "190 391 Loss: 0.125 | Train Acc: 95.636% (23381/24448)\n",
            "200 391 Loss: 0.126 | Train Acc: 95.608% (24598/25728)\n",
            "210 391 Loss: 0.126 | Train Acc: 95.598% (25819/27008)\n",
            "220 391 Loss: 0.127 | Train Acc: 95.560% (27032/28288)\n",
            "230 391 Loss: 0.127 | Train Acc: 95.563% (28256/29568)\n",
            "240 391 Loss: 0.128 | Train Acc: 95.526% (29468/30848)\n",
            "250 391 Loss: 0.128 | Train Acc: 95.537% (30694/32128)\n",
            "260 391 Loss: 0.128 | Train Acc: 95.543% (31919/33408)\n",
            "270 391 Loss: 0.128 | Train Acc: 95.526% (33136/34688)\n",
            "280 391 Loss: 0.128 | Train Acc: 95.510% (34353/35968)\n",
            "290 391 Loss: 0.128 | Train Acc: 95.535% (35585/37248)\n",
            "300 391 Loss: 0.129 | Train Acc: 95.505% (36796/38528)\n",
            "310 391 Loss: 0.129 | Train Acc: 95.486% (38011/39808)\n",
            "320 391 Loss: 0.130 | Train Acc: 95.456% (39221/41088)\n",
            "330 391 Loss: 0.131 | Train Acc: 95.447% (40439/42368)\n",
            "340 391 Loss: 0.131 | Train Acc: 95.443% (41659/43648)\n",
            "350 391 Loss: 0.132 | Train Acc: 95.417% (42869/44928)\n",
            "360 391 Loss: 0.132 | Train Acc: 95.421% (44092/46208)\n",
            "370 391 Loss: 0.132 | Train Acc: 95.424% (45315/47488)\n",
            "380 391 Loss: 0.132 | Train Acc: 95.423% (46536/48768)\n",
            "390 391 Loss: 0.132 | Train Acc: 95.434% (47717/50000)\n",
            "0 100 Loss: 0.368 | Test Acc: 86.000% (86/100)\n",
            "10 100 Loss: 0.307 | Test Acc: 89.364% (983/1100)\n",
            "20 100 Loss: 0.288 | Test Acc: 89.952% (1889/2100)\n",
            "30 100 Loss: 0.294 | Test Acc: 90.194% (2796/3100)\n",
            "40 100 Loss: 0.296 | Test Acc: 90.024% (3691/4100)\n",
            "50 100 Loss: 0.287 | Test Acc: 90.255% (4603/5100)\n",
            "60 100 Loss: 0.286 | Test Acc: 90.459% (5518/6100)\n",
            "70 100 Loss: 0.289 | Test Acc: 90.451% (6422/7100)\n",
            "80 100 Loss: 0.290 | Test Acc: 90.494% (7330/8100)\n",
            "90 100 Loss: 0.290 | Test Acc: 90.505% (8236/9100)\n",
            "\n",
            "Epoch: 126\n",
            "0 391 Loss: 0.098 | Train Acc: 96.094% (123/128)\n",
            "10 391 Loss: 0.129 | Train Acc: 95.170% (1340/1408)\n",
            "20 391 Loss: 0.110 | Train Acc: 95.908% (2578/2688)\n",
            "30 391 Loss: 0.110 | Train Acc: 96.069% (3812/3968)\n",
            "40 391 Loss: 0.110 | Train Acc: 96.075% (5042/5248)\n",
            "50 391 Loss: 0.110 | Train Acc: 96.094% (6273/6528)\n",
            "60 391 Loss: 0.111 | Train Acc: 95.940% (7491/7808)\n",
            "70 391 Loss: 0.110 | Train Acc: 95.962% (8721/9088)\n",
            "80 391 Loss: 0.113 | Train Acc: 95.882% (9941/10368)\n",
            "90 391 Loss: 0.111 | Train Acc: 95.871% (11167/11648)\n",
            "100 391 Loss: 0.111 | Train Acc: 95.916% (12400/12928)\n",
            "110 391 Loss: 0.110 | Train Acc: 95.939% (13631/14208)\n",
            "120 391 Loss: 0.112 | Train Acc: 95.907% (14854/15488)\n",
            "130 391 Loss: 0.112 | Train Acc: 95.980% (16094/16768)\n",
            "140 391 Loss: 0.112 | Train Acc: 95.961% (17319/18048)\n",
            "150 391 Loss: 0.112 | Train Acc: 95.944% (18544/19328)\n",
            "160 391 Loss: 0.114 | Train Acc: 95.905% (19764/20608)\n",
            "170 391 Loss: 0.115 | Train Acc: 95.874% (20985/21888)\n",
            "180 391 Loss: 0.116 | Train Acc: 95.839% (22204/23168)\n",
            "190 391 Loss: 0.115 | Train Acc: 95.893% (23444/24448)\n",
            "200 391 Loss: 0.115 | Train Acc: 95.868% (24665/25728)\n",
            "210 391 Loss: 0.115 | Train Acc: 95.879% (25895/27008)\n",
            "220 391 Loss: 0.116 | Train Acc: 95.850% (27114/28288)\n",
            "230 391 Loss: 0.117 | Train Acc: 95.843% (28339/29568)\n",
            "240 391 Loss: 0.118 | Train Acc: 95.815% (29557/30848)\n",
            "250 391 Loss: 0.117 | Train Acc: 95.826% (30787/32128)\n",
            "260 391 Loss: 0.118 | Train Acc: 95.800% (32005/33408)\n",
            "270 391 Loss: 0.118 | Train Acc: 95.805% (33233/34688)\n",
            "280 391 Loss: 0.118 | Train Acc: 95.793% (34455/35968)\n",
            "290 391 Loss: 0.118 | Train Acc: 95.806% (35686/37248)\n",
            "300 391 Loss: 0.118 | Train Acc: 95.803% (36911/38528)\n",
            "310 391 Loss: 0.119 | Train Acc: 95.752% (38117/39808)\n",
            "320 391 Loss: 0.119 | Train Acc: 95.748% (39341/41088)\n",
            "330 391 Loss: 0.120 | Train Acc: 95.709% (40550/42368)\n",
            "340 391 Loss: 0.121 | Train Acc: 95.677% (41761/43648)\n",
            "350 391 Loss: 0.122 | Train Acc: 95.664% (42980/44928)\n",
            "360 391 Loss: 0.122 | Train Acc: 95.641% (44194/46208)\n",
            "370 391 Loss: 0.124 | Train Acc: 95.607% (45402/47488)\n",
            "380 391 Loss: 0.124 | Train Acc: 95.579% (46612/48768)\n",
            "390 391 Loss: 0.126 | Train Acc: 95.522% (47761/50000)\n",
            "0 100 Loss: 0.467 | Test Acc: 88.000% (88/100)\n",
            "10 100 Loss: 0.493 | Test Acc: 87.182% (959/1100)\n",
            "20 100 Loss: 0.500 | Test Acc: 86.381% (1814/2100)\n",
            "30 100 Loss: 0.492 | Test Acc: 86.290% (2675/3100)\n",
            "40 100 Loss: 0.486 | Test Acc: 86.317% (3539/4100)\n",
            "50 100 Loss: 0.485 | Test Acc: 86.333% (4403/5100)\n",
            "60 100 Loss: 0.489 | Test Acc: 86.230% (5260/6100)\n",
            "70 100 Loss: 0.475 | Test Acc: 86.366% (6132/7100)\n",
            "80 100 Loss: 0.472 | Test Acc: 86.407% (6999/8100)\n",
            "90 100 Loss: 0.463 | Test Acc: 86.560% (7877/9100)\n",
            "\n",
            "Epoch: 127\n",
            "0 391 Loss: 0.102 | Train Acc: 97.656% (125/128)\n",
            "10 391 Loss: 0.154 | Train Acc: 94.389% (1329/1408)\n",
            "20 391 Loss: 0.140 | Train Acc: 95.052% (2555/2688)\n",
            "30 391 Loss: 0.143 | Train Acc: 95.161% (3776/3968)\n",
            "40 391 Loss: 0.138 | Train Acc: 95.351% (5004/5248)\n",
            "50 391 Loss: 0.136 | Train Acc: 95.466% (6232/6528)\n",
            "60 391 Loss: 0.131 | Train Acc: 95.658% (7469/7808)\n",
            "70 391 Loss: 0.129 | Train Acc: 95.764% (8703/9088)\n",
            "80 391 Loss: 0.125 | Train Acc: 95.862% (9939/10368)\n",
            "90 391 Loss: 0.122 | Train Acc: 95.948% (11176/11648)\n",
            "100 391 Loss: 0.121 | Train Acc: 95.900% (12398/12928)\n",
            "110 391 Loss: 0.119 | Train Acc: 95.939% (13631/14208)\n",
            "120 391 Loss: 0.119 | Train Acc: 95.861% (14847/15488)\n",
            "130 391 Loss: 0.119 | Train Acc: 95.867% (16075/16768)\n",
            "140 391 Loss: 0.120 | Train Acc: 95.855% (17300/18048)\n",
            "150 391 Loss: 0.119 | Train Acc: 95.871% (18530/19328)\n",
            "160 391 Loss: 0.118 | Train Acc: 95.905% (19764/20608)\n",
            "170 391 Loss: 0.118 | Train Acc: 95.902% (20991/21888)\n",
            "180 391 Loss: 0.118 | Train Acc: 95.887% (22215/23168)\n",
            "190 391 Loss: 0.117 | Train Acc: 95.914% (23449/24448)\n",
            "200 391 Loss: 0.117 | Train Acc: 95.903% (24674/25728)\n",
            "210 391 Loss: 0.118 | Train Acc: 95.875% (25894/27008)\n",
            "220 391 Loss: 0.118 | Train Acc: 95.853% (27115/28288)\n",
            "230 391 Loss: 0.118 | Train Acc: 95.847% (28340/29568)\n",
            "240 391 Loss: 0.118 | Train Acc: 95.847% (29567/30848)\n",
            "250 391 Loss: 0.119 | Train Acc: 95.823% (30786/32128)\n",
            "260 391 Loss: 0.120 | Train Acc: 95.821% (32012/33408)\n",
            "270 391 Loss: 0.120 | Train Acc: 95.840% (33245/34688)\n",
            "280 391 Loss: 0.120 | Train Acc: 95.852% (34476/35968)\n",
            "290 391 Loss: 0.121 | Train Acc: 95.820% (35691/37248)\n",
            "300 391 Loss: 0.121 | Train Acc: 95.798% (36909/38528)\n",
            "310 391 Loss: 0.122 | Train Acc: 95.785% (38130/39808)\n",
            "320 391 Loss: 0.122 | Train Acc: 95.777% (39353/41088)\n",
            "330 391 Loss: 0.121 | Train Acc: 95.782% (40581/42368)\n",
            "340 391 Loss: 0.122 | Train Acc: 95.746% (41791/43648)\n",
            "350 391 Loss: 0.122 | Train Acc: 95.740% (43014/44928)\n",
            "360 391 Loss: 0.123 | Train Acc: 95.728% (44234/46208)\n",
            "370 391 Loss: 0.123 | Train Acc: 95.723% (45457/47488)\n",
            "380 391 Loss: 0.123 | Train Acc: 95.739% (46690/48768)\n",
            "390 391 Loss: 0.123 | Train Acc: 95.736% (47868/50000)\n",
            "0 100 Loss: 0.167 | Test Acc: 95.000% (95/100)\n",
            "10 100 Loss: 0.389 | Test Acc: 89.273% (982/1100)\n",
            "20 100 Loss: 0.409 | Test Acc: 88.714% (1863/2100)\n",
            "30 100 Loss: 0.413 | Test Acc: 88.516% (2744/3100)\n",
            "40 100 Loss: 0.398 | Test Acc: 89.049% (3651/4100)\n",
            "50 100 Loss: 0.398 | Test Acc: 88.863% (4532/5100)\n",
            "60 100 Loss: 0.397 | Test Acc: 88.869% (5421/6100)\n",
            "70 100 Loss: 0.392 | Test Acc: 88.930% (6314/7100)\n",
            "80 100 Loss: 0.388 | Test Acc: 89.000% (7209/8100)\n",
            "90 100 Loss: 0.385 | Test Acc: 88.901% (8090/9100)\n",
            "\n",
            "Epoch: 128\n",
            "0 391 Loss: 0.079 | Train Acc: 96.094% (123/128)\n",
            "10 391 Loss: 0.113 | Train Acc: 95.597% (1346/1408)\n",
            "20 391 Loss: 0.104 | Train Acc: 96.057% (2582/2688)\n",
            "30 391 Loss: 0.104 | Train Acc: 96.220% (3818/3968)\n",
            "40 391 Loss: 0.104 | Train Acc: 96.151% (5046/5248)\n",
            "50 391 Loss: 0.105 | Train Acc: 96.078% (6272/6528)\n",
            "60 391 Loss: 0.101 | Train Acc: 96.324% (7521/7808)\n",
            "70 391 Loss: 0.101 | Train Acc: 96.314% (8753/9088)\n",
            "80 391 Loss: 0.103 | Train Acc: 96.200% (9974/10368)\n",
            "90 391 Loss: 0.104 | Train Acc: 96.205% (11206/11648)\n",
            "100 391 Loss: 0.106 | Train Acc: 96.109% (12425/12928)\n",
            "110 391 Loss: 0.108 | Train Acc: 96.101% (13654/14208)\n",
            "120 391 Loss: 0.111 | Train Acc: 96.049% (14876/15488)\n",
            "130 391 Loss: 0.113 | Train Acc: 96.004% (16098/16768)\n",
            "140 391 Loss: 0.113 | Train Acc: 96.061% (17337/18048)\n",
            "150 391 Loss: 0.114 | Train Acc: 96.047% (18564/19328)\n",
            "160 391 Loss: 0.114 | Train Acc: 96.036% (19791/20608)\n",
            "170 391 Loss: 0.114 | Train Acc: 96.007% (21014/21888)\n",
            "180 391 Loss: 0.114 | Train Acc: 96.042% (22251/23168)\n",
            "190 391 Loss: 0.115 | Train Acc: 96.065% (23486/24448)\n",
            "200 391 Loss: 0.115 | Train Acc: 96.070% (24717/25728)\n",
            "210 391 Loss: 0.114 | Train Acc: 96.079% (25949/27008)\n",
            "220 391 Loss: 0.114 | Train Acc: 96.076% (27178/28288)\n",
            "230 391 Loss: 0.115 | Train Acc: 96.046% (28399/29568)\n",
            "240 391 Loss: 0.115 | Train Acc: 96.061% (29633/30848)\n",
            "250 391 Loss: 0.115 | Train Acc: 96.044% (30857/32128)\n",
            "260 391 Loss: 0.115 | Train Acc: 96.022% (32079/33408)\n",
            "270 391 Loss: 0.115 | Train Acc: 96.013% (33305/34688)\n",
            "280 391 Loss: 0.115 | Train Acc: 96.013% (34534/35968)\n",
            "290 391 Loss: 0.116 | Train Acc: 96.000% (35758/37248)\n",
            "300 391 Loss: 0.116 | Train Acc: 96.018% (36994/38528)\n",
            "310 391 Loss: 0.116 | Train Acc: 96.013% (38221/39808)\n",
            "320 391 Loss: 0.116 | Train Acc: 95.999% (39444/41088)\n",
            "330 391 Loss: 0.117 | Train Acc: 95.964% (40658/42368)\n",
            "340 391 Loss: 0.118 | Train Acc: 95.952% (41881/43648)\n",
            "350 391 Loss: 0.118 | Train Acc: 95.947% (43107/44928)\n",
            "360 391 Loss: 0.118 | Train Acc: 95.944% (44334/46208)\n",
            "370 391 Loss: 0.118 | Train Acc: 95.948% (45564/47488)\n",
            "380 391 Loss: 0.118 | Train Acc: 95.975% (46805/48768)\n",
            "390 391 Loss: 0.118 | Train Acc: 95.972% (47986/50000)\n",
            "0 100 Loss: 0.264 | Test Acc: 90.000% (90/100)\n",
            "10 100 Loss: 0.332 | Test Acc: 90.455% (995/1100)\n",
            "20 100 Loss: 0.340 | Test Acc: 90.286% (1896/2100)\n",
            "30 100 Loss: 0.349 | Test Acc: 90.000% (2790/3100)\n",
            "40 100 Loss: 0.358 | Test Acc: 89.732% (3679/4100)\n",
            "50 100 Loss: 0.357 | Test Acc: 89.627% (4571/5100)\n",
            "60 100 Loss: 0.354 | Test Acc: 89.721% (5473/6100)\n",
            "70 100 Loss: 0.356 | Test Acc: 89.577% (6360/7100)\n",
            "80 100 Loss: 0.351 | Test Acc: 89.728% (7268/8100)\n",
            "90 100 Loss: 0.349 | Test Acc: 89.769% (8169/9100)\n",
            "\n",
            "Epoch: 129\n",
            "0 391 Loss: 0.065 | Train Acc: 98.438% (126/128)\n",
            "10 391 Loss: 0.103 | Train Acc: 96.591% (1360/1408)\n",
            "20 391 Loss: 0.110 | Train Acc: 96.205% (2586/2688)\n",
            "30 391 Loss: 0.113 | Train Acc: 96.018% (3810/3968)\n",
            "40 391 Loss: 0.107 | Train Acc: 96.303% (5054/5248)\n",
            "50 391 Loss: 0.109 | Train Acc: 96.124% (6275/6528)\n",
            "60 391 Loss: 0.111 | Train Acc: 96.004% (7496/7808)\n",
            "70 391 Loss: 0.110 | Train Acc: 96.028% (8727/9088)\n",
            "80 391 Loss: 0.111 | Train Acc: 95.997% (9953/10368)\n",
            "90 391 Loss: 0.110 | Train Acc: 96.102% (11194/11648)\n",
            "100 391 Loss: 0.108 | Train Acc: 96.148% (12430/12928)\n",
            "110 391 Loss: 0.110 | Train Acc: 96.080% (13651/14208)\n",
            "120 391 Loss: 0.110 | Train Acc: 96.055% (14877/15488)\n",
            "130 391 Loss: 0.111 | Train Acc: 96.022% (16101/16768)\n",
            "140 391 Loss: 0.111 | Train Acc: 96.033% (17332/18048)\n",
            "150 391 Loss: 0.112 | Train Acc: 96.021% (18559/19328)\n",
            "160 391 Loss: 0.111 | Train Acc: 96.021% (19788/20608)\n",
            "170 391 Loss: 0.112 | Train Acc: 95.993% (21011/21888)\n",
            "180 391 Loss: 0.113 | Train Acc: 95.982% (22237/23168)\n",
            "190 391 Loss: 0.113 | Train Acc: 95.971% (23463/24448)\n",
            "200 391 Loss: 0.113 | Train Acc: 95.969% (24691/25728)\n",
            "210 391 Loss: 0.114 | Train Acc: 95.957% (25916/27008)\n",
            "220 391 Loss: 0.114 | Train Acc: 95.963% (27146/28288)\n",
            "230 391 Loss: 0.113 | Train Acc: 95.972% (28377/29568)\n",
            "240 391 Loss: 0.114 | Train Acc: 95.987% (29610/30848)\n",
            "250 391 Loss: 0.114 | Train Acc: 96.003% (30844/32128)\n",
            "260 391 Loss: 0.114 | Train Acc: 96.016% (32077/33408)\n",
            "270 391 Loss: 0.115 | Train Acc: 95.958% (33286/34688)\n",
            "280 391 Loss: 0.115 | Train Acc: 95.966% (34517/35968)\n",
            "290 391 Loss: 0.115 | Train Acc: 95.984% (35752/37248)\n",
            "300 391 Loss: 0.115 | Train Acc: 96.003% (36988/38528)\n",
            "310 391 Loss: 0.115 | Train Acc: 95.983% (38209/39808)\n",
            "320 391 Loss: 0.115 | Train Acc: 95.967% (39431/41088)\n",
            "330 391 Loss: 0.116 | Train Acc: 95.954% (40654/42368)\n",
            "340 391 Loss: 0.116 | Train Acc: 95.938% (41875/43648)\n",
            "350 391 Loss: 0.116 | Train Acc: 95.925% (43097/44928)\n",
            "360 391 Loss: 0.117 | Train Acc: 95.890% (44309/46208)\n",
            "370 391 Loss: 0.117 | Train Acc: 95.856% (45520/47488)\n",
            "380 391 Loss: 0.117 | Train Acc: 95.837% (46738/48768)\n",
            "390 391 Loss: 0.118 | Train Acc: 95.822% (47911/50000)\n",
            "0 100 Loss: 0.294 | Test Acc: 91.000% (91/100)\n",
            "10 100 Loss: 0.343 | Test Acc: 90.000% (990/1100)\n",
            "20 100 Loss: 0.361 | Test Acc: 89.667% (1883/2100)\n",
            "30 100 Loss: 0.363 | Test Acc: 89.710% (2781/3100)\n",
            "40 100 Loss: 0.377 | Test Acc: 89.512% (3670/4100)\n",
            "50 100 Loss: 0.380 | Test Acc: 89.216% (4550/5100)\n",
            "60 100 Loss: 0.382 | Test Acc: 89.197% (5441/6100)\n",
            "70 100 Loss: 0.387 | Test Acc: 89.056% (6323/7100)\n",
            "80 100 Loss: 0.381 | Test Acc: 89.185% (7224/8100)\n",
            "90 100 Loss: 0.382 | Test Acc: 89.231% (8120/9100)\n",
            "\n",
            "Epoch: 130\n",
            "0 391 Loss: 0.072 | Train Acc: 96.875% (124/128)\n",
            "10 391 Loss: 0.102 | Train Acc: 96.662% (1361/1408)\n",
            "20 391 Loss: 0.104 | Train Acc: 96.280% (2588/2688)\n",
            "30 391 Loss: 0.104 | Train Acc: 96.396% (3825/3968)\n",
            "40 391 Loss: 0.106 | Train Acc: 96.322% (5055/5248)\n",
            "50 391 Loss: 0.102 | Train Acc: 96.477% (6298/6528)\n",
            "60 391 Loss: 0.103 | Train Acc: 96.401% (7527/7808)\n",
            "70 391 Loss: 0.106 | Train Acc: 96.226% (8745/9088)\n",
            "80 391 Loss: 0.109 | Train Acc: 96.094% (9963/10368)\n",
            "90 391 Loss: 0.109 | Train Acc: 96.077% (11191/11648)\n",
            "100 391 Loss: 0.112 | Train Acc: 96.078% (12421/12928)\n",
            "110 391 Loss: 0.112 | Train Acc: 96.052% (13647/14208)\n",
            "120 391 Loss: 0.112 | Train Acc: 96.107% (14885/15488)\n",
            "130 391 Loss: 0.113 | Train Acc: 96.046% (16105/16768)\n",
            "140 391 Loss: 0.115 | Train Acc: 96.033% (17332/18048)\n",
            "150 391 Loss: 0.116 | Train Acc: 95.970% (18549/19328)\n",
            "160 391 Loss: 0.116 | Train Acc: 96.002% (19784/20608)\n",
            "170 391 Loss: 0.115 | Train Acc: 96.034% (21020/21888)\n",
            "180 391 Loss: 0.115 | Train Acc: 96.029% (22248/23168)\n",
            "190 391 Loss: 0.115 | Train Acc: 96.032% (23478/24448)\n",
            "200 391 Loss: 0.115 | Train Acc: 96.032% (24707/25728)\n",
            "210 391 Loss: 0.116 | Train Acc: 95.994% (25926/27008)\n",
            "220 391 Loss: 0.116 | Train Acc: 95.998% (27156/28288)\n",
            "230 391 Loss: 0.117 | Train Acc: 95.979% (28379/29568)\n",
            "240 391 Loss: 0.116 | Train Acc: 96.006% (29616/30848)\n",
            "250 391 Loss: 0.115 | Train Acc: 96.013% (30847/32128)\n",
            "260 391 Loss: 0.115 | Train Acc: 96.028% (32081/33408)\n",
            "270 391 Loss: 0.114 | Train Acc: 96.045% (33316/34688)\n",
            "280 391 Loss: 0.114 | Train Acc: 96.035% (34542/35968)\n",
            "290 391 Loss: 0.114 | Train Acc: 96.035% (35771/37248)\n",
            "300 391 Loss: 0.114 | Train Acc: 96.024% (36996/38528)\n",
            "310 391 Loss: 0.114 | Train Acc: 96.044% (38233/39808)\n",
            "320 391 Loss: 0.115 | Train Acc: 96.026% (39455/41088)\n",
            "330 391 Loss: 0.117 | Train Acc: 95.966% (40659/42368)\n",
            "340 391 Loss: 0.117 | Train Acc: 95.954% (41882/43648)\n",
            "350 391 Loss: 0.117 | Train Acc: 95.945% (43106/44928)\n",
            "360 391 Loss: 0.118 | Train Acc: 95.940% (44332/46208)\n",
            "370 391 Loss: 0.118 | Train Acc: 95.915% (45548/47488)\n",
            "380 391 Loss: 0.118 | Train Acc: 95.907% (46772/48768)\n",
            "390 391 Loss: 0.118 | Train Acc: 95.922% (47961/50000)\n",
            "0 100 Loss: 0.188 | Test Acc: 94.000% (94/100)\n",
            "10 100 Loss: 0.287 | Test Acc: 91.545% (1007/1100)\n",
            "20 100 Loss: 0.302 | Test Acc: 90.857% (1908/2100)\n",
            "30 100 Loss: 0.324 | Test Acc: 90.323% (2800/3100)\n",
            "40 100 Loss: 0.317 | Test Acc: 90.463% (3709/4100)\n",
            "50 100 Loss: 0.311 | Test Acc: 90.569% (4619/5100)\n",
            "60 100 Loss: 0.311 | Test Acc: 90.574% (5525/6100)\n",
            "70 100 Loss: 0.297 | Test Acc: 90.817% (6448/7100)\n",
            "80 100 Loss: 0.290 | Test Acc: 90.926% (7365/8100)\n",
            "90 100 Loss: 0.287 | Test Acc: 90.978% (8279/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 131\n",
            "0 391 Loss: 0.123 | Train Acc: 95.312% (122/128)\n",
            "10 391 Loss: 0.115 | Train Acc: 96.378% (1357/1408)\n",
            "20 391 Loss: 0.116 | Train Acc: 96.131% (2584/2688)\n",
            "30 391 Loss: 0.109 | Train Acc: 96.295% (3821/3968)\n",
            "40 391 Loss: 0.109 | Train Acc: 96.418% (5060/5248)\n",
            "50 391 Loss: 0.107 | Train Acc: 96.523% (6301/6528)\n",
            "60 391 Loss: 0.107 | Train Acc: 96.363% (7524/7808)\n",
            "70 391 Loss: 0.109 | Train Acc: 96.292% (8751/9088)\n",
            "80 391 Loss: 0.109 | Train Acc: 96.209% (9975/10368)\n",
            "90 391 Loss: 0.110 | Train Acc: 96.188% (11204/11648)\n",
            "100 391 Loss: 0.110 | Train Acc: 96.233% (12441/12928)\n",
            "110 391 Loss: 0.109 | Train Acc: 96.312% (13684/14208)\n",
            "120 391 Loss: 0.108 | Train Acc: 96.339% (14921/15488)\n",
            "130 391 Loss: 0.108 | Train Acc: 96.362% (16158/16768)\n",
            "140 391 Loss: 0.107 | Train Acc: 96.343% (17388/18048)\n",
            "150 391 Loss: 0.109 | Train Acc: 96.316% (18616/19328)\n",
            "160 391 Loss: 0.109 | Train Acc: 96.346% (19855/20608)\n",
            "170 391 Loss: 0.109 | Train Acc: 96.318% (21082/21888)\n",
            "180 391 Loss: 0.109 | Train Acc: 96.292% (22309/23168)\n",
            "190 391 Loss: 0.110 | Train Acc: 96.233% (23527/24448)\n",
            "200 391 Loss: 0.111 | Train Acc: 96.183% (24746/25728)\n",
            "210 391 Loss: 0.110 | Train Acc: 96.216% (25986/27008)\n",
            "220 391 Loss: 0.111 | Train Acc: 96.203% (27214/28288)\n",
            "230 391 Loss: 0.111 | Train Acc: 96.182% (28439/29568)\n",
            "240 391 Loss: 0.111 | Train Acc: 96.185% (29671/30848)\n",
            "250 391 Loss: 0.111 | Train Acc: 96.196% (30906/32128)\n",
            "260 391 Loss: 0.111 | Train Acc: 96.181% (32132/33408)\n",
            "270 391 Loss: 0.111 | Train Acc: 96.180% (33363/34688)\n",
            "280 391 Loss: 0.111 | Train Acc: 96.180% (34594/35968)\n",
            "290 391 Loss: 0.111 | Train Acc: 96.164% (35819/37248)\n",
            "300 391 Loss: 0.112 | Train Acc: 96.151% (37045/38528)\n",
            "310 391 Loss: 0.112 | Train Acc: 96.149% (38275/39808)\n",
            "320 391 Loss: 0.112 | Train Acc: 96.150% (39506/41088)\n",
            "330 391 Loss: 0.112 | Train Acc: 96.160% (40741/42368)\n",
            "340 391 Loss: 0.113 | Train Acc: 96.121% (41955/43648)\n",
            "350 391 Loss: 0.113 | Train Acc: 96.127% (43188/44928)\n",
            "360 391 Loss: 0.113 | Train Acc: 96.126% (44418/46208)\n",
            "370 391 Loss: 0.113 | Train Acc: 96.134% (45652/47488)\n",
            "380 391 Loss: 0.113 | Train Acc: 96.131% (46881/48768)\n",
            "390 391 Loss: 0.113 | Train Acc: 96.152% (48076/50000)\n",
            "0 100 Loss: 0.378 | Test Acc: 89.000% (89/100)\n",
            "10 100 Loss: 0.313 | Test Acc: 90.364% (994/1100)\n",
            "20 100 Loss: 0.306 | Test Acc: 90.381% (1898/2100)\n",
            "30 100 Loss: 0.323 | Test Acc: 90.129% (2794/3100)\n",
            "40 100 Loss: 0.313 | Test Acc: 90.610% (3715/4100)\n",
            "50 100 Loss: 0.305 | Test Acc: 90.863% (4634/5100)\n",
            "60 100 Loss: 0.301 | Test Acc: 91.148% (5560/6100)\n",
            "70 100 Loss: 0.295 | Test Acc: 91.127% (6470/7100)\n",
            "80 100 Loss: 0.291 | Test Acc: 91.309% (7396/8100)\n",
            "90 100 Loss: 0.289 | Test Acc: 91.341% (8312/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 132\n",
            "0 391 Loss: 0.078 | Train Acc: 96.875% (124/128)\n",
            "10 391 Loss: 0.099 | Train Acc: 96.804% (1363/1408)\n",
            "20 391 Loss: 0.090 | Train Acc: 96.987% (2607/2688)\n",
            "30 391 Loss: 0.086 | Train Acc: 97.077% (3852/3968)\n",
            "40 391 Loss: 0.085 | Train Acc: 97.066% (5094/5248)\n",
            "50 391 Loss: 0.087 | Train Acc: 97.074% (6337/6528)\n",
            "60 391 Loss: 0.087 | Train Acc: 97.080% (7580/7808)\n",
            "70 391 Loss: 0.086 | Train Acc: 97.183% (8832/9088)\n",
            "80 391 Loss: 0.086 | Train Acc: 97.164% (10074/10368)\n",
            "90 391 Loss: 0.086 | Train Acc: 97.167% (11318/11648)\n",
            "100 391 Loss: 0.085 | Train Acc: 97.184% (12564/12928)\n",
            "110 391 Loss: 0.085 | Train Acc: 97.171% (13806/14208)\n",
            "120 391 Loss: 0.086 | Train Acc: 97.146% (15046/15488)\n",
            "130 391 Loss: 0.087 | Train Acc: 97.131% (16287/16768)\n",
            "140 391 Loss: 0.087 | Train Acc: 97.052% (17516/18048)\n",
            "150 391 Loss: 0.089 | Train Acc: 97.015% (18751/19328)\n",
            "160 391 Loss: 0.089 | Train Acc: 96.991% (19988/20608)\n",
            "170 391 Loss: 0.091 | Train Acc: 96.934% (21217/21888)\n",
            "180 391 Loss: 0.091 | Train Acc: 96.948% (22461/23168)\n",
            "190 391 Loss: 0.093 | Train Acc: 96.916% (23694/24448)\n",
            "200 391 Loss: 0.094 | Train Acc: 96.879% (24925/25728)\n",
            "210 391 Loss: 0.095 | Train Acc: 96.805% (26145/27008)\n",
            "220 391 Loss: 0.096 | Train Acc: 96.762% (27372/28288)\n",
            "230 391 Loss: 0.096 | Train Acc: 96.726% (28600/29568)\n",
            "240 391 Loss: 0.096 | Train Acc: 96.729% (29839/30848)\n",
            "250 391 Loss: 0.097 | Train Acc: 96.701% (31068/32128)\n",
            "260 391 Loss: 0.097 | Train Acc: 96.701% (32306/33408)\n",
            "270 391 Loss: 0.097 | Train Acc: 96.685% (33538/34688)\n",
            "280 391 Loss: 0.097 | Train Acc: 96.675% (34772/35968)\n",
            "290 391 Loss: 0.098 | Train Acc: 96.658% (36003/37248)\n",
            "300 391 Loss: 0.099 | Train Acc: 96.623% (37227/38528)\n",
            "310 391 Loss: 0.099 | Train Acc: 96.601% (38455/39808)\n",
            "320 391 Loss: 0.099 | Train Acc: 96.612% (39696/41088)\n",
            "330 391 Loss: 0.100 | Train Acc: 96.575% (40917/42368)\n",
            "340 391 Loss: 0.101 | Train Acc: 96.538% (42137/43648)\n",
            "350 391 Loss: 0.102 | Train Acc: 96.494% (43353/44928)\n",
            "360 391 Loss: 0.103 | Train Acc: 96.481% (44582/46208)\n",
            "370 391 Loss: 0.103 | Train Acc: 96.464% (45809/47488)\n",
            "380 391 Loss: 0.103 | Train Acc: 96.457% (47040/48768)\n",
            "390 391 Loss: 0.104 | Train Acc: 96.446% (48223/50000)\n",
            "0 100 Loss: 0.247 | Test Acc: 94.000% (94/100)\n",
            "10 100 Loss: 0.321 | Test Acc: 90.909% (1000/1100)\n",
            "20 100 Loss: 0.351 | Test Acc: 90.143% (1893/2100)\n",
            "30 100 Loss: 0.362 | Test Acc: 90.226% (2797/3100)\n",
            "40 100 Loss: 0.351 | Test Acc: 90.561% (3713/4100)\n",
            "50 100 Loss: 0.342 | Test Acc: 90.647% (4623/5100)\n",
            "60 100 Loss: 0.339 | Test Acc: 90.689% (5532/6100)\n",
            "70 100 Loss: 0.330 | Test Acc: 90.746% (6443/7100)\n",
            "80 100 Loss: 0.320 | Test Acc: 90.975% (7369/8100)\n",
            "90 100 Loss: 0.323 | Test Acc: 90.912% (8273/9100)\n",
            "\n",
            "Epoch: 133\n",
            "0 391 Loss: 0.071 | Train Acc: 96.875% (124/128)\n",
            "10 391 Loss: 0.091 | Train Acc: 96.946% (1365/1408)\n",
            "20 391 Loss: 0.095 | Train Acc: 96.726% (2600/2688)\n",
            "30 391 Loss: 0.097 | Train Acc: 96.648% (3835/3968)\n",
            "40 391 Loss: 0.099 | Train Acc: 96.608% (5070/5248)\n",
            "50 391 Loss: 0.098 | Train Acc: 96.768% (6317/6528)\n",
            "60 391 Loss: 0.100 | Train Acc: 96.721% (7552/7808)\n",
            "70 391 Loss: 0.103 | Train Acc: 96.611% (8780/9088)\n",
            "80 391 Loss: 0.104 | Train Acc: 96.615% (10017/10368)\n",
            "90 391 Loss: 0.105 | Train Acc: 96.566% (11248/11648)\n",
            "100 391 Loss: 0.102 | Train Acc: 96.674% (12498/12928)\n",
            "110 391 Loss: 0.101 | Train Acc: 96.727% (13743/14208)\n",
            "120 391 Loss: 0.102 | Train Acc: 96.701% (14977/15488)\n",
            "130 391 Loss: 0.101 | Train Acc: 96.750% (16223/16768)\n",
            "140 391 Loss: 0.100 | Train Acc: 96.753% (17462/18048)\n",
            "150 391 Loss: 0.099 | Train Acc: 96.756% (18701/19328)\n",
            "160 391 Loss: 0.099 | Train Acc: 96.763% (19941/20608)\n",
            "170 391 Loss: 0.099 | Train Acc: 96.743% (21175/21888)\n",
            "180 391 Loss: 0.099 | Train Acc: 96.702% (22404/23168)\n",
            "190 391 Loss: 0.100 | Train Acc: 96.670% (23634/24448)\n",
            "200 391 Loss: 0.100 | Train Acc: 96.688% (24876/25728)\n",
            "210 391 Loss: 0.100 | Train Acc: 96.679% (26111/27008)\n",
            "220 391 Loss: 0.100 | Train Acc: 96.670% (27346/28288)\n",
            "230 391 Loss: 0.102 | Train Acc: 96.618% (28568/29568)\n",
            "240 391 Loss: 0.102 | Train Acc: 96.599% (29799/30848)\n",
            "250 391 Loss: 0.103 | Train Acc: 96.573% (31027/32128)\n",
            "260 391 Loss: 0.102 | Train Acc: 96.582% (32266/33408)\n",
            "270 391 Loss: 0.102 | Train Acc: 96.569% (33498/34688)\n",
            "280 391 Loss: 0.103 | Train Acc: 96.550% (34727/35968)\n",
            "290 391 Loss: 0.103 | Train Acc: 96.537% (35958/37248)\n",
            "300 391 Loss: 0.103 | Train Acc: 96.545% (37197/38528)\n",
            "310 391 Loss: 0.104 | Train Acc: 96.538% (38430/39808)\n",
            "320 391 Loss: 0.104 | Train Acc: 96.534% (39664/41088)\n",
            "330 391 Loss: 0.104 | Train Acc: 96.526% (40896/42368)\n",
            "340 391 Loss: 0.105 | Train Acc: 96.506% (42123/43648)\n",
            "350 391 Loss: 0.105 | Train Acc: 96.517% (43363/44928)\n",
            "360 391 Loss: 0.105 | Train Acc: 96.492% (44587/46208)\n",
            "370 391 Loss: 0.105 | Train Acc: 96.485% (45819/47488)\n",
            "380 391 Loss: 0.106 | Train Acc: 96.442% (47033/48768)\n",
            "390 391 Loss: 0.106 | Train Acc: 96.426% (48213/50000)\n",
            "0 100 Loss: 0.310 | Test Acc: 91.000% (91/100)\n",
            "10 100 Loss: 0.325 | Test Acc: 91.364% (1005/1100)\n",
            "20 100 Loss: 0.337 | Test Acc: 91.190% (1915/2100)\n",
            "30 100 Loss: 0.345 | Test Acc: 90.935% (2819/3100)\n",
            "40 100 Loss: 0.341 | Test Acc: 90.902% (3727/4100)\n",
            "50 100 Loss: 0.338 | Test Acc: 90.843% (4633/5100)\n",
            "60 100 Loss: 0.342 | Test Acc: 90.820% (5540/6100)\n",
            "70 100 Loss: 0.331 | Test Acc: 90.958% (6458/7100)\n",
            "80 100 Loss: 0.326 | Test Acc: 91.099% (7379/8100)\n",
            "90 100 Loss: 0.323 | Test Acc: 91.088% (8289/9100)\n",
            "\n",
            "Epoch: 134\n",
            "0 391 Loss: 0.081 | Train Acc: 96.094% (123/128)\n",
            "10 391 Loss: 0.090 | Train Acc: 97.088% (1367/1408)\n",
            "20 391 Loss: 0.097 | Train Acc: 96.354% (2590/2688)\n",
            "30 391 Loss: 0.096 | Train Acc: 96.598% (3833/3968)\n",
            "40 391 Loss: 0.095 | Train Acc: 96.684% (5074/5248)\n",
            "50 391 Loss: 0.095 | Train Acc: 96.615% (6307/6528)\n",
            "60 391 Loss: 0.096 | Train Acc: 96.619% (7544/7808)\n",
            "70 391 Loss: 0.099 | Train Acc: 96.512% (8771/9088)\n",
            "80 391 Loss: 0.097 | Train Acc: 96.557% (10011/10368)\n",
            "90 391 Loss: 0.097 | Train Acc: 96.583% (11250/11648)\n",
            "100 391 Loss: 0.097 | Train Acc: 96.550% (12482/12928)\n",
            "110 391 Loss: 0.097 | Train Acc: 96.558% (13719/14208)\n",
            "120 391 Loss: 0.097 | Train Acc: 96.546% (14953/15488)\n",
            "130 391 Loss: 0.097 | Train Acc: 96.589% (16196/16768)\n",
            "140 391 Loss: 0.097 | Train Acc: 96.587% (17432/18048)\n",
            "150 391 Loss: 0.096 | Train Acc: 96.596% (18670/19328)\n",
            "160 391 Loss: 0.095 | Train Acc: 96.628% (19913/20608)\n",
            "170 391 Loss: 0.097 | Train Acc: 96.573% (21138/21888)\n",
            "180 391 Loss: 0.097 | Train Acc: 96.560% (22371/23168)\n",
            "190 391 Loss: 0.097 | Train Acc: 96.572% (23610/24448)\n",
            "200 391 Loss: 0.097 | Train Acc: 96.572% (24846/25728)\n",
            "210 391 Loss: 0.099 | Train Acc: 96.527% (26070/27008)\n",
            "220 391 Loss: 0.100 | Train Acc: 96.454% (27285/28288)\n",
            "230 391 Loss: 0.099 | Train Acc: 96.462% (28522/29568)\n",
            "240 391 Loss: 0.101 | Train Acc: 96.431% (29747/30848)\n",
            "250 391 Loss: 0.101 | Train Acc: 96.414% (30976/32128)\n",
            "260 391 Loss: 0.101 | Train Acc: 96.435% (32217/33408)\n",
            "270 391 Loss: 0.101 | Train Acc: 96.457% (33459/34688)\n",
            "280 391 Loss: 0.101 | Train Acc: 96.461% (34695/35968)\n",
            "290 391 Loss: 0.101 | Train Acc: 96.459% (35929/37248)\n",
            "300 391 Loss: 0.102 | Train Acc: 96.449% (37160/38528)\n",
            "310 391 Loss: 0.102 | Train Acc: 96.468% (38402/39808)\n",
            "320 391 Loss: 0.102 | Train Acc: 96.432% (39622/41088)\n",
            "330 391 Loss: 0.103 | Train Acc: 96.427% (40854/42368)\n",
            "340 391 Loss: 0.103 | Train Acc: 96.424% (42087/43648)\n",
            "350 391 Loss: 0.102 | Train Acc: 96.445% (43331/44928)\n",
            "360 391 Loss: 0.103 | Train Acc: 96.421% (44554/46208)\n",
            "370 391 Loss: 0.103 | Train Acc: 96.420% (45788/47488)\n",
            "380 391 Loss: 0.103 | Train Acc: 96.403% (47014/48768)\n",
            "390 391 Loss: 0.103 | Train Acc: 96.396% (48198/50000)\n",
            "0 100 Loss: 0.283 | Test Acc: 92.000% (92/100)\n",
            "10 100 Loss: 0.326 | Test Acc: 90.182% (992/1100)\n",
            "20 100 Loss: 0.317 | Test Acc: 90.619% (1903/2100)\n",
            "30 100 Loss: 0.343 | Test Acc: 90.290% (2799/3100)\n",
            "40 100 Loss: 0.350 | Test Acc: 90.171% (3697/4100)\n",
            "50 100 Loss: 0.349 | Test Acc: 90.176% (4599/5100)\n",
            "60 100 Loss: 0.339 | Test Acc: 90.197% (5502/6100)\n",
            "70 100 Loss: 0.334 | Test Acc: 90.225% (6406/7100)\n",
            "80 100 Loss: 0.331 | Test Acc: 90.296% (7314/8100)\n",
            "90 100 Loss: 0.328 | Test Acc: 90.308% (8218/9100)\n",
            "\n",
            "Epoch: 135\n",
            "0 391 Loss: 0.041 | Train Acc: 99.219% (127/128)\n",
            "10 391 Loss: 0.075 | Train Acc: 97.159% (1368/1408)\n",
            "20 391 Loss: 0.075 | Train Acc: 97.024% (2608/2688)\n",
            "30 391 Loss: 0.083 | Train Acc: 97.001% (3849/3968)\n",
            "40 391 Loss: 0.079 | Train Acc: 97.104% (5096/5248)\n",
            "50 391 Loss: 0.078 | Train Acc: 97.197% (6345/6528)\n",
            "60 391 Loss: 0.078 | Train Acc: 97.195% (7589/7808)\n",
            "70 391 Loss: 0.078 | Train Acc: 97.161% (8830/9088)\n",
            "80 391 Loss: 0.081 | Train Acc: 97.087% (10066/10368)\n",
            "90 391 Loss: 0.083 | Train Acc: 97.064% (11306/11648)\n",
            "100 391 Loss: 0.085 | Train Acc: 97.076% (12550/12928)\n",
            "110 391 Loss: 0.087 | Train Acc: 97.016% (13784/14208)\n",
            "120 391 Loss: 0.088 | Train Acc: 96.952% (15016/15488)\n",
            "130 391 Loss: 0.089 | Train Acc: 96.953% (16257/16768)\n",
            "140 391 Loss: 0.090 | Train Acc: 96.897% (17488/18048)\n",
            "150 391 Loss: 0.089 | Train Acc: 96.896% (18728/19328)\n",
            "160 391 Loss: 0.091 | Train Acc: 96.817% (19952/20608)\n",
            "170 391 Loss: 0.091 | Train Acc: 96.829% (21194/21888)\n",
            "180 391 Loss: 0.092 | Train Acc: 96.810% (22429/23168)\n",
            "190 391 Loss: 0.091 | Train Acc: 96.826% (23672/24448)\n",
            "200 391 Loss: 0.092 | Train Acc: 96.793% (24903/25728)\n",
            "210 391 Loss: 0.093 | Train Acc: 96.782% (26139/27008)\n",
            "220 391 Loss: 0.093 | Train Acc: 96.765% (27373/28288)\n",
            "230 391 Loss: 0.093 | Train Acc: 96.774% (28614/29568)\n",
            "240 391 Loss: 0.093 | Train Acc: 96.745% (29844/30848)\n",
            "250 391 Loss: 0.095 | Train Acc: 96.698% (31067/32128)\n",
            "260 391 Loss: 0.095 | Train Acc: 96.686% (32301/33408)\n",
            "270 391 Loss: 0.095 | Train Acc: 96.696% (33542/34688)\n",
            "280 391 Loss: 0.095 | Train Acc: 96.689% (34777/35968)\n",
            "290 391 Loss: 0.095 | Train Acc: 96.684% (36013/37248)\n",
            "300 391 Loss: 0.096 | Train Acc: 96.670% (37245/38528)\n",
            "310 391 Loss: 0.095 | Train Acc: 96.697% (38493/39808)\n",
            "320 391 Loss: 0.095 | Train Acc: 96.688% (39727/41088)\n",
            "330 391 Loss: 0.096 | Train Acc: 96.663% (40954/42368)\n",
            "340 391 Loss: 0.097 | Train Acc: 96.637% (42180/43648)\n",
            "350 391 Loss: 0.098 | Train Acc: 96.588% (43395/44928)\n",
            "360 391 Loss: 0.099 | Train Acc: 96.574% (44625/46208)\n",
            "370 391 Loss: 0.099 | Train Acc: 96.553% (45851/47488)\n",
            "380 391 Loss: 0.100 | Train Acc: 96.537% (47079/48768)\n",
            "390 391 Loss: 0.100 | Train Acc: 96.538% (48269/50000)\n",
            "0 100 Loss: 0.401 | Test Acc: 85.000% (85/100)\n",
            "10 100 Loss: 0.307 | Test Acc: 90.545% (996/1100)\n",
            "20 100 Loss: 0.321 | Test Acc: 90.095% (1892/2100)\n",
            "30 100 Loss: 0.330 | Test Acc: 90.161% (2795/3100)\n",
            "40 100 Loss: 0.333 | Test Acc: 90.341% (3704/4100)\n",
            "50 100 Loss: 0.333 | Test Acc: 90.196% (4600/5100)\n",
            "60 100 Loss: 0.326 | Test Acc: 90.475% (5519/6100)\n",
            "70 100 Loss: 0.322 | Test Acc: 90.535% (6428/7100)\n",
            "80 100 Loss: 0.318 | Test Acc: 90.630% (7341/8100)\n",
            "90 100 Loss: 0.321 | Test Acc: 90.637% (8248/9100)\n",
            "\n",
            "Epoch: 136\n",
            "0 391 Loss: 0.091 | Train Acc: 96.094% (123/128)\n",
            "10 391 Loss: 0.114 | Train Acc: 96.449% (1358/1408)\n",
            "20 391 Loss: 0.104 | Train Acc: 96.503% (2594/2688)\n",
            "30 391 Loss: 0.105 | Train Acc: 96.497% (3829/3968)\n",
            "40 391 Loss: 0.101 | Train Acc: 96.627% (5071/5248)\n",
            "50 391 Loss: 0.097 | Train Acc: 96.691% (6312/6528)\n",
            "60 391 Loss: 0.095 | Train Acc: 96.760% (7555/7808)\n",
            "70 391 Loss: 0.093 | Train Acc: 96.809% (8798/9088)\n",
            "80 391 Loss: 0.094 | Train Acc: 96.750% (10031/10368)\n",
            "90 391 Loss: 0.093 | Train Acc: 96.815% (11277/11648)\n",
            "100 391 Loss: 0.093 | Train Acc: 96.836% (12519/12928)\n",
            "110 391 Loss: 0.092 | Train Acc: 96.868% (13763/14208)\n",
            "120 391 Loss: 0.091 | Train Acc: 96.894% (15007/15488)\n",
            "130 391 Loss: 0.090 | Train Acc: 96.923% (16252/16768)\n",
            "140 391 Loss: 0.090 | Train Acc: 96.964% (17500/18048)\n",
            "150 391 Loss: 0.090 | Train Acc: 96.927% (18734/19328)\n",
            "160 391 Loss: 0.091 | Train Acc: 96.928% (19975/20608)\n",
            "170 391 Loss: 0.091 | Train Acc: 96.902% (21210/21888)\n",
            "180 391 Loss: 0.093 | Train Acc: 96.832% (22434/23168)\n",
            "190 391 Loss: 0.093 | Train Acc: 96.822% (23671/24448)\n",
            "200 391 Loss: 0.094 | Train Acc: 96.786% (24901/25728)\n",
            "210 391 Loss: 0.094 | Train Acc: 96.768% (26135/27008)\n",
            "220 391 Loss: 0.095 | Train Acc: 96.765% (27373/28288)\n",
            "230 391 Loss: 0.094 | Train Acc: 96.780% (28616/29568)\n",
            "240 391 Loss: 0.095 | Train Acc: 96.765% (29850/30848)\n",
            "250 391 Loss: 0.095 | Train Acc: 96.750% (31084/32128)\n",
            "260 391 Loss: 0.095 | Train Acc: 96.731% (32316/33408)\n",
            "270 391 Loss: 0.097 | Train Acc: 96.682% (33537/34688)\n",
            "280 391 Loss: 0.097 | Train Acc: 96.678% (34773/35968)\n",
            "290 391 Loss: 0.097 | Train Acc: 96.671% (36008/37248)\n",
            "300 391 Loss: 0.098 | Train Acc: 96.649% (37237/38528)\n",
            "310 391 Loss: 0.098 | Train Acc: 96.646% (38473/39808)\n",
            "320 391 Loss: 0.098 | Train Acc: 96.632% (39704/41088)\n",
            "330 391 Loss: 0.098 | Train Acc: 96.639% (40944/42368)\n",
            "340 391 Loss: 0.098 | Train Acc: 96.639% (42181/43648)\n",
            "350 391 Loss: 0.097 | Train Acc: 96.644% (43420/44928)\n",
            "360 391 Loss: 0.097 | Train Acc: 96.643% (44657/46208)\n",
            "370 391 Loss: 0.097 | Train Acc: 96.673% (45908/47488)\n",
            "380 391 Loss: 0.097 | Train Acc: 96.674% (47146/48768)\n",
            "390 391 Loss: 0.097 | Train Acc: 96.654% (48327/50000)\n",
            "0 100 Loss: 0.393 | Test Acc: 90.000% (90/100)\n",
            "10 100 Loss: 0.353 | Test Acc: 90.091% (991/1100)\n",
            "20 100 Loss: 0.345 | Test Acc: 90.429% (1899/2100)\n",
            "30 100 Loss: 0.351 | Test Acc: 90.710% (2812/3100)\n",
            "40 100 Loss: 0.347 | Test Acc: 90.707% (3719/4100)\n",
            "50 100 Loss: 0.337 | Test Acc: 90.784% (4630/5100)\n",
            "60 100 Loss: 0.331 | Test Acc: 90.869% (5543/6100)\n",
            "70 100 Loss: 0.316 | Test Acc: 91.042% (6464/7100)\n",
            "80 100 Loss: 0.313 | Test Acc: 91.111% (7380/8100)\n",
            "90 100 Loss: 0.312 | Test Acc: 91.253% (8304/9100)\n",
            "\n",
            "Epoch: 137\n",
            "0 391 Loss: 0.111 | Train Acc: 95.312% (122/128)\n",
            "10 391 Loss: 0.084 | Train Acc: 96.733% (1362/1408)\n",
            "20 391 Loss: 0.088 | Train Acc: 96.912% (2605/2688)\n",
            "30 391 Loss: 0.086 | Train Acc: 96.875% (3844/3968)\n",
            "40 391 Loss: 0.088 | Train Acc: 96.951% (5088/5248)\n",
            "50 391 Loss: 0.085 | Train Acc: 97.028% (6334/6528)\n",
            "60 391 Loss: 0.084 | Train Acc: 97.029% (7576/7808)\n",
            "70 391 Loss: 0.083 | Train Acc: 97.106% (8825/9088)\n",
            "80 391 Loss: 0.081 | Train Acc: 97.164% (10074/10368)\n",
            "90 391 Loss: 0.081 | Train Acc: 97.150% (11316/11648)\n",
            "100 391 Loss: 0.082 | Train Acc: 97.177% (12563/12928)\n",
            "110 391 Loss: 0.081 | Train Acc: 97.192% (13809/14208)\n",
            "120 391 Loss: 0.082 | Train Acc: 97.159% (15048/15488)\n",
            "130 391 Loss: 0.082 | Train Acc: 97.161% (16292/16768)\n",
            "140 391 Loss: 0.081 | Train Acc: 97.213% (17545/18048)\n",
            "150 391 Loss: 0.082 | Train Acc: 97.185% (18784/19328)\n",
            "160 391 Loss: 0.083 | Train Acc: 97.113% (20013/20608)\n",
            "170 391 Loss: 0.082 | Train Acc: 97.113% (21256/21888)\n",
            "180 391 Loss: 0.082 | Train Acc: 97.130% (22503/23168)\n",
            "190 391 Loss: 0.082 | Train Acc: 97.120% (23744/24448)\n",
            "200 391 Loss: 0.082 | Train Acc: 97.135% (24991/25728)\n",
            "210 391 Loss: 0.083 | Train Acc: 97.101% (26225/27008)\n",
            "220 391 Loss: 0.084 | Train Acc: 97.066% (27458/28288)\n",
            "230 391 Loss: 0.086 | Train Acc: 97.004% (28682/29568)\n",
            "240 391 Loss: 0.086 | Train Acc: 96.982% (29917/30848)\n",
            "250 391 Loss: 0.088 | Train Acc: 96.900% (31132/32128)\n",
            "260 391 Loss: 0.089 | Train Acc: 96.842% (32353/33408)\n",
            "270 391 Loss: 0.090 | Train Acc: 96.812% (33582/34688)\n",
            "280 391 Loss: 0.092 | Train Acc: 96.747% (34798/35968)\n",
            "290 391 Loss: 0.093 | Train Acc: 96.738% (36033/37248)\n",
            "300 391 Loss: 0.093 | Train Acc: 96.740% (37272/38528)\n",
            "310 391 Loss: 0.093 | Train Acc: 96.719% (38502/39808)\n",
            "320 391 Loss: 0.093 | Train Acc: 96.714% (39738/41088)\n",
            "330 391 Loss: 0.094 | Train Acc: 96.712% (40975/42368)\n",
            "340 391 Loss: 0.094 | Train Acc: 96.708% (42211/43648)\n",
            "350 391 Loss: 0.094 | Train Acc: 96.719% (43454/44928)\n",
            "360 391 Loss: 0.094 | Train Acc: 96.700% (44683/46208)\n",
            "370 391 Loss: 0.094 | Train Acc: 96.690% (45916/47488)\n",
            "380 391 Loss: 0.094 | Train Acc: 96.701% (47159/48768)\n",
            "390 391 Loss: 0.094 | Train Acc: 96.700% (48350/50000)\n",
            "0 100 Loss: 0.232 | Test Acc: 92.000% (92/100)\n",
            "10 100 Loss: 0.305 | Test Acc: 91.000% (1001/1100)\n",
            "20 100 Loss: 0.304 | Test Acc: 91.714% (1926/2100)\n",
            "30 100 Loss: 0.317 | Test Acc: 91.452% (2835/3100)\n",
            "40 100 Loss: 0.309 | Test Acc: 91.317% (3744/4100)\n",
            "50 100 Loss: 0.306 | Test Acc: 91.314% (4657/5100)\n",
            "60 100 Loss: 0.301 | Test Acc: 91.393% (5575/6100)\n",
            "70 100 Loss: 0.299 | Test Acc: 91.423% (6491/7100)\n",
            "80 100 Loss: 0.295 | Test Acc: 91.469% (7409/8100)\n",
            "90 100 Loss: 0.298 | Test Acc: 91.297% (8308/9100)\n",
            "\n",
            "Epoch: 138\n",
            "0 391 Loss: 0.070 | Train Acc: 96.875% (124/128)\n",
            "10 391 Loss: 0.088 | Train Acc: 97.159% (1368/1408)\n",
            "20 391 Loss: 0.081 | Train Acc: 97.359% (2617/2688)\n",
            "30 391 Loss: 0.077 | Train Acc: 97.404% (3865/3968)\n",
            "40 391 Loss: 0.072 | Train Acc: 97.523% (5118/5248)\n",
            "50 391 Loss: 0.073 | Train Acc: 97.396% (6358/6528)\n",
            "60 391 Loss: 0.076 | Train Acc: 97.234% (7592/7808)\n",
            "70 391 Loss: 0.078 | Train Acc: 97.128% (8827/9088)\n",
            "80 391 Loss: 0.078 | Train Acc: 97.116% (10069/10368)\n",
            "90 391 Loss: 0.078 | Train Acc: 97.107% (11311/11648)\n",
            "100 391 Loss: 0.079 | Train Acc: 97.061% (12548/12928)\n",
            "110 391 Loss: 0.080 | Train Acc: 97.065% (13791/14208)\n",
            "120 391 Loss: 0.079 | Train Acc: 97.101% (15039/15488)\n",
            "130 391 Loss: 0.080 | Train Acc: 97.102% (16282/16768)\n",
            "140 391 Loss: 0.081 | Train Acc: 97.041% (17514/18048)\n",
            "150 391 Loss: 0.082 | Train Acc: 97.015% (18751/19328)\n",
            "160 391 Loss: 0.082 | Train Acc: 97.040% (19998/20608)\n",
            "170 391 Loss: 0.082 | Train Acc: 97.035% (21239/21888)\n",
            "180 391 Loss: 0.083 | Train Acc: 97.004% (22474/23168)\n",
            "190 391 Loss: 0.082 | Train Acc: 97.063% (23730/24448)\n",
            "200 391 Loss: 0.082 | Train Acc: 97.073% (24975/25728)\n",
            "210 391 Loss: 0.081 | Train Acc: 97.108% (26227/27008)\n",
            "220 391 Loss: 0.081 | Train Acc: 97.105% (27469/28288)\n",
            "230 391 Loss: 0.082 | Train Acc: 97.064% (28700/29568)\n",
            "240 391 Loss: 0.082 | Train Acc: 97.082% (29948/30848)\n",
            "250 391 Loss: 0.085 | Train Acc: 97.031% (31174/32128)\n",
            "260 391 Loss: 0.086 | Train Acc: 96.995% (32404/33408)\n",
            "270 391 Loss: 0.086 | Train Acc: 96.961% (33634/34688)\n",
            "280 391 Loss: 0.086 | Train Acc: 96.953% (34872/35968)\n",
            "290 391 Loss: 0.088 | Train Acc: 96.918% (36100/37248)\n",
            "300 391 Loss: 0.088 | Train Acc: 96.901% (37334/38528)\n",
            "310 391 Loss: 0.088 | Train Acc: 96.895% (38572/39808)\n",
            "320 391 Loss: 0.089 | Train Acc: 96.882% (39807/41088)\n",
            "330 391 Loss: 0.088 | Train Acc: 96.887% (41049/42368)\n",
            "340 391 Loss: 0.089 | Train Acc: 96.859% (42277/43648)\n",
            "350 391 Loss: 0.089 | Train Acc: 96.846% (43511/44928)\n",
            "360 391 Loss: 0.090 | Train Acc: 96.853% (44754/46208)\n",
            "370 391 Loss: 0.090 | Train Acc: 96.846% (45990/47488)\n",
            "380 391 Loss: 0.089 | Train Acc: 96.865% (47239/48768)\n",
            "390 391 Loss: 0.090 | Train Acc: 96.838% (48419/50000)\n",
            "0 100 Loss: 0.477 | Test Acc: 85.000% (85/100)\n",
            "10 100 Loss: 0.324 | Test Acc: 90.909% (1000/1100)\n",
            "20 100 Loss: 0.339 | Test Acc: 90.095% (1892/2100)\n",
            "30 100 Loss: 0.340 | Test Acc: 90.258% (2798/3100)\n",
            "40 100 Loss: 0.336 | Test Acc: 90.293% (3702/4100)\n",
            "50 100 Loss: 0.340 | Test Acc: 90.333% (4607/5100)\n",
            "60 100 Loss: 0.338 | Test Acc: 90.541% (5523/6100)\n",
            "70 100 Loss: 0.336 | Test Acc: 90.493% (6425/7100)\n",
            "80 100 Loss: 0.332 | Test Acc: 90.617% (7340/8100)\n",
            "90 100 Loss: 0.334 | Test Acc: 90.462% (8232/9100)\n",
            "\n",
            "Epoch: 139\n",
            "0 391 Loss: 0.079 | Train Acc: 97.656% (125/128)\n",
            "10 391 Loss: 0.081 | Train Acc: 97.017% (1366/1408)\n",
            "20 391 Loss: 0.084 | Train Acc: 96.875% (2604/2688)\n",
            "30 391 Loss: 0.083 | Train Acc: 97.152% (3855/3968)\n",
            "40 391 Loss: 0.086 | Train Acc: 97.123% (5097/5248)\n",
            "50 391 Loss: 0.084 | Train Acc: 97.227% (6347/6528)\n",
            "60 391 Loss: 0.082 | Train Acc: 97.246% (7593/7808)\n",
            "70 391 Loss: 0.083 | Train Acc: 97.183% (8832/9088)\n",
            "80 391 Loss: 0.080 | Train Acc: 97.242% (10082/10368)\n",
            "90 391 Loss: 0.082 | Train Acc: 97.150% (11316/11648)\n",
            "100 391 Loss: 0.083 | Train Acc: 97.138% (12558/12928)\n",
            "110 391 Loss: 0.082 | Train Acc: 97.121% (13799/14208)\n",
            "120 391 Loss: 0.084 | Train Acc: 97.062% (15033/15488)\n",
            "130 391 Loss: 0.083 | Train Acc: 97.066% (16276/16768)\n",
            "140 391 Loss: 0.083 | Train Acc: 97.063% (17518/18048)\n",
            "150 391 Loss: 0.084 | Train Acc: 97.066% (18761/19328)\n",
            "160 391 Loss: 0.084 | Train Acc: 97.074% (20005/20608)\n",
            "170 391 Loss: 0.084 | Train Acc: 97.044% (21241/21888)\n",
            "180 391 Loss: 0.084 | Train Acc: 97.069% (22489/23168)\n",
            "190 391 Loss: 0.085 | Train Acc: 97.026% (23721/24448)\n",
            "200 391 Loss: 0.086 | Train Acc: 97.011% (24959/25728)\n",
            "210 391 Loss: 0.086 | Train Acc: 96.979% (26192/27008)\n",
            "220 391 Loss: 0.087 | Train Acc: 96.956% (27427/28288)\n",
            "230 391 Loss: 0.088 | Train Acc: 96.899% (28651/29568)\n",
            "240 391 Loss: 0.089 | Train Acc: 96.878% (29885/30848)\n",
            "250 391 Loss: 0.089 | Train Acc: 96.863% (31120/32128)\n",
            "260 391 Loss: 0.090 | Train Acc: 96.824% (32347/33408)\n",
            "270 391 Loss: 0.090 | Train Acc: 96.823% (33586/34688)\n",
            "280 391 Loss: 0.091 | Train Acc: 96.817% (34823/35968)\n",
            "290 391 Loss: 0.090 | Train Acc: 96.819% (36063/37248)\n",
            "300 391 Loss: 0.090 | Train Acc: 96.836% (37309/38528)\n",
            "310 391 Loss: 0.090 | Train Acc: 96.845% (38552/39808)\n",
            "320 391 Loss: 0.090 | Train Acc: 96.834% (39787/41088)\n",
            "330 391 Loss: 0.090 | Train Acc: 96.840% (41029/42368)\n",
            "340 391 Loss: 0.090 | Train Acc: 96.818% (42259/43648)\n",
            "350 391 Loss: 0.090 | Train Acc: 96.828% (43503/44928)\n",
            "360 391 Loss: 0.090 | Train Acc: 96.817% (44737/46208)\n",
            "370 391 Loss: 0.091 | Train Acc: 96.793% (45965/47488)\n",
            "380 391 Loss: 0.091 | Train Acc: 96.801% (47208/48768)\n",
            "390 391 Loss: 0.091 | Train Acc: 96.780% (48390/50000)\n",
            "0 100 Loss: 0.161 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.310 | Test Acc: 90.909% (1000/1100)\n",
            "20 100 Loss: 0.327 | Test Acc: 90.571% (1902/2100)\n",
            "30 100 Loss: 0.354 | Test Acc: 90.548% (2807/3100)\n",
            "40 100 Loss: 0.354 | Test Acc: 90.268% (3701/4100)\n",
            "50 100 Loss: 0.350 | Test Acc: 90.235% (4602/5100)\n",
            "60 100 Loss: 0.348 | Test Acc: 90.295% (5508/6100)\n",
            "70 100 Loss: 0.341 | Test Acc: 90.535% (6428/7100)\n",
            "80 100 Loss: 0.342 | Test Acc: 90.457% (7327/8100)\n",
            "90 100 Loss: 0.338 | Test Acc: 90.538% (8239/9100)\n",
            "\n",
            "Epoch: 140\n",
            "0 391 Loss: 0.125 | Train Acc: 93.750% (120/128)\n",
            "10 391 Loss: 0.065 | Train Acc: 97.798% (1377/1408)\n",
            "20 391 Loss: 0.059 | Train Acc: 98.140% (2638/2688)\n",
            "30 391 Loss: 0.064 | Train Acc: 97.959% (3887/3968)\n",
            "40 391 Loss: 0.069 | Train Acc: 97.790% (5132/5248)\n",
            "50 391 Loss: 0.070 | Train Acc: 97.672% (6376/6528)\n",
            "60 391 Loss: 0.068 | Train Acc: 97.759% (7633/7808)\n",
            "70 391 Loss: 0.072 | Train Acc: 97.568% (8867/9088)\n",
            "80 391 Loss: 0.073 | Train Acc: 97.531% (10112/10368)\n",
            "90 391 Loss: 0.074 | Train Acc: 97.467% (11353/11648)\n",
            "100 391 Loss: 0.074 | Train Acc: 97.463% (12600/12928)\n",
            "110 391 Loss: 0.074 | Train Acc: 97.501% (13853/14208)\n",
            "120 391 Loss: 0.075 | Train Acc: 97.463% (15095/15488)\n",
            "130 391 Loss: 0.075 | Train Acc: 97.412% (16334/16768)\n",
            "140 391 Loss: 0.075 | Train Acc: 97.396% (17578/18048)\n",
            "150 391 Loss: 0.076 | Train Acc: 97.351% (18816/19328)\n",
            "160 391 Loss: 0.077 | Train Acc: 97.331% (20058/20608)\n",
            "170 391 Loss: 0.076 | Train Acc: 97.359% (21310/21888)\n",
            "180 391 Loss: 0.077 | Train Acc: 97.350% (22554/23168)\n",
            "190 391 Loss: 0.078 | Train Acc: 97.296% (23787/24448)\n",
            "200 391 Loss: 0.079 | Train Acc: 97.217% (25012/25728)\n",
            "210 391 Loss: 0.079 | Train Acc: 97.208% (26254/27008)\n",
            "220 391 Loss: 0.079 | Train Acc: 97.193% (27494/28288)\n",
            "230 391 Loss: 0.080 | Train Acc: 97.169% (28731/29568)\n",
            "240 391 Loss: 0.080 | Train Acc: 97.160% (29972/30848)\n",
            "250 391 Loss: 0.081 | Train Acc: 97.121% (31203/32128)\n",
            "260 391 Loss: 0.081 | Train Acc: 97.144% (32454/33408)\n",
            "270 391 Loss: 0.081 | Train Acc: 97.123% (33690/34688)\n",
            "280 391 Loss: 0.083 | Train Acc: 97.070% (34914/35968)\n",
            "290 391 Loss: 0.083 | Train Acc: 97.076% (36159/37248)\n",
            "300 391 Loss: 0.083 | Train Acc: 97.075% (37401/38528)\n",
            "310 391 Loss: 0.083 | Train Acc: 97.089% (38649/39808)\n",
            "320 391 Loss: 0.083 | Train Acc: 97.079% (39888/41088)\n",
            "330 391 Loss: 0.083 | Train Acc: 97.083% (41132/42368)\n",
            "340 391 Loss: 0.083 | Train Acc: 97.077% (42372/43648)\n",
            "350 391 Loss: 0.083 | Train Acc: 97.082% (43617/44928)\n",
            "360 391 Loss: 0.084 | Train Acc: 97.070% (44854/46208)\n",
            "370 391 Loss: 0.084 | Train Acc: 97.060% (46092/47488)\n",
            "380 391 Loss: 0.084 | Train Acc: 97.060% (47334/48768)\n",
            "390 391 Loss: 0.084 | Train Acc: 97.062% (48531/50000)\n",
            "0 100 Loss: 0.247 | Test Acc: 93.000% (93/100)\n",
            "10 100 Loss: 0.371 | Test Acc: 89.636% (986/1100)\n",
            "20 100 Loss: 0.359 | Test Acc: 89.810% (1886/2100)\n",
            "30 100 Loss: 0.392 | Test Acc: 89.194% (2765/3100)\n",
            "40 100 Loss: 0.395 | Test Acc: 89.220% (3658/4100)\n",
            "50 100 Loss: 0.388 | Test Acc: 89.471% (4563/5100)\n",
            "60 100 Loss: 0.385 | Test Acc: 89.443% (5456/6100)\n",
            "70 100 Loss: 0.380 | Test Acc: 89.479% (6353/7100)\n",
            "80 100 Loss: 0.371 | Test Acc: 89.568% (7255/8100)\n",
            "90 100 Loss: 0.370 | Test Acc: 89.516% (8146/9100)\n",
            "\n",
            "Epoch: 141\n",
            "0 391 Loss: 0.067 | Train Acc: 96.875% (124/128)\n",
            "10 391 Loss: 0.080 | Train Acc: 96.591% (1360/1408)\n",
            "20 391 Loss: 0.080 | Train Acc: 97.061% (2609/2688)\n",
            "30 391 Loss: 0.088 | Train Acc: 96.850% (3843/3968)\n",
            "40 391 Loss: 0.087 | Train Acc: 96.875% (5084/5248)\n",
            "50 391 Loss: 0.083 | Train Acc: 97.089% (6338/6528)\n",
            "60 391 Loss: 0.080 | Train Acc: 97.195% (7589/7808)\n",
            "70 391 Loss: 0.079 | Train Acc: 97.194% (8833/9088)\n",
            "80 391 Loss: 0.080 | Train Acc: 97.213% (10079/10368)\n",
            "90 391 Loss: 0.080 | Train Acc: 97.184% (11320/11648)\n",
            "100 391 Loss: 0.079 | Train Acc: 97.246% (12572/12928)\n",
            "110 391 Loss: 0.079 | Train Acc: 97.192% (13809/14208)\n",
            "120 391 Loss: 0.079 | Train Acc: 97.243% (15061/15488)\n",
            "130 391 Loss: 0.079 | Train Acc: 97.233% (16304/16768)\n",
            "140 391 Loss: 0.079 | Train Acc: 97.230% (17548/18048)\n",
            "150 391 Loss: 0.079 | Train Acc: 97.222% (18791/19328)\n",
            "160 391 Loss: 0.080 | Train Acc: 97.205% (20032/20608)\n",
            "170 391 Loss: 0.080 | Train Acc: 97.204% (21276/21888)\n",
            "180 391 Loss: 0.080 | Train Acc: 97.164% (22511/23168)\n",
            "190 391 Loss: 0.081 | Train Acc: 97.112% (23742/24448)\n",
            "200 391 Loss: 0.081 | Train Acc: 97.128% (24989/25728)\n",
            "210 391 Loss: 0.081 | Train Acc: 97.138% (26235/27008)\n",
            "220 391 Loss: 0.081 | Train Acc: 97.137% (27478/28288)\n",
            "230 391 Loss: 0.081 | Train Acc: 97.119% (28716/29568)\n",
            "240 391 Loss: 0.081 | Train Acc: 97.112% (29957/30848)\n",
            "250 391 Loss: 0.081 | Train Acc: 97.102% (31197/32128)\n",
            "260 391 Loss: 0.082 | Train Acc: 97.079% (32432/33408)\n",
            "270 391 Loss: 0.082 | Train Acc: 97.068% (33671/34688)\n",
            "280 391 Loss: 0.081 | Train Acc: 97.111% (34929/35968)\n",
            "290 391 Loss: 0.081 | Train Acc: 97.119% (36175/37248)\n",
            "300 391 Loss: 0.081 | Train Acc: 97.135% (37424/38528)\n",
            "310 391 Loss: 0.081 | Train Acc: 97.151% (38674/39808)\n",
            "320 391 Loss: 0.081 | Train Acc: 97.148% (39916/41088)\n",
            "330 391 Loss: 0.081 | Train Acc: 97.120% (41148/42368)\n",
            "340 391 Loss: 0.081 | Train Acc: 97.138% (42399/43648)\n",
            "350 391 Loss: 0.081 | Train Acc: 97.144% (43645/44928)\n",
            "360 391 Loss: 0.081 | Train Acc: 97.148% (44890/46208)\n",
            "370 391 Loss: 0.081 | Train Acc: 97.147% (46133/47488)\n",
            "380 391 Loss: 0.081 | Train Acc: 97.146% (47376/48768)\n",
            "390 391 Loss: 0.081 | Train Acc: 97.164% (48582/50000)\n",
            "0 100 Loss: 0.285 | Test Acc: 91.000% (91/100)\n",
            "10 100 Loss: 0.362 | Test Acc: 89.636% (986/1100)\n",
            "20 100 Loss: 0.334 | Test Acc: 90.286% (1896/2100)\n",
            "30 100 Loss: 0.343 | Test Acc: 90.355% (2801/3100)\n",
            "40 100 Loss: 0.347 | Test Acc: 90.439% (3708/4100)\n",
            "50 100 Loss: 0.343 | Test Acc: 90.412% (4611/5100)\n",
            "60 100 Loss: 0.340 | Test Acc: 90.541% (5523/6100)\n",
            "70 100 Loss: 0.337 | Test Acc: 90.437% (6421/7100)\n",
            "80 100 Loss: 0.327 | Test Acc: 90.691% (7346/8100)\n",
            "90 100 Loss: 0.328 | Test Acc: 90.626% (8247/9100)\n",
            "\n",
            "Epoch: 142\n",
            "0 391 Loss: 0.111 | Train Acc: 97.656% (125/128)\n",
            "10 391 Loss: 0.078 | Train Acc: 97.443% (1372/1408)\n",
            "20 391 Loss: 0.076 | Train Acc: 97.582% (2623/2688)\n",
            "30 391 Loss: 0.076 | Train Acc: 97.606% (3873/3968)\n",
            "40 391 Loss: 0.076 | Train Acc: 97.580% (5121/5248)\n",
            "50 391 Loss: 0.074 | Train Acc: 97.595% (6371/6528)\n",
            "60 391 Loss: 0.072 | Train Acc: 97.643% (7624/7808)\n",
            "70 391 Loss: 0.071 | Train Acc: 97.645% (8874/9088)\n",
            "80 391 Loss: 0.070 | Train Acc: 97.666% (10126/10368)\n",
            "90 391 Loss: 0.071 | Train Acc: 97.596% (11368/11648)\n",
            "100 391 Loss: 0.071 | Train Acc: 97.579% (12615/12928)\n",
            "110 391 Loss: 0.071 | Train Acc: 97.600% (13867/14208)\n",
            "120 391 Loss: 0.070 | Train Acc: 97.605% (15117/15488)\n",
            "130 391 Loss: 0.071 | Train Acc: 97.585% (16363/16768)\n",
            "140 391 Loss: 0.071 | Train Acc: 97.601% (17615/18048)\n",
            "150 391 Loss: 0.072 | Train Acc: 97.558% (18856/19328)\n",
            "160 391 Loss: 0.071 | Train Acc: 97.579% (20109/20608)\n",
            "170 391 Loss: 0.071 | Train Acc: 97.606% (21364/21888)\n",
            "180 391 Loss: 0.070 | Train Acc: 97.635% (22620/23168)\n",
            "190 391 Loss: 0.070 | Train Acc: 97.636% (23870/24448)\n",
            "200 391 Loss: 0.072 | Train Acc: 97.586% (25107/25728)\n",
            "210 391 Loss: 0.073 | Train Acc: 97.523% (26339/27008)\n",
            "220 391 Loss: 0.074 | Train Acc: 97.504% (27582/28288)\n",
            "230 391 Loss: 0.075 | Train Acc: 97.447% (28813/29568)\n",
            "240 391 Loss: 0.076 | Train Acc: 97.410% (30049/30848)\n",
            "250 391 Loss: 0.077 | Train Acc: 97.379% (31286/32128)\n",
            "260 391 Loss: 0.078 | Train Acc: 97.306% (32508/33408)\n",
            "270 391 Loss: 0.079 | Train Acc: 97.267% (33740/34688)\n",
            "280 391 Loss: 0.079 | Train Acc: 97.256% (34981/35968)\n",
            "290 391 Loss: 0.079 | Train Acc: 97.256% (36226/37248)\n",
            "300 391 Loss: 0.080 | Train Acc: 97.236% (37463/38528)\n",
            "310 391 Loss: 0.079 | Train Acc: 97.239% (38709/39808)\n",
            "320 391 Loss: 0.080 | Train Acc: 97.223% (39947/41088)\n",
            "330 391 Loss: 0.079 | Train Acc: 97.253% (41204/42368)\n",
            "340 391 Loss: 0.079 | Train Acc: 97.226% (42437/43648)\n",
            "350 391 Loss: 0.079 | Train Acc: 97.231% (43684/44928)\n",
            "360 391 Loss: 0.080 | Train Acc: 97.202% (44915/46208)\n",
            "370 391 Loss: 0.080 | Train Acc: 97.206% (46161/47488)\n",
            "380 391 Loss: 0.080 | Train Acc: 97.201% (47403/48768)\n",
            "390 391 Loss: 0.080 | Train Acc: 97.200% (48600/50000)\n",
            "0 100 Loss: 0.250 | Test Acc: 90.000% (90/100)\n",
            "10 100 Loss: 0.322 | Test Acc: 91.000% (1001/1100)\n",
            "20 100 Loss: 0.313 | Test Acc: 91.333% (1918/2100)\n",
            "30 100 Loss: 0.337 | Test Acc: 91.032% (2822/3100)\n",
            "40 100 Loss: 0.335 | Test Acc: 90.927% (3728/4100)\n",
            "50 100 Loss: 0.334 | Test Acc: 90.980% (4640/5100)\n",
            "60 100 Loss: 0.327 | Test Acc: 90.902% (5545/6100)\n",
            "70 100 Loss: 0.327 | Test Acc: 90.930% (6456/7100)\n",
            "80 100 Loss: 0.322 | Test Acc: 91.037% (7374/8100)\n",
            "90 100 Loss: 0.320 | Test Acc: 91.176% (8297/9100)\n",
            "\n",
            "Epoch: 143\n",
            "0 391 Loss: 0.029 | Train Acc: 99.219% (127/128)\n",
            "10 391 Loss: 0.071 | Train Acc: 97.301% (1370/1408)\n",
            "20 391 Loss: 0.076 | Train Acc: 97.396% (2618/2688)\n",
            "30 391 Loss: 0.081 | Train Acc: 97.127% (3854/3968)\n",
            "40 391 Loss: 0.078 | Train Acc: 97.294% (5106/5248)\n",
            "50 391 Loss: 0.073 | Train Acc: 97.534% (6367/6528)\n",
            "60 391 Loss: 0.071 | Train Acc: 97.631% (7623/7808)\n",
            "70 391 Loss: 0.070 | Train Acc: 97.667% (8876/9088)\n",
            "80 391 Loss: 0.068 | Train Acc: 97.685% (10128/10368)\n",
            "90 391 Loss: 0.069 | Train Acc: 97.673% (11377/11648)\n",
            "100 391 Loss: 0.068 | Train Acc: 97.687% (12629/12928)\n",
            "110 391 Loss: 0.068 | Train Acc: 97.734% (13886/14208)\n",
            "120 391 Loss: 0.066 | Train Acc: 97.805% (15148/15488)\n",
            "130 391 Loss: 0.065 | Train Acc: 97.811% (16401/16768)\n",
            "140 391 Loss: 0.065 | Train Acc: 97.822% (17655/18048)\n",
            "150 391 Loss: 0.066 | Train Acc: 97.796% (18902/19328)\n",
            "160 391 Loss: 0.067 | Train Acc: 97.778% (20150/20608)\n",
            "170 391 Loss: 0.067 | Train Acc: 97.761% (21398/21888)\n",
            "180 391 Loss: 0.068 | Train Acc: 97.721% (22640/23168)\n",
            "190 391 Loss: 0.069 | Train Acc: 97.685% (23882/24448)\n",
            "200 391 Loss: 0.069 | Train Acc: 97.668% (25128/25728)\n",
            "210 391 Loss: 0.069 | Train Acc: 97.664% (26377/27008)\n",
            "220 391 Loss: 0.069 | Train Acc: 97.646% (27622/28288)\n",
            "230 391 Loss: 0.070 | Train Acc: 97.595% (28857/29568)\n",
            "240 391 Loss: 0.070 | Train Acc: 97.578% (30101/30848)\n",
            "250 391 Loss: 0.071 | Train Acc: 97.563% (31345/32128)\n",
            "260 391 Loss: 0.072 | Train Acc: 97.551% (32590/33408)\n",
            "270 391 Loss: 0.072 | Train Acc: 97.521% (33828/34688)\n",
            "280 391 Loss: 0.072 | Train Acc: 97.526% (35078/35968)\n",
            "290 391 Loss: 0.073 | Train Acc: 97.498% (36316/37248)\n",
            "300 391 Loss: 0.073 | Train Acc: 97.498% (37564/38528)\n",
            "310 391 Loss: 0.073 | Train Acc: 97.473% (38802/39808)\n",
            "320 391 Loss: 0.073 | Train Acc: 97.479% (40052/41088)\n",
            "330 391 Loss: 0.073 | Train Acc: 97.477% (41299/42368)\n",
            "340 391 Loss: 0.073 | Train Acc: 97.468% (42543/43648)\n",
            "350 391 Loss: 0.073 | Train Acc: 97.454% (43784/44928)\n",
            "360 391 Loss: 0.074 | Train Acc: 97.440% (45025/46208)\n",
            "370 391 Loss: 0.073 | Train Acc: 97.458% (46281/47488)\n",
            "380 391 Loss: 0.074 | Train Acc: 97.441% (47520/48768)\n",
            "390 391 Loss: 0.074 | Train Acc: 97.446% (48723/50000)\n",
            "0 100 Loss: 0.253 | Test Acc: 92.000% (92/100)\n",
            "10 100 Loss: 0.298 | Test Acc: 91.636% (1008/1100)\n",
            "20 100 Loss: 0.291 | Test Acc: 92.143% (1935/2100)\n",
            "30 100 Loss: 0.296 | Test Acc: 92.161% (2857/3100)\n",
            "40 100 Loss: 0.298 | Test Acc: 92.122% (3777/4100)\n",
            "50 100 Loss: 0.291 | Test Acc: 92.118% (4698/5100)\n",
            "60 100 Loss: 0.278 | Test Acc: 92.361% (5634/6100)\n",
            "70 100 Loss: 0.275 | Test Acc: 92.423% (6562/7100)\n",
            "80 100 Loss: 0.268 | Test Acc: 92.531% (7495/8100)\n",
            "90 100 Loss: 0.265 | Test Acc: 92.659% (8432/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 144\n",
            "0 391 Loss: 0.059 | Train Acc: 97.656% (125/128)\n",
            "10 391 Loss: 0.055 | Train Acc: 98.224% (1383/1408)\n",
            "20 391 Loss: 0.061 | Train Acc: 97.842% (2630/2688)\n",
            "30 391 Loss: 0.059 | Train Acc: 97.984% (3888/3968)\n",
            "40 391 Loss: 0.057 | Train Acc: 98.133% (5150/5248)\n",
            "50 391 Loss: 0.056 | Train Acc: 98.192% (6410/6528)\n",
            "60 391 Loss: 0.059 | Train Acc: 98.040% (7655/7808)\n",
            "70 391 Loss: 0.059 | Train Acc: 98.030% (8909/9088)\n",
            "80 391 Loss: 0.061 | Train Acc: 97.926% (10153/10368)\n",
            "90 391 Loss: 0.061 | Train Acc: 97.914% (11405/11648)\n",
            "100 391 Loss: 0.063 | Train Acc: 97.857% (12651/12928)\n",
            "110 391 Loss: 0.064 | Train Acc: 97.903% (13910/14208)\n",
            "120 391 Loss: 0.065 | Train Acc: 97.843% (15154/15488)\n",
            "130 391 Loss: 0.067 | Train Acc: 97.764% (16393/16768)\n",
            "140 391 Loss: 0.068 | Train Acc: 97.728% (17638/18048)\n",
            "150 391 Loss: 0.070 | Train Acc: 97.656% (18875/19328)\n",
            "160 391 Loss: 0.071 | Train Acc: 97.579% (20109/20608)\n",
            "170 391 Loss: 0.071 | Train Acc: 97.579% (21358/21888)\n",
            "180 391 Loss: 0.070 | Train Acc: 97.592% (22610/23168)\n",
            "190 391 Loss: 0.070 | Train Acc: 97.611% (23864/24448)\n",
            "200 391 Loss: 0.071 | Train Acc: 97.582% (25106/25728)\n",
            "210 391 Loss: 0.070 | Train Acc: 97.604% (26361/27008)\n",
            "220 391 Loss: 0.070 | Train Acc: 97.624% (27616/28288)\n",
            "230 391 Loss: 0.069 | Train Acc: 97.649% (28873/29568)\n",
            "240 391 Loss: 0.069 | Train Acc: 97.640% (30120/30848)\n",
            "250 391 Loss: 0.069 | Train Acc: 97.653% (31374/32128)\n",
            "260 391 Loss: 0.068 | Train Acc: 97.674% (32631/33408)\n",
            "270 391 Loss: 0.069 | Train Acc: 97.665% (33878/34688)\n",
            "280 391 Loss: 0.069 | Train Acc: 97.626% (35114/35968)\n",
            "290 391 Loss: 0.069 | Train Acc: 97.619% (36361/37248)\n",
            "300 391 Loss: 0.070 | Train Acc: 97.607% (37606/38528)\n",
            "310 391 Loss: 0.070 | Train Acc: 97.601% (38853/39808)\n",
            "320 391 Loss: 0.070 | Train Acc: 97.605% (40104/41088)\n",
            "330 391 Loss: 0.070 | Train Acc: 97.604% (41353/42368)\n",
            "340 391 Loss: 0.070 | Train Acc: 97.592% (42597/43648)\n",
            "350 391 Loss: 0.071 | Train Acc: 97.574% (43838/44928)\n",
            "360 391 Loss: 0.071 | Train Acc: 97.572% (45086/46208)\n",
            "370 391 Loss: 0.071 | Train Acc: 97.561% (46330/47488)\n",
            "380 391 Loss: 0.071 | Train Acc: 97.541% (47569/48768)\n",
            "390 391 Loss: 0.071 | Train Acc: 97.532% (48766/50000)\n",
            "0 100 Loss: 0.264 | Test Acc: 90.000% (90/100)\n",
            "10 100 Loss: 0.308 | Test Acc: 90.364% (994/1100)\n",
            "20 100 Loss: 0.340 | Test Acc: 90.286% (1896/2100)\n",
            "30 100 Loss: 0.372 | Test Acc: 89.871% (2786/3100)\n",
            "40 100 Loss: 0.377 | Test Acc: 89.854% (3684/4100)\n",
            "50 100 Loss: 0.367 | Test Acc: 90.020% (4591/5100)\n",
            "60 100 Loss: 0.358 | Test Acc: 90.000% (5490/6100)\n",
            "70 100 Loss: 0.349 | Test Acc: 90.268% (6409/7100)\n",
            "80 100 Loss: 0.350 | Test Acc: 90.309% (7315/8100)\n",
            "90 100 Loss: 0.347 | Test Acc: 90.440% (8230/9100)\n",
            "\n",
            "Epoch: 145\n",
            "0 391 Loss: 0.054 | Train Acc: 98.438% (126/128)\n",
            "10 391 Loss: 0.062 | Train Acc: 97.940% (1379/1408)\n",
            "20 391 Loss: 0.070 | Train Acc: 97.470% (2620/2688)\n",
            "30 391 Loss: 0.064 | Train Acc: 97.656% (3875/3968)\n",
            "40 391 Loss: 0.068 | Train Acc: 97.466% (5115/5248)\n",
            "50 391 Loss: 0.069 | Train Acc: 97.534% (6367/6528)\n",
            "60 391 Loss: 0.067 | Train Acc: 97.605% (7621/7808)\n",
            "70 391 Loss: 0.065 | Train Acc: 97.678% (8877/9088)\n",
            "80 391 Loss: 0.063 | Train Acc: 97.772% (10137/10368)\n",
            "90 391 Loss: 0.064 | Train Acc: 97.742% (11385/11648)\n",
            "100 391 Loss: 0.063 | Train Acc: 97.780% (12641/12928)\n",
            "110 391 Loss: 0.064 | Train Acc: 97.776% (13892/14208)\n",
            "120 391 Loss: 0.063 | Train Acc: 97.805% (15148/15488)\n",
            "130 391 Loss: 0.061 | Train Acc: 97.841% (16406/16768)\n",
            "140 391 Loss: 0.062 | Train Acc: 97.817% (17654/18048)\n",
            "150 391 Loss: 0.061 | Train Acc: 97.843% (18911/19328)\n",
            "160 391 Loss: 0.062 | Train Acc: 97.831% (20161/20608)\n",
            "170 391 Loss: 0.061 | Train Acc: 97.862% (21420/21888)\n",
            "180 391 Loss: 0.061 | Train Acc: 97.898% (22681/23168)\n",
            "190 391 Loss: 0.060 | Train Acc: 97.918% (23939/24448)\n",
            "200 391 Loss: 0.060 | Train Acc: 97.905% (25189/25728)\n",
            "210 391 Loss: 0.061 | Train Acc: 97.897% (26440/27008)\n",
            "220 391 Loss: 0.061 | Train Acc: 97.865% (27684/28288)\n",
            "230 391 Loss: 0.062 | Train Acc: 97.835% (28928/29568)\n",
            "240 391 Loss: 0.063 | Train Acc: 97.809% (30172/30848)\n",
            "250 391 Loss: 0.064 | Train Acc: 97.784% (31416/32128)\n",
            "260 391 Loss: 0.065 | Train Acc: 97.749% (32656/33408)\n",
            "270 391 Loss: 0.065 | Train Acc: 97.740% (33904/34688)\n",
            "280 391 Loss: 0.066 | Train Acc: 97.692% (35138/35968)\n",
            "290 391 Loss: 0.067 | Train Acc: 97.637% (36368/37248)\n",
            "300 391 Loss: 0.068 | Train Acc: 97.625% (37613/38528)\n",
            "310 391 Loss: 0.068 | Train Acc: 97.603% (38854/39808)\n",
            "320 391 Loss: 0.069 | Train Acc: 97.556% (40084/41088)\n",
            "330 391 Loss: 0.070 | Train Acc: 97.515% (41315/42368)\n",
            "340 391 Loss: 0.070 | Train Acc: 97.519% (42565/43648)\n",
            "350 391 Loss: 0.071 | Train Acc: 97.494% (43802/44928)\n",
            "360 391 Loss: 0.072 | Train Acc: 97.477% (45042/46208)\n",
            "370 391 Loss: 0.072 | Train Acc: 97.475% (46289/47488)\n",
            "380 391 Loss: 0.072 | Train Acc: 97.441% (47520/48768)\n",
            "390 391 Loss: 0.073 | Train Acc: 97.424% (48712/50000)\n",
            "0 100 Loss: 0.290 | Test Acc: 90.000% (90/100)\n",
            "10 100 Loss: 0.325 | Test Acc: 90.636% (997/1100)\n",
            "20 100 Loss: 0.323 | Test Acc: 90.762% (1906/2100)\n",
            "30 100 Loss: 0.337 | Test Acc: 90.774% (2814/3100)\n",
            "40 100 Loss: 0.346 | Test Acc: 90.780% (3722/4100)\n",
            "50 100 Loss: 0.340 | Test Acc: 90.667% (4624/5100)\n",
            "60 100 Loss: 0.333 | Test Acc: 90.705% (5533/6100)\n",
            "70 100 Loss: 0.331 | Test Acc: 90.761% (6444/7100)\n",
            "80 100 Loss: 0.321 | Test Acc: 91.062% (7376/8100)\n",
            "90 100 Loss: 0.321 | Test Acc: 91.132% (8293/9100)\n",
            "\n",
            "Epoch: 146\n",
            "0 391 Loss: 0.050 | Train Acc: 98.438% (126/128)\n",
            "10 391 Loss: 0.054 | Train Acc: 98.509% (1387/1408)\n",
            "20 391 Loss: 0.067 | Train Acc: 97.842% (2630/2688)\n",
            "30 391 Loss: 0.071 | Train Acc: 97.656% (3875/3968)\n",
            "40 391 Loss: 0.074 | Train Acc: 97.523% (5118/5248)\n",
            "50 391 Loss: 0.073 | Train Acc: 97.656% (6375/6528)\n",
            "60 391 Loss: 0.072 | Train Acc: 97.618% (7622/7808)\n",
            "70 391 Loss: 0.069 | Train Acc: 97.722% (8881/9088)\n",
            "80 391 Loss: 0.068 | Train Acc: 97.685% (10128/10368)\n",
            "90 391 Loss: 0.069 | Train Acc: 97.656% (11375/11648)\n",
            "100 391 Loss: 0.069 | Train Acc: 97.679% (12628/12928)\n",
            "110 391 Loss: 0.067 | Train Acc: 97.706% (13882/14208)\n",
            "120 391 Loss: 0.066 | Train Acc: 97.785% (15145/15488)\n",
            "130 391 Loss: 0.064 | Train Acc: 97.841% (16406/16768)\n",
            "140 391 Loss: 0.064 | Train Acc: 97.878% (17665/18048)\n",
            "150 391 Loss: 0.062 | Train Acc: 97.905% (18923/19328)\n",
            "160 391 Loss: 0.062 | Train Acc: 97.909% (20177/20608)\n",
            "170 391 Loss: 0.061 | Train Acc: 97.926% (21434/21888)\n",
            "180 391 Loss: 0.061 | Train Acc: 97.950% (22693/23168)\n",
            "190 391 Loss: 0.060 | Train Acc: 97.971% (23952/24448)\n",
            "200 391 Loss: 0.061 | Train Acc: 97.893% (25186/25728)\n",
            "210 391 Loss: 0.063 | Train Acc: 97.830% (26422/27008)\n",
            "220 391 Loss: 0.063 | Train Acc: 97.801% (27666/28288)\n",
            "230 391 Loss: 0.065 | Train Acc: 97.744% (28901/29568)\n",
            "240 391 Loss: 0.065 | Train Acc: 97.744% (30152/30848)\n",
            "250 391 Loss: 0.065 | Train Acc: 97.734% (31400/32128)\n",
            "260 391 Loss: 0.066 | Train Acc: 97.719% (32646/33408)\n",
            "270 391 Loss: 0.066 | Train Acc: 97.725% (33899/34688)\n",
            "280 391 Loss: 0.066 | Train Acc: 97.712% (35145/35968)\n",
            "290 391 Loss: 0.067 | Train Acc: 97.680% (36384/37248)\n",
            "300 391 Loss: 0.068 | Train Acc: 97.643% (37620/38528)\n",
            "310 391 Loss: 0.069 | Train Acc: 97.609% (38856/39808)\n",
            "320 391 Loss: 0.069 | Train Acc: 97.608% (40105/41088)\n",
            "330 391 Loss: 0.068 | Train Acc: 97.614% (41357/42368)\n",
            "340 391 Loss: 0.069 | Train Acc: 97.601% (42601/43648)\n",
            "350 391 Loss: 0.069 | Train Acc: 97.601% (43850/44928)\n",
            "360 391 Loss: 0.069 | Train Acc: 97.598% (45098/46208)\n",
            "370 391 Loss: 0.069 | Train Acc: 97.604% (46350/47488)\n",
            "380 391 Loss: 0.069 | Train Acc: 97.597% (47596/48768)\n",
            "390 391 Loss: 0.069 | Train Acc: 97.572% (48786/50000)\n",
            "0 100 Loss: 0.220 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.266 | Test Acc: 92.636% (1019/1100)\n",
            "20 100 Loss: 0.290 | Test Acc: 91.571% (1923/2100)\n",
            "30 100 Loss: 0.310 | Test Acc: 91.548% (2838/3100)\n",
            "40 100 Loss: 0.297 | Test Acc: 92.098% (3776/4100)\n",
            "50 100 Loss: 0.293 | Test Acc: 92.157% (4700/5100)\n",
            "60 100 Loss: 0.285 | Test Acc: 92.344% (5633/6100)\n",
            "70 100 Loss: 0.279 | Test Acc: 92.408% (6561/7100)\n",
            "80 100 Loss: 0.274 | Test Acc: 92.469% (7490/8100)\n",
            "90 100 Loss: 0.271 | Test Acc: 92.549% (8422/9100)\n",
            "\n",
            "Epoch: 147\n",
            "0 391 Loss: 0.060 | Train Acc: 97.656% (125/128)\n",
            "10 391 Loss: 0.056 | Train Acc: 97.940% (1379/1408)\n",
            "20 391 Loss: 0.064 | Train Acc: 97.842% (2630/2688)\n",
            "30 391 Loss: 0.056 | Train Acc: 98.185% (3896/3968)\n",
            "40 391 Loss: 0.055 | Train Acc: 98.247% (5156/5248)\n",
            "50 391 Loss: 0.054 | Train Acc: 98.269% (6415/6528)\n",
            "60 391 Loss: 0.053 | Train Acc: 98.245% (7671/7808)\n",
            "70 391 Loss: 0.055 | Train Acc: 98.173% (8922/9088)\n",
            "80 391 Loss: 0.055 | Train Acc: 98.167% (10178/10368)\n",
            "90 391 Loss: 0.054 | Train Acc: 98.206% (11439/11648)\n",
            "100 391 Loss: 0.054 | Train Acc: 98.213% (12697/12928)\n",
            "110 391 Loss: 0.054 | Train Acc: 98.205% (13953/14208)\n",
            "120 391 Loss: 0.054 | Train Acc: 98.205% (15210/15488)\n",
            "130 391 Loss: 0.054 | Train Acc: 98.187% (16464/16768)\n",
            "140 391 Loss: 0.055 | Train Acc: 98.127% (17710/18048)\n",
            "150 391 Loss: 0.055 | Train Acc: 98.148% (18970/19328)\n",
            "160 391 Loss: 0.055 | Train Acc: 98.137% (20224/20608)\n",
            "170 391 Loss: 0.056 | Train Acc: 98.109% (21474/21888)\n",
            "180 391 Loss: 0.056 | Train Acc: 98.122% (22733/23168)\n",
            "190 391 Loss: 0.056 | Train Acc: 98.118% (23988/24448)\n",
            "200 391 Loss: 0.057 | Train Acc: 98.092% (25237/25728)\n",
            "210 391 Loss: 0.057 | Train Acc: 98.071% (26487/27008)\n",
            "220 391 Loss: 0.057 | Train Acc: 98.091% (27748/28288)\n",
            "230 391 Loss: 0.058 | Train Acc: 98.069% (28997/29568)\n",
            "240 391 Loss: 0.059 | Train Acc: 98.052% (30247/30848)\n",
            "250 391 Loss: 0.059 | Train Acc: 98.042% (31499/32128)\n",
            "260 391 Loss: 0.059 | Train Acc: 98.060% (32760/33408)\n",
            "270 391 Loss: 0.059 | Train Acc: 98.031% (34005/34688)\n",
            "280 391 Loss: 0.060 | Train Acc: 98.007% (35251/35968)\n",
            "290 391 Loss: 0.060 | Train Acc: 97.957% (36487/37248)\n",
            "300 391 Loss: 0.061 | Train Acc: 97.944% (37736/38528)\n",
            "310 391 Loss: 0.061 | Train Acc: 97.945% (38990/39808)\n",
            "320 391 Loss: 0.061 | Train Acc: 97.953% (40247/41088)\n",
            "330 391 Loss: 0.061 | Train Acc: 97.963% (41505/42368)\n",
            "340 391 Loss: 0.061 | Train Acc: 97.972% (42763/43648)\n",
            "350 391 Loss: 0.061 | Train Acc: 97.963% (44013/44928)\n",
            "360 391 Loss: 0.061 | Train Acc: 97.951% (45261/46208)\n",
            "370 391 Loss: 0.062 | Train Acc: 97.938% (46509/47488)\n",
            "380 391 Loss: 0.062 | Train Acc: 97.933% (47760/48768)\n",
            "390 391 Loss: 0.062 | Train Acc: 97.938% (48969/50000)\n",
            "0 100 Loss: 0.328 | Test Acc: 94.000% (94/100)\n",
            "10 100 Loss: 0.343 | Test Acc: 91.182% (1003/1100)\n",
            "20 100 Loss: 0.344 | Test Acc: 90.905% (1909/2100)\n",
            "30 100 Loss: 0.363 | Test Acc: 90.806% (2815/3100)\n",
            "40 100 Loss: 0.362 | Test Acc: 90.780% (3722/4100)\n",
            "50 100 Loss: 0.353 | Test Acc: 90.961% (4639/5100)\n",
            "60 100 Loss: 0.343 | Test Acc: 91.000% (5551/6100)\n",
            "70 100 Loss: 0.329 | Test Acc: 91.225% (6477/7100)\n",
            "80 100 Loss: 0.322 | Test Acc: 91.321% (7397/8100)\n",
            "90 100 Loss: 0.318 | Test Acc: 91.363% (8314/9100)\n",
            "\n",
            "Epoch: 148\n",
            "0 391 Loss: 0.013 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.050 | Train Acc: 98.366% (1385/1408)\n",
            "20 391 Loss: 0.052 | Train Acc: 98.289% (2642/2688)\n",
            "30 391 Loss: 0.049 | Train Acc: 98.337% (3902/3968)\n",
            "40 391 Loss: 0.048 | Train Acc: 98.342% (5161/5248)\n",
            "50 391 Loss: 0.048 | Train Acc: 98.438% (6426/6528)\n",
            "60 391 Loss: 0.047 | Train Acc: 98.476% (7689/7808)\n",
            "70 391 Loss: 0.049 | Train Acc: 98.382% (8941/9088)\n",
            "80 391 Loss: 0.049 | Train Acc: 98.438% (10206/10368)\n",
            "90 391 Loss: 0.049 | Train Acc: 98.480% (11471/11648)\n",
            "100 391 Loss: 0.050 | Train Acc: 98.383% (12719/12928)\n",
            "110 391 Loss: 0.050 | Train Acc: 98.402% (13981/14208)\n",
            "120 391 Loss: 0.050 | Train Acc: 98.399% (15240/15488)\n",
            "130 391 Loss: 0.051 | Train Acc: 98.336% (16489/16768)\n",
            "140 391 Loss: 0.052 | Train Acc: 98.260% (17734/18048)\n",
            "150 391 Loss: 0.052 | Train Acc: 98.262% (18992/19328)\n",
            "160 391 Loss: 0.053 | Train Acc: 98.258% (20249/20608)\n",
            "170 391 Loss: 0.053 | Train Acc: 98.250% (21505/21888)\n",
            "180 391 Loss: 0.052 | Train Acc: 98.265% (22766/23168)\n",
            "190 391 Loss: 0.052 | Train Acc: 98.278% (24027/24448)\n",
            "200 391 Loss: 0.052 | Train Acc: 98.282% (25286/25728)\n",
            "210 391 Loss: 0.052 | Train Acc: 98.293% (26547/27008)\n",
            "220 391 Loss: 0.052 | Train Acc: 98.289% (27804/28288)\n",
            "230 391 Loss: 0.052 | Train Acc: 98.285% (29061/29568)\n",
            "240 391 Loss: 0.052 | Train Acc: 98.259% (30311/30848)\n",
            "250 391 Loss: 0.052 | Train Acc: 98.245% (31564/32128)\n",
            "260 391 Loss: 0.052 | Train Acc: 98.252% (32824/33408)\n",
            "270 391 Loss: 0.053 | Train Acc: 98.227% (34073/34688)\n",
            "280 391 Loss: 0.053 | Train Acc: 98.226% (35330/35968)\n",
            "290 391 Loss: 0.054 | Train Acc: 98.201% (36578/37248)\n",
            "300 391 Loss: 0.054 | Train Acc: 98.194% (37832/38528)\n",
            "310 391 Loss: 0.054 | Train Acc: 98.174% (39081/39808)\n",
            "320 391 Loss: 0.054 | Train Acc: 98.160% (40332/41088)\n",
            "330 391 Loss: 0.055 | Train Acc: 98.142% (41581/42368)\n",
            "340 391 Loss: 0.055 | Train Acc: 98.144% (42838/43648)\n",
            "350 391 Loss: 0.054 | Train Acc: 98.150% (44097/44928)\n",
            "360 391 Loss: 0.054 | Train Acc: 98.150% (45353/46208)\n",
            "370 391 Loss: 0.055 | Train Acc: 98.145% (46607/47488)\n",
            "380 391 Loss: 0.055 | Train Acc: 98.144% (47863/48768)\n",
            "390 391 Loss: 0.055 | Train Acc: 98.130% (49065/50000)\n",
            "0 100 Loss: 0.350 | Test Acc: 92.000% (92/100)\n",
            "10 100 Loss: 0.277 | Test Acc: 92.364% (1016/1100)\n",
            "20 100 Loss: 0.292 | Test Acc: 92.619% (1945/2100)\n",
            "30 100 Loss: 0.310 | Test Acc: 92.129% (2856/3100)\n",
            "40 100 Loss: 0.312 | Test Acc: 92.146% (3778/4100)\n",
            "50 100 Loss: 0.306 | Test Acc: 92.176% (4701/5100)\n",
            "60 100 Loss: 0.304 | Test Acc: 92.082% (5617/6100)\n",
            "70 100 Loss: 0.303 | Test Acc: 92.141% (6542/7100)\n",
            "80 100 Loss: 0.297 | Test Acc: 92.160% (7465/8100)\n",
            "90 100 Loss: 0.291 | Test Acc: 92.297% (8399/9100)\n",
            "\n",
            "Epoch: 149\n",
            "0 391 Loss: 0.067 | Train Acc: 98.438% (126/128)\n",
            "10 391 Loss: 0.056 | Train Acc: 97.514% (1373/1408)\n",
            "20 391 Loss: 0.054 | Train Acc: 97.917% (2632/2688)\n",
            "30 391 Loss: 0.053 | Train Acc: 98.059% (3891/3968)\n",
            "40 391 Loss: 0.055 | Train Acc: 97.980% (5142/5248)\n",
            "50 391 Loss: 0.054 | Train Acc: 97.963% (6395/6528)\n",
            "60 391 Loss: 0.051 | Train Acc: 98.053% (7656/7808)\n",
            "70 391 Loss: 0.049 | Train Acc: 98.173% (8922/9088)\n",
            "80 391 Loss: 0.050 | Train Acc: 98.206% (10182/10368)\n",
            "90 391 Loss: 0.049 | Train Acc: 98.274% (11447/11648)\n",
            "100 391 Loss: 0.049 | Train Acc: 98.221% (12698/12928)\n",
            "110 391 Loss: 0.049 | Train Acc: 98.247% (13959/14208)\n",
            "120 391 Loss: 0.050 | Train Acc: 98.231% (15214/15488)\n",
            "130 391 Loss: 0.049 | Train Acc: 98.229% (16471/16768)\n",
            "140 391 Loss: 0.050 | Train Acc: 98.194% (17722/18048)\n",
            "150 391 Loss: 0.050 | Train Acc: 98.194% (18979/19328)\n",
            "160 391 Loss: 0.051 | Train Acc: 98.175% (20232/20608)\n",
            "170 391 Loss: 0.052 | Train Acc: 98.131% (21479/21888)\n",
            "180 391 Loss: 0.052 | Train Acc: 98.114% (22731/23168)\n",
            "190 391 Loss: 0.053 | Train Acc: 98.090% (23981/24448)\n",
            "200 391 Loss: 0.053 | Train Acc: 98.099% (25239/25728)\n",
            "210 391 Loss: 0.055 | Train Acc: 98.023% (26474/27008)\n",
            "220 391 Loss: 0.057 | Train Acc: 97.992% (27720/28288)\n",
            "230 391 Loss: 0.057 | Train Acc: 97.988% (28973/29568)\n",
            "240 391 Loss: 0.058 | Train Acc: 97.980% (30225/30848)\n",
            "250 391 Loss: 0.060 | Train Acc: 97.915% (31458/32128)\n",
            "260 391 Loss: 0.060 | Train Acc: 97.890% (32703/33408)\n",
            "270 391 Loss: 0.061 | Train Acc: 97.867% (33948/34688)\n",
            "280 391 Loss: 0.062 | Train Acc: 97.829% (35187/35968)\n",
            "290 391 Loss: 0.062 | Train Acc: 97.831% (36440/37248)\n",
            "300 391 Loss: 0.063 | Train Acc: 97.802% (37681/38528)\n",
            "310 391 Loss: 0.063 | Train Acc: 97.804% (38934/39808)\n",
            "320 391 Loss: 0.063 | Train Acc: 97.797% (40183/41088)\n",
            "330 391 Loss: 0.064 | Train Acc: 97.765% (41421/42368)\n",
            "340 391 Loss: 0.064 | Train Acc: 97.762% (42671/43648)\n",
            "350 391 Loss: 0.064 | Train Acc: 97.734% (43910/44928)\n",
            "360 391 Loss: 0.065 | Train Acc: 97.726% (45157/46208)\n",
            "370 391 Loss: 0.065 | Train Acc: 97.713% (46402/47488)\n",
            "380 391 Loss: 0.065 | Train Acc: 97.722% (47657/48768)\n",
            "390 391 Loss: 0.065 | Train Acc: 97.720% (48860/50000)\n",
            "0 100 Loss: 0.224 | Test Acc: 95.000% (95/100)\n",
            "10 100 Loss: 0.315 | Test Acc: 91.545% (1007/1100)\n",
            "20 100 Loss: 0.326 | Test Acc: 90.952% (1910/2100)\n",
            "30 100 Loss: 0.343 | Test Acc: 90.774% (2814/3100)\n",
            "40 100 Loss: 0.327 | Test Acc: 91.073% (3734/4100)\n",
            "50 100 Loss: 0.317 | Test Acc: 91.431% (4663/5100)\n",
            "60 100 Loss: 0.305 | Test Acc: 91.705% (5594/6100)\n",
            "70 100 Loss: 0.305 | Test Acc: 91.648% (6507/7100)\n",
            "80 100 Loss: 0.298 | Test Acc: 91.802% (7436/8100)\n",
            "90 100 Loss: 0.294 | Test Acc: 91.956% (8368/9100)\n",
            "\n",
            "Epoch: 150\n",
            "0 391 Loss: 0.025 | Train Acc: 99.219% (127/128)\n",
            "10 391 Loss: 0.052 | Train Acc: 98.295% (1384/1408)\n",
            "20 391 Loss: 0.052 | Train Acc: 98.177% (2639/2688)\n",
            "30 391 Loss: 0.051 | Train Acc: 98.211% (3897/3968)\n",
            "40 391 Loss: 0.052 | Train Acc: 98.056% (5146/5248)\n",
            "50 391 Loss: 0.050 | Train Acc: 98.162% (6408/6528)\n",
            "60 391 Loss: 0.048 | Train Acc: 98.258% (7672/7808)\n",
            "70 391 Loss: 0.048 | Train Acc: 98.283% (8932/9088)\n",
            "80 391 Loss: 0.049 | Train Acc: 98.264% (10188/10368)\n",
            "90 391 Loss: 0.049 | Train Acc: 98.266% (11446/11648)\n",
            "100 391 Loss: 0.049 | Train Acc: 98.283% (12706/12928)\n",
            "110 391 Loss: 0.049 | Train Acc: 98.304% (13967/14208)\n",
            "120 391 Loss: 0.049 | Train Acc: 98.295% (15224/15488)\n",
            "130 391 Loss: 0.048 | Train Acc: 98.342% (16490/16768)\n",
            "140 391 Loss: 0.048 | Train Acc: 98.371% (17754/18048)\n",
            "150 391 Loss: 0.046 | Train Acc: 98.438% (19026/19328)\n",
            "160 391 Loss: 0.047 | Train Acc: 98.413% (20281/20608)\n",
            "170 391 Loss: 0.046 | Train Acc: 98.442% (21547/21888)\n",
            "180 391 Loss: 0.047 | Train Acc: 98.399% (22797/23168)\n",
            "190 391 Loss: 0.047 | Train Acc: 98.409% (24059/24448)\n",
            "200 391 Loss: 0.046 | Train Acc: 98.430% (25324/25728)\n",
            "210 391 Loss: 0.047 | Train Acc: 98.415% (26580/27008)\n",
            "220 391 Loss: 0.047 | Train Acc: 98.402% (27836/28288)\n",
            "230 391 Loss: 0.048 | Train Acc: 98.400% (29095/29568)\n",
            "240 391 Loss: 0.047 | Train Acc: 98.421% (30361/30848)\n",
            "250 391 Loss: 0.047 | Train Acc: 98.428% (31623/32128)\n",
            "260 391 Loss: 0.047 | Train Acc: 98.417% (32879/33408)\n",
            "270 391 Loss: 0.048 | Train Acc: 98.400% (34133/34688)\n",
            "280 391 Loss: 0.048 | Train Acc: 98.393% (35390/35968)\n",
            "290 391 Loss: 0.048 | Train Acc: 98.389% (36648/37248)\n",
            "300 391 Loss: 0.048 | Train Acc: 98.396% (37910/38528)\n",
            "310 391 Loss: 0.048 | Train Acc: 98.385% (39165/39808)\n",
            "320 391 Loss: 0.048 | Train Acc: 98.362% (40415/41088)\n",
            "330 391 Loss: 0.048 | Train Acc: 98.374% (41679/42368)\n",
            "340 391 Loss: 0.049 | Train Acc: 98.362% (42933/43648)\n",
            "350 391 Loss: 0.049 | Train Acc: 98.346% (44185/44928)\n",
            "360 391 Loss: 0.049 | Train Acc: 98.353% (45447/46208)\n",
            "370 391 Loss: 0.049 | Train Acc: 98.330% (46695/47488)\n",
            "380 391 Loss: 0.049 | Train Acc: 98.333% (47955/48768)\n",
            "390 391 Loss: 0.049 | Train Acc: 98.324% (49162/50000)\n",
            "0 100 Loss: 0.201 | Test Acc: 92.000% (92/100)\n",
            "10 100 Loss: 0.224 | Test Acc: 92.818% (1021/1100)\n",
            "20 100 Loss: 0.271 | Test Acc: 92.333% (1939/2100)\n",
            "30 100 Loss: 0.289 | Test Acc: 92.194% (2858/3100)\n",
            "40 100 Loss: 0.286 | Test Acc: 92.268% (3783/4100)\n",
            "50 100 Loss: 0.279 | Test Acc: 92.314% (4708/5100)\n",
            "60 100 Loss: 0.266 | Test Acc: 92.623% (5650/6100)\n",
            "70 100 Loss: 0.262 | Test Acc: 92.704% (6582/7100)\n",
            "80 100 Loss: 0.258 | Test Acc: 92.827% (7519/8100)\n",
            "90 100 Loss: 0.258 | Test Acc: 92.890% (8453/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 151\n",
            "0 391 Loss: 0.054 | Train Acc: 99.219% (127/128)\n",
            "10 391 Loss: 0.046 | Train Acc: 98.722% (1390/1408)\n",
            "20 391 Loss: 0.052 | Train Acc: 98.251% (2641/2688)\n",
            "30 391 Loss: 0.047 | Train Acc: 98.438% (3906/3968)\n",
            "40 391 Loss: 0.045 | Train Acc: 98.590% (5174/5248)\n",
            "50 391 Loss: 0.044 | Train Acc: 98.575% (6435/6528)\n",
            "60 391 Loss: 0.046 | Train Acc: 98.502% (7691/7808)\n",
            "70 391 Loss: 0.046 | Train Acc: 98.504% (8952/9088)\n",
            "80 391 Loss: 0.045 | Train Acc: 98.563% (10219/10368)\n",
            "90 391 Loss: 0.045 | Train Acc: 98.566% (11481/11648)\n",
            "100 391 Loss: 0.044 | Train Acc: 98.530% (12738/12928)\n",
            "110 391 Loss: 0.045 | Train Acc: 98.487% (13993/14208)\n",
            "120 391 Loss: 0.046 | Train Acc: 98.476% (15252/15488)\n",
            "130 391 Loss: 0.047 | Train Acc: 98.443% (16507/16768)\n",
            "140 391 Loss: 0.047 | Train Acc: 98.460% (17770/18048)\n",
            "150 391 Loss: 0.047 | Train Acc: 98.448% (19028/19328)\n",
            "160 391 Loss: 0.047 | Train Acc: 98.408% (20280/20608)\n",
            "170 391 Loss: 0.048 | Train Acc: 98.392% (21536/21888)\n",
            "180 391 Loss: 0.047 | Train Acc: 98.394% (22796/23168)\n",
            "190 391 Loss: 0.048 | Train Acc: 98.405% (24058/24448)\n",
            "200 391 Loss: 0.048 | Train Acc: 98.395% (25315/25728)\n",
            "210 391 Loss: 0.048 | Train Acc: 98.393% (26574/27008)\n",
            "220 391 Loss: 0.048 | Train Acc: 98.381% (27830/28288)\n",
            "230 391 Loss: 0.048 | Train Acc: 98.377% (29088/29568)\n",
            "240 391 Loss: 0.049 | Train Acc: 98.356% (30341/30848)\n",
            "250 391 Loss: 0.049 | Train Acc: 98.360% (31601/32128)\n",
            "260 391 Loss: 0.049 | Train Acc: 98.345% (32855/33408)\n",
            "270 391 Loss: 0.050 | Train Acc: 98.342% (34113/34688)\n",
            "280 391 Loss: 0.050 | Train Acc: 98.354% (35376/35968)\n",
            "290 391 Loss: 0.050 | Train Acc: 98.354% (36635/37248)\n",
            "300 391 Loss: 0.050 | Train Acc: 98.349% (37892/38528)\n",
            "310 391 Loss: 0.050 | Train Acc: 98.355% (39153/39808)\n",
            "320 391 Loss: 0.050 | Train Acc: 98.345% (40408/41088)\n",
            "330 391 Loss: 0.050 | Train Acc: 98.360% (41673/42368)\n",
            "340 391 Loss: 0.050 | Train Acc: 98.337% (42922/43648)\n",
            "350 391 Loss: 0.050 | Train Acc: 98.346% (44185/44928)\n",
            "360 391 Loss: 0.050 | Train Acc: 98.344% (45443/46208)\n",
            "370 391 Loss: 0.051 | Train Acc: 98.330% (46695/47488)\n",
            "380 391 Loss: 0.051 | Train Acc: 98.310% (47944/48768)\n",
            "390 391 Loss: 0.051 | Train Acc: 98.288% (49144/50000)\n",
            "0 100 Loss: 0.154 | Test Acc: 98.000% (98/100)\n",
            "10 100 Loss: 0.286 | Test Acc: 92.455% (1017/1100)\n",
            "20 100 Loss: 0.302 | Test Acc: 91.571% (1923/2100)\n",
            "30 100 Loss: 0.303 | Test Acc: 91.581% (2839/3100)\n",
            "40 100 Loss: 0.291 | Test Acc: 91.829% (3765/4100)\n",
            "50 100 Loss: 0.295 | Test Acc: 91.882% (4686/5100)\n",
            "60 100 Loss: 0.288 | Test Acc: 92.082% (5617/6100)\n",
            "70 100 Loss: 0.285 | Test Acc: 92.155% (6543/7100)\n",
            "80 100 Loss: 0.287 | Test Acc: 92.148% (7464/8100)\n",
            "90 100 Loss: 0.290 | Test Acc: 92.143% (8385/9100)\n",
            "\n",
            "Epoch: 152\n",
            "0 391 Loss: 0.047 | Train Acc: 98.438% (126/128)\n",
            "10 391 Loss: 0.059 | Train Acc: 97.727% (1376/1408)\n",
            "20 391 Loss: 0.057 | Train Acc: 97.954% (2633/2688)\n",
            "30 391 Loss: 0.055 | Train Acc: 97.908% (3885/3968)\n",
            "40 391 Loss: 0.055 | Train Acc: 97.942% (5140/5248)\n",
            "50 391 Loss: 0.054 | Train Acc: 98.039% (6400/6528)\n",
            "60 391 Loss: 0.053 | Train Acc: 98.079% (7658/7808)\n",
            "70 391 Loss: 0.052 | Train Acc: 98.074% (8913/9088)\n",
            "80 391 Loss: 0.052 | Train Acc: 98.100% (10171/10368)\n",
            "90 391 Loss: 0.050 | Train Acc: 98.171% (11435/11648)\n",
            "100 391 Loss: 0.051 | Train Acc: 98.151% (12689/12928)\n",
            "110 391 Loss: 0.050 | Train Acc: 98.184% (13950/14208)\n",
            "120 391 Loss: 0.050 | Train Acc: 98.173% (15205/15488)\n",
            "130 391 Loss: 0.051 | Train Acc: 98.145% (16457/16768)\n",
            "140 391 Loss: 0.051 | Train Acc: 98.155% (17715/18048)\n",
            "150 391 Loss: 0.052 | Train Acc: 98.163% (18973/19328)\n",
            "160 391 Loss: 0.052 | Train Acc: 98.146% (20226/20608)\n",
            "170 391 Loss: 0.051 | Train Acc: 98.177% (21489/21888)\n",
            "180 391 Loss: 0.051 | Train Acc: 98.222% (22756/23168)\n",
            "190 391 Loss: 0.050 | Train Acc: 98.249% (24020/24448)\n",
            "200 391 Loss: 0.050 | Train Acc: 98.274% (25284/25728)\n",
            "210 391 Loss: 0.050 | Train Acc: 98.278% (26543/27008)\n",
            "220 391 Loss: 0.050 | Train Acc: 98.285% (27803/28288)\n",
            "230 391 Loss: 0.049 | Train Acc: 98.306% (29067/29568)\n",
            "240 391 Loss: 0.049 | Train Acc: 98.327% (30332/30848)\n",
            "250 391 Loss: 0.049 | Train Acc: 98.335% (31593/32128)\n",
            "260 391 Loss: 0.049 | Train Acc: 98.315% (32845/33408)\n",
            "270 391 Loss: 0.049 | Train Acc: 98.308% (34101/34688)\n",
            "280 391 Loss: 0.049 | Train Acc: 98.301% (35357/35968)\n",
            "290 391 Loss: 0.049 | Train Acc: 98.279% (36607/37248)\n",
            "300 391 Loss: 0.049 | Train Acc: 98.279% (37865/38528)\n",
            "310 391 Loss: 0.049 | Train Acc: 98.277% (39122/39808)\n",
            "320 391 Loss: 0.049 | Train Acc: 98.270% (40377/41088)\n",
            "330 391 Loss: 0.049 | Train Acc: 98.260% (41631/42368)\n",
            "340 391 Loss: 0.050 | Train Acc: 98.254% (42886/43648)\n",
            "350 391 Loss: 0.050 | Train Acc: 98.253% (44143/44928)\n",
            "360 391 Loss: 0.050 | Train Acc: 98.260% (45404/46208)\n",
            "370 391 Loss: 0.050 | Train Acc: 98.248% (46656/47488)\n",
            "380 391 Loss: 0.051 | Train Acc: 98.228% (47904/48768)\n",
            "390 391 Loss: 0.051 | Train Acc: 98.224% (49112/50000)\n",
            "0 100 Loss: 0.239 | Test Acc: 90.000% (90/100)\n",
            "10 100 Loss: 0.337 | Test Acc: 91.273% (1004/1100)\n",
            "20 100 Loss: 0.350 | Test Acc: 91.238% (1916/2100)\n",
            "30 100 Loss: 0.350 | Test Acc: 91.290% (2830/3100)\n",
            "40 100 Loss: 0.341 | Test Acc: 91.390% (3747/4100)\n",
            "50 100 Loss: 0.335 | Test Acc: 91.373% (4660/5100)\n",
            "60 100 Loss: 0.333 | Test Acc: 91.311% (5570/6100)\n",
            "70 100 Loss: 0.328 | Test Acc: 91.451% (6493/7100)\n",
            "80 100 Loss: 0.329 | Test Acc: 91.407% (7404/8100)\n",
            "90 100 Loss: 0.329 | Test Acc: 91.440% (8321/9100)\n",
            "\n",
            "Epoch: 153\n",
            "0 391 Loss: 0.057 | Train Acc: 99.219% (127/128)\n",
            "10 391 Loss: 0.055 | Train Acc: 98.295% (1384/1408)\n",
            "20 391 Loss: 0.049 | Train Acc: 98.326% (2643/2688)\n",
            "30 391 Loss: 0.053 | Train Acc: 98.236% (3898/3968)\n",
            "40 391 Loss: 0.053 | Train Acc: 98.228% (5155/5248)\n",
            "50 391 Loss: 0.053 | Train Acc: 98.208% (6411/6528)\n",
            "60 391 Loss: 0.053 | Train Acc: 98.220% (7669/7808)\n",
            "70 391 Loss: 0.052 | Train Acc: 98.162% (8921/9088)\n",
            "80 391 Loss: 0.051 | Train Acc: 98.216% (10183/10368)\n",
            "90 391 Loss: 0.049 | Train Acc: 98.274% (11447/11648)\n",
            "100 391 Loss: 0.048 | Train Acc: 98.345% (12714/12928)\n",
            "110 391 Loss: 0.046 | Train Acc: 98.381% (13978/14208)\n",
            "120 391 Loss: 0.045 | Train Acc: 98.470% (15251/15488)\n",
            "130 391 Loss: 0.045 | Train Acc: 98.449% (16508/16768)\n",
            "140 391 Loss: 0.045 | Train Acc: 98.454% (17769/18048)\n",
            "150 391 Loss: 0.044 | Train Acc: 98.458% (19030/19328)\n",
            "160 391 Loss: 0.044 | Train Acc: 98.481% (20295/20608)\n",
            "170 391 Loss: 0.044 | Train Acc: 98.474% (21554/21888)\n",
            "180 391 Loss: 0.045 | Train Acc: 98.468% (22813/23168)\n",
            "190 391 Loss: 0.045 | Train Acc: 98.454% (24070/24448)\n",
            "200 391 Loss: 0.045 | Train Acc: 98.472% (25335/25728)\n",
            "210 391 Loss: 0.044 | Train Acc: 98.493% (26601/27008)\n",
            "220 391 Loss: 0.045 | Train Acc: 98.473% (27856/28288)\n",
            "230 391 Loss: 0.045 | Train Acc: 98.475% (29117/29568)\n",
            "240 391 Loss: 0.045 | Train Acc: 98.463% (30374/30848)\n",
            "250 391 Loss: 0.046 | Train Acc: 98.438% (31626/32128)\n",
            "260 391 Loss: 0.046 | Train Acc: 98.432% (32884/33408)\n",
            "270 391 Loss: 0.047 | Train Acc: 98.400% (34133/34688)\n",
            "280 391 Loss: 0.046 | Train Acc: 98.410% (35396/35968)\n",
            "290 391 Loss: 0.046 | Train Acc: 98.408% (36655/37248)\n",
            "300 391 Loss: 0.046 | Train Acc: 98.406% (37914/38528)\n",
            "310 391 Loss: 0.046 | Train Acc: 98.405% (39173/39808)\n",
            "320 391 Loss: 0.047 | Train Acc: 98.389% (40426/41088)\n",
            "330 391 Loss: 0.047 | Train Acc: 98.383% (41683/42368)\n",
            "340 391 Loss: 0.047 | Train Acc: 98.394% (42947/43648)\n",
            "350 391 Loss: 0.047 | Train Acc: 98.382% (44201/44928)\n",
            "360 391 Loss: 0.047 | Train Acc: 98.394% (45466/46208)\n",
            "370 391 Loss: 0.047 | Train Acc: 98.419% (46737/47488)\n",
            "380 391 Loss: 0.047 | Train Acc: 98.407% (47991/48768)\n",
            "390 391 Loss: 0.047 | Train Acc: 98.410% (49205/50000)\n",
            "0 100 Loss: 0.320 | Test Acc: 93.000% (93/100)\n",
            "10 100 Loss: 0.242 | Test Acc: 92.545% (1018/1100)\n",
            "20 100 Loss: 0.293 | Test Acc: 91.905% (1930/2100)\n",
            "30 100 Loss: 0.306 | Test Acc: 91.935% (2850/3100)\n",
            "40 100 Loss: 0.307 | Test Acc: 91.829% (3765/4100)\n",
            "50 100 Loss: 0.309 | Test Acc: 91.804% (4682/5100)\n",
            "60 100 Loss: 0.301 | Test Acc: 92.115% (5619/6100)\n",
            "70 100 Loss: 0.295 | Test Acc: 92.268% (6551/7100)\n",
            "80 100 Loss: 0.291 | Test Acc: 92.370% (7482/8100)\n",
            "90 100 Loss: 0.290 | Test Acc: 92.407% (8409/9100)\n",
            "\n",
            "Epoch: 154\n",
            "0 391 Loss: 0.070 | Train Acc: 98.438% (126/128)\n",
            "10 391 Loss: 0.038 | Train Acc: 98.651% (1389/1408)\n",
            "20 391 Loss: 0.040 | Train Acc: 98.549% (2649/2688)\n",
            "30 391 Loss: 0.040 | Train Acc: 98.589% (3912/3968)\n",
            "40 391 Loss: 0.040 | Train Acc: 98.609% (5175/5248)\n",
            "50 391 Loss: 0.038 | Train Acc: 98.698% (6443/6528)\n",
            "60 391 Loss: 0.037 | Train Acc: 98.758% (7711/7808)\n",
            "70 391 Loss: 0.037 | Train Acc: 98.768% (8976/9088)\n",
            "80 391 Loss: 0.037 | Train Acc: 98.775% (10241/10368)\n",
            "90 391 Loss: 0.036 | Train Acc: 98.798% (11508/11648)\n",
            "100 391 Loss: 0.037 | Train Acc: 98.739% (12765/12928)\n",
            "110 391 Loss: 0.039 | Train Acc: 98.705% (14024/14208)\n",
            "120 391 Loss: 0.038 | Train Acc: 98.715% (15289/15488)\n",
            "130 391 Loss: 0.038 | Train Acc: 98.712% (16552/16768)\n",
            "140 391 Loss: 0.038 | Train Acc: 98.709% (17815/18048)\n",
            "150 391 Loss: 0.038 | Train Acc: 98.727% (19082/19328)\n",
            "160 391 Loss: 0.038 | Train Acc: 98.738% (20348/20608)\n",
            "170 391 Loss: 0.038 | Train Acc: 98.748% (21614/21888)\n",
            "180 391 Loss: 0.038 | Train Acc: 98.740% (22876/23168)\n",
            "190 391 Loss: 0.038 | Train Acc: 98.720% (24135/24448)\n",
            "200 391 Loss: 0.038 | Train Acc: 98.725% (25400/25728)\n",
            "210 391 Loss: 0.038 | Train Acc: 98.715% (26661/27008)\n",
            "220 391 Loss: 0.038 | Train Acc: 98.678% (27914/28288)\n",
            "230 391 Loss: 0.039 | Train Acc: 98.674% (29176/29568)\n",
            "240 391 Loss: 0.039 | Train Acc: 98.668% (30437/30848)\n",
            "250 391 Loss: 0.039 | Train Acc: 98.668% (31700/32128)\n",
            "260 391 Loss: 0.039 | Train Acc: 98.674% (32965/33408)\n",
            "270 391 Loss: 0.039 | Train Acc: 98.648% (34219/34688)\n",
            "280 391 Loss: 0.040 | Train Acc: 98.629% (35475/35968)\n",
            "290 391 Loss: 0.039 | Train Acc: 98.650% (36745/37248)\n",
            "300 391 Loss: 0.040 | Train Acc: 98.632% (38001/38528)\n",
            "310 391 Loss: 0.040 | Train Acc: 98.638% (39266/39808)\n",
            "320 391 Loss: 0.040 | Train Acc: 98.637% (40528/41088)\n",
            "330 391 Loss: 0.040 | Train Acc: 98.619% (41783/42368)\n",
            "340 391 Loss: 0.041 | Train Acc: 98.607% (43040/43648)\n",
            "350 391 Loss: 0.041 | Train Acc: 98.591% (44295/44928)\n",
            "360 391 Loss: 0.041 | Train Acc: 98.589% (45556/46208)\n",
            "370 391 Loss: 0.042 | Train Acc: 98.574% (46811/47488)\n",
            "380 391 Loss: 0.042 | Train Acc: 98.556% (48064/48768)\n",
            "390 391 Loss: 0.042 | Train Acc: 98.550% (49275/50000)\n",
            "0 100 Loss: 0.346 | Test Acc: 94.000% (94/100)\n",
            "10 100 Loss: 0.294 | Test Acc: 92.545% (1018/1100)\n",
            "20 100 Loss: 0.289 | Test Acc: 92.571% (1944/2100)\n",
            "30 100 Loss: 0.301 | Test Acc: 92.548% (2869/3100)\n",
            "40 100 Loss: 0.300 | Test Acc: 92.634% (3798/4100)\n",
            "50 100 Loss: 0.302 | Test Acc: 92.412% (4713/5100)\n",
            "60 100 Loss: 0.295 | Test Acc: 92.443% (5639/6100)\n",
            "70 100 Loss: 0.292 | Test Acc: 92.521% (6569/7100)\n",
            "80 100 Loss: 0.290 | Test Acc: 92.481% (7491/8100)\n",
            "90 100 Loss: 0.290 | Test Acc: 92.473% (8415/9100)\n",
            "\n",
            "Epoch: 155\n",
            "0 391 Loss: 0.021 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.036 | Train Acc: 99.006% (1394/1408)\n",
            "20 391 Loss: 0.036 | Train Acc: 98.884% (2658/2688)\n",
            "30 391 Loss: 0.037 | Train Acc: 98.790% (3920/3968)\n",
            "40 391 Loss: 0.036 | Train Acc: 98.761% (5183/5248)\n",
            "50 391 Loss: 0.035 | Train Acc: 98.866% (6454/6528)\n",
            "60 391 Loss: 0.035 | Train Acc: 98.860% (7719/7808)\n",
            "70 391 Loss: 0.035 | Train Acc: 98.845% (8983/9088)\n",
            "80 391 Loss: 0.035 | Train Acc: 98.843% (10248/10368)\n",
            "90 391 Loss: 0.034 | Train Acc: 98.884% (11518/11648)\n",
            "100 391 Loss: 0.034 | Train Acc: 98.878% (12783/12928)\n",
            "110 391 Loss: 0.034 | Train Acc: 98.867% (14047/14208)\n",
            "120 391 Loss: 0.034 | Train Acc: 98.870% (15313/15488)\n",
            "130 391 Loss: 0.034 | Train Acc: 98.861% (16577/16768)\n",
            "140 391 Loss: 0.034 | Train Acc: 98.848% (17840/18048)\n",
            "150 391 Loss: 0.034 | Train Acc: 98.841% (19104/19328)\n",
            "160 391 Loss: 0.035 | Train Acc: 98.850% (20371/20608)\n",
            "170 391 Loss: 0.035 | Train Acc: 98.862% (21639/21888)\n",
            "180 391 Loss: 0.035 | Train Acc: 98.848% (22901/23168)\n",
            "190 391 Loss: 0.035 | Train Acc: 98.847% (24166/24448)\n",
            "200 391 Loss: 0.035 | Train Acc: 98.850% (25432/25728)\n",
            "210 391 Loss: 0.035 | Train Acc: 98.863% (26701/27008)\n",
            "220 391 Loss: 0.035 | Train Acc: 98.844% (27961/28288)\n",
            "230 391 Loss: 0.036 | Train Acc: 98.840% (29225/29568)\n",
            "240 391 Loss: 0.036 | Train Acc: 98.817% (30483/30848)\n",
            "250 391 Loss: 0.037 | Train Acc: 98.792% (31740/32128)\n",
            "260 391 Loss: 0.037 | Train Acc: 98.764% (32995/33408)\n",
            "270 391 Loss: 0.037 | Train Acc: 98.763% (34259/34688)\n",
            "280 391 Loss: 0.037 | Train Acc: 98.763% (35523/35968)\n",
            "290 391 Loss: 0.038 | Train Acc: 98.752% (36783/37248)\n",
            "300 391 Loss: 0.038 | Train Acc: 98.733% (38040/38528)\n",
            "310 391 Loss: 0.038 | Train Acc: 98.719% (39298/39808)\n",
            "320 391 Loss: 0.038 | Train Acc: 98.713% (40559/41088)\n",
            "330 391 Loss: 0.038 | Train Acc: 98.704% (41819/42368)\n",
            "340 391 Loss: 0.039 | Train Acc: 98.687% (43075/43648)\n",
            "350 391 Loss: 0.039 | Train Acc: 98.671% (44331/44928)\n",
            "360 391 Loss: 0.039 | Train Acc: 98.669% (45593/46208)\n",
            "370 391 Loss: 0.039 | Train Acc: 98.667% (46855/47488)\n",
            "380 391 Loss: 0.039 | Train Acc: 98.667% (48118/48768)\n",
            "390 391 Loss: 0.040 | Train Acc: 98.652% (49326/50000)\n",
            "0 100 Loss: 0.313 | Test Acc: 92.000% (92/100)\n",
            "10 100 Loss: 0.248 | Test Acc: 93.545% (1029/1100)\n",
            "20 100 Loss: 0.251 | Test Acc: 93.238% (1958/2100)\n",
            "30 100 Loss: 0.266 | Test Acc: 92.903% (2880/3100)\n",
            "40 100 Loss: 0.263 | Test Acc: 92.976% (3812/4100)\n",
            "50 100 Loss: 0.266 | Test Acc: 93.196% (4753/5100)\n",
            "60 100 Loss: 0.261 | Test Acc: 93.197% (5685/6100)\n",
            "70 100 Loss: 0.252 | Test Acc: 93.169% (6615/7100)\n",
            "80 100 Loss: 0.247 | Test Acc: 93.247% (7553/8100)\n",
            "90 100 Loss: 0.245 | Test Acc: 93.297% (8490/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 156\n",
            "0 391 Loss: 0.026 | Train Acc: 99.219% (127/128)\n",
            "10 391 Loss: 0.027 | Train Acc: 99.219% (1397/1408)\n",
            "20 391 Loss: 0.031 | Train Acc: 98.921% (2659/2688)\n",
            "30 391 Loss: 0.036 | Train Acc: 98.740% (3918/3968)\n",
            "40 391 Loss: 0.035 | Train Acc: 98.800% (5185/5248)\n",
            "50 391 Loss: 0.036 | Train Acc: 98.790% (6449/6528)\n",
            "60 391 Loss: 0.035 | Train Acc: 98.835% (7717/7808)\n",
            "70 391 Loss: 0.034 | Train Acc: 98.812% (8980/9088)\n",
            "80 391 Loss: 0.033 | Train Acc: 98.891% (10253/10368)\n",
            "90 391 Loss: 0.032 | Train Acc: 98.910% (11521/11648)\n",
            "100 391 Loss: 0.033 | Train Acc: 98.871% (12782/12928)\n",
            "110 391 Loss: 0.033 | Train Acc: 98.853% (14045/14208)\n",
            "120 391 Loss: 0.033 | Train Acc: 98.864% (15312/15488)\n",
            "130 391 Loss: 0.033 | Train Acc: 98.843% (16574/16768)\n",
            "140 391 Loss: 0.033 | Train Acc: 98.881% (17846/18048)\n",
            "150 391 Loss: 0.033 | Train Acc: 98.877% (19111/19328)\n",
            "160 391 Loss: 0.033 | Train Acc: 98.898% (20381/20608)\n",
            "170 391 Loss: 0.033 | Train Acc: 98.881% (21643/21888)\n",
            "180 391 Loss: 0.033 | Train Acc: 98.899% (22913/23168)\n",
            "190 391 Loss: 0.033 | Train Acc: 98.900% (24179/24448)\n",
            "200 391 Loss: 0.033 | Train Acc: 98.912% (25448/25728)\n",
            "210 391 Loss: 0.032 | Train Acc: 98.930% (26719/27008)\n",
            "220 391 Loss: 0.032 | Train Acc: 98.947% (27990/28288)\n",
            "230 391 Loss: 0.032 | Train Acc: 98.938% (29254/29568)\n",
            "240 391 Loss: 0.033 | Train Acc: 98.904% (30510/30848)\n",
            "250 391 Loss: 0.033 | Train Acc: 98.892% (31772/32128)\n",
            "260 391 Loss: 0.034 | Train Acc: 98.886% (33036/33408)\n",
            "270 391 Loss: 0.034 | Train Acc: 98.870% (34296/34688)\n",
            "280 391 Loss: 0.034 | Train Acc: 98.860% (35558/35968)\n",
            "290 391 Loss: 0.034 | Train Acc: 98.856% (36822/37248)\n",
            "300 391 Loss: 0.035 | Train Acc: 98.845% (38083/38528)\n",
            "310 391 Loss: 0.035 | Train Acc: 98.842% (39347/39808)\n",
            "320 391 Loss: 0.035 | Train Acc: 98.834% (40609/41088)\n",
            "330 391 Loss: 0.035 | Train Acc: 98.843% (41878/42368)\n",
            "340 391 Loss: 0.035 | Train Acc: 98.827% (43136/43648)\n",
            "350 391 Loss: 0.035 | Train Acc: 98.825% (44400/44928)\n",
            "360 391 Loss: 0.035 | Train Acc: 98.825% (45665/46208)\n",
            "370 391 Loss: 0.036 | Train Acc: 98.806% (46921/47488)\n",
            "380 391 Loss: 0.036 | Train Acc: 98.802% (48184/48768)\n",
            "390 391 Loss: 0.036 | Train Acc: 98.804% (49402/50000)\n",
            "0 100 Loss: 0.230 | Test Acc: 95.000% (95/100)\n",
            "10 100 Loss: 0.250 | Test Acc: 93.909% (1033/1100)\n",
            "20 100 Loss: 0.276 | Test Acc: 92.762% (1948/2100)\n",
            "30 100 Loss: 0.291 | Test Acc: 92.677% (2873/3100)\n",
            "40 100 Loss: 0.281 | Test Acc: 92.902% (3809/4100)\n",
            "50 100 Loss: 0.279 | Test Acc: 93.039% (4745/5100)\n",
            "60 100 Loss: 0.274 | Test Acc: 93.098% (5679/6100)\n",
            "70 100 Loss: 0.266 | Test Acc: 93.141% (6613/7100)\n",
            "80 100 Loss: 0.269 | Test Acc: 93.012% (7534/8100)\n",
            "90 100 Loss: 0.265 | Test Acc: 93.132% (8475/9100)\n",
            "\n",
            "Epoch: 157\n",
            "0 391 Loss: 0.025 | Train Acc: 98.438% (126/128)\n",
            "10 391 Loss: 0.030 | Train Acc: 99.006% (1394/1408)\n",
            "20 391 Loss: 0.034 | Train Acc: 98.996% (2661/2688)\n",
            "30 391 Loss: 0.032 | Train Acc: 98.942% (3926/3968)\n",
            "40 391 Loss: 0.031 | Train Acc: 98.914% (5191/5248)\n",
            "50 391 Loss: 0.031 | Train Acc: 98.943% (6459/6528)\n",
            "60 391 Loss: 0.032 | Train Acc: 98.937% (7725/7808)\n",
            "70 391 Loss: 0.032 | Train Acc: 98.922% (8990/9088)\n",
            "80 391 Loss: 0.032 | Train Acc: 98.958% (10260/10368)\n",
            "90 391 Loss: 0.032 | Train Acc: 98.935% (11524/11648)\n",
            "100 391 Loss: 0.032 | Train Acc: 98.940% (12791/12928)\n",
            "110 391 Loss: 0.033 | Train Acc: 98.902% (14052/14208)\n",
            "120 391 Loss: 0.033 | Train Acc: 98.902% (15318/15488)\n",
            "130 391 Loss: 0.033 | Train Acc: 98.873% (16579/16768)\n",
            "140 391 Loss: 0.033 | Train Acc: 98.903% (17850/18048)\n",
            "150 391 Loss: 0.033 | Train Acc: 98.913% (19118/19328)\n",
            "160 391 Loss: 0.034 | Train Acc: 98.874% (20376/20608)\n",
            "170 391 Loss: 0.035 | Train Acc: 98.835% (21633/21888)\n",
            "180 391 Loss: 0.035 | Train Acc: 98.839% (22899/23168)\n",
            "190 391 Loss: 0.034 | Train Acc: 98.859% (24169/24448)\n",
            "200 391 Loss: 0.035 | Train Acc: 98.842% (25430/25728)\n",
            "210 391 Loss: 0.035 | Train Acc: 98.834% (26693/27008)\n",
            "220 391 Loss: 0.035 | Train Acc: 98.823% (27955/28288)\n",
            "230 391 Loss: 0.035 | Train Acc: 98.810% (29216/29568)\n",
            "240 391 Loss: 0.036 | Train Acc: 98.771% (30469/30848)\n",
            "250 391 Loss: 0.036 | Train Acc: 98.774% (31734/32128)\n",
            "260 391 Loss: 0.036 | Train Acc: 98.764% (32995/33408)\n",
            "270 391 Loss: 0.036 | Train Acc: 98.769% (34261/34688)\n",
            "280 391 Loss: 0.036 | Train Acc: 98.757% (35521/35968)\n",
            "290 391 Loss: 0.036 | Train Acc: 98.762% (36787/37248)\n",
            "300 391 Loss: 0.037 | Train Acc: 98.752% (38047/38528)\n",
            "310 391 Loss: 0.037 | Train Acc: 98.741% (39307/39808)\n",
            "320 391 Loss: 0.037 | Train Acc: 98.730% (40566/41088)\n",
            "330 391 Loss: 0.038 | Train Acc: 98.716% (41824/42368)\n",
            "340 391 Loss: 0.038 | Train Acc: 98.699% (43080/43648)\n",
            "350 391 Loss: 0.038 | Train Acc: 98.680% (44335/44928)\n",
            "360 391 Loss: 0.038 | Train Acc: 98.691% (45603/46208)\n",
            "370 391 Loss: 0.038 | Train Acc: 98.694% (46868/47488)\n",
            "380 391 Loss: 0.038 | Train Acc: 98.694% (48131/48768)\n",
            "390 391 Loss: 0.038 | Train Acc: 98.694% (49347/50000)\n",
            "0 100 Loss: 0.310 | Test Acc: 94.000% (94/100)\n",
            "10 100 Loss: 0.288 | Test Acc: 92.909% (1022/1100)\n",
            "20 100 Loss: 0.287 | Test Acc: 92.524% (1943/2100)\n",
            "30 100 Loss: 0.291 | Test Acc: 92.581% (2870/3100)\n",
            "40 100 Loss: 0.290 | Test Acc: 92.659% (3799/4100)\n",
            "50 100 Loss: 0.282 | Test Acc: 92.765% (4731/5100)\n",
            "60 100 Loss: 0.275 | Test Acc: 92.902% (5667/6100)\n",
            "70 100 Loss: 0.271 | Test Acc: 92.887% (6595/7100)\n",
            "80 100 Loss: 0.264 | Test Acc: 92.951% (7529/8100)\n",
            "90 100 Loss: 0.262 | Test Acc: 93.055% (8468/9100)\n",
            "\n",
            "Epoch: 158\n",
            "0 391 Loss: 0.014 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.027 | Train Acc: 99.148% (1396/1408)\n",
            "20 391 Loss: 0.028 | Train Acc: 99.219% (2667/2688)\n",
            "30 391 Loss: 0.029 | Train Acc: 99.017% (3929/3968)\n",
            "40 391 Loss: 0.028 | Train Acc: 99.085% (5200/5248)\n",
            "50 391 Loss: 0.027 | Train Acc: 99.127% (6471/6528)\n",
            "60 391 Loss: 0.027 | Train Acc: 99.091% (7737/7808)\n",
            "70 391 Loss: 0.027 | Train Acc: 99.087% (9005/9088)\n",
            "80 391 Loss: 0.028 | Train Acc: 99.064% (10271/10368)\n",
            "90 391 Loss: 0.027 | Train Acc: 99.090% (11542/11648)\n",
            "100 391 Loss: 0.027 | Train Acc: 99.080% (12809/12928)\n",
            "110 391 Loss: 0.028 | Train Acc: 99.071% (14076/14208)\n",
            "120 391 Loss: 0.029 | Train Acc: 99.038% (15339/15488)\n",
            "130 391 Loss: 0.028 | Train Acc: 99.052% (16609/16768)\n",
            "140 391 Loss: 0.028 | Train Acc: 99.041% (17875/18048)\n",
            "150 391 Loss: 0.028 | Train Acc: 99.058% (19146/19328)\n",
            "160 391 Loss: 0.028 | Train Acc: 99.059% (20414/20608)\n",
            "170 391 Loss: 0.029 | Train Acc: 99.045% (21679/21888)\n",
            "180 391 Loss: 0.029 | Train Acc: 99.012% (22939/23168)\n",
            "190 391 Loss: 0.029 | Train Acc: 98.998% (24203/24448)\n",
            "200 391 Loss: 0.030 | Train Acc: 98.986% (25467/25728)\n",
            "210 391 Loss: 0.029 | Train Acc: 99.008% (26740/27008)\n",
            "220 391 Loss: 0.030 | Train Acc: 98.978% (27999/28288)\n",
            "230 391 Loss: 0.030 | Train Acc: 98.975% (29265/29568)\n",
            "240 391 Loss: 0.030 | Train Acc: 98.985% (30535/30848)\n",
            "250 391 Loss: 0.030 | Train Acc: 98.979% (31800/32128)\n",
            "260 391 Loss: 0.030 | Train Acc: 98.997% (33073/33408)\n",
            "270 391 Loss: 0.030 | Train Acc: 98.985% (34336/34688)\n",
            "280 391 Loss: 0.030 | Train Acc: 98.999% (35608/35968)\n",
            "290 391 Loss: 0.030 | Train Acc: 98.993% (36873/37248)\n",
            "300 391 Loss: 0.030 | Train Acc: 98.998% (38142/38528)\n",
            "310 391 Loss: 0.030 | Train Acc: 98.998% (39409/39808)\n",
            "320 391 Loss: 0.030 | Train Acc: 99.002% (40678/41088)\n",
            "330 391 Loss: 0.030 | Train Acc: 98.997% (41943/42368)\n",
            "340 391 Loss: 0.030 | Train Acc: 98.985% (43205/43648)\n",
            "350 391 Loss: 0.030 | Train Acc: 98.996% (44477/44928)\n",
            "360 391 Loss: 0.030 | Train Acc: 99.005% (45748/46208)\n",
            "370 391 Loss: 0.030 | Train Acc: 99.006% (47016/47488)\n",
            "380 391 Loss: 0.030 | Train Acc: 99.005% (48283/48768)\n",
            "390 391 Loss: 0.030 | Train Acc: 99.006% (49503/50000)\n",
            "0 100 Loss: 0.164 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.237 | Test Acc: 94.091% (1035/1100)\n",
            "20 100 Loss: 0.282 | Test Acc: 93.000% (1953/2100)\n",
            "30 100 Loss: 0.279 | Test Acc: 93.290% (2892/3100)\n",
            "40 100 Loss: 0.281 | Test Acc: 93.268% (3824/4100)\n",
            "50 100 Loss: 0.281 | Test Acc: 93.294% (4758/5100)\n",
            "60 100 Loss: 0.273 | Test Acc: 93.344% (5694/6100)\n",
            "70 100 Loss: 0.268 | Test Acc: 93.521% (6640/7100)\n",
            "80 100 Loss: 0.265 | Test Acc: 93.605% (7582/8100)\n",
            "90 100 Loss: 0.265 | Test Acc: 93.549% (8513/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 159\n",
            "0 391 Loss: 0.076 | Train Acc: 96.094% (123/128)\n",
            "10 391 Loss: 0.030 | Train Acc: 99.006% (1394/1408)\n",
            "20 391 Loss: 0.026 | Train Acc: 99.144% (2665/2688)\n",
            "30 391 Loss: 0.027 | Train Acc: 99.118% (3933/3968)\n",
            "40 391 Loss: 0.030 | Train Acc: 99.066% (5199/5248)\n",
            "50 391 Loss: 0.030 | Train Acc: 98.974% (6461/6528)\n",
            "60 391 Loss: 0.031 | Train Acc: 98.950% (7726/7808)\n",
            "70 391 Loss: 0.030 | Train Acc: 98.944% (8992/9088)\n",
            "80 391 Loss: 0.030 | Train Acc: 98.900% (10254/10368)\n",
            "90 391 Loss: 0.030 | Train Acc: 98.927% (11523/11648)\n",
            "100 391 Loss: 0.030 | Train Acc: 98.933% (12790/12928)\n",
            "110 391 Loss: 0.030 | Train Acc: 98.923% (14055/14208)\n",
            "120 391 Loss: 0.030 | Train Acc: 98.928% (15322/15488)\n",
            "130 391 Loss: 0.029 | Train Acc: 98.950% (16592/16768)\n",
            "140 391 Loss: 0.029 | Train Acc: 98.980% (17864/18048)\n",
            "150 391 Loss: 0.029 | Train Acc: 98.991% (19133/19328)\n",
            "160 391 Loss: 0.028 | Train Acc: 99.005% (20403/20608)\n",
            "170 391 Loss: 0.028 | Train Acc: 99.009% (21671/21888)\n",
            "180 391 Loss: 0.030 | Train Acc: 98.964% (22928/23168)\n",
            "190 391 Loss: 0.030 | Train Acc: 98.961% (24194/24448)\n",
            "200 391 Loss: 0.030 | Train Acc: 98.966% (25462/25728)\n",
            "210 391 Loss: 0.030 | Train Acc: 98.937% (26721/27008)\n",
            "220 391 Loss: 0.030 | Train Acc: 98.939% (27988/28288)\n",
            "230 391 Loss: 0.030 | Train Acc: 98.945% (29256/29568)\n",
            "240 391 Loss: 0.030 | Train Acc: 98.940% (30521/30848)\n",
            "250 391 Loss: 0.030 | Train Acc: 98.948% (31790/32128)\n",
            "260 391 Loss: 0.030 | Train Acc: 98.931% (33051/33408)\n",
            "270 391 Loss: 0.031 | Train Acc: 98.925% (34315/34688)\n",
            "280 391 Loss: 0.031 | Train Acc: 98.910% (35576/35968)\n",
            "290 391 Loss: 0.031 | Train Acc: 98.913% (36843/37248)\n",
            "300 391 Loss: 0.031 | Train Acc: 98.907% (38107/38528)\n",
            "310 391 Loss: 0.031 | Train Acc: 98.910% (39374/39808)\n",
            "320 391 Loss: 0.031 | Train Acc: 98.912% (40641/41088)\n",
            "330 391 Loss: 0.031 | Train Acc: 98.926% (41913/42368)\n",
            "340 391 Loss: 0.031 | Train Acc: 98.914% (43174/43648)\n",
            "350 391 Loss: 0.031 | Train Acc: 98.909% (44438/44928)\n",
            "360 391 Loss: 0.031 | Train Acc: 98.914% (45706/46208)\n",
            "370 391 Loss: 0.032 | Train Acc: 98.901% (46966/47488)\n",
            "380 391 Loss: 0.032 | Train Acc: 98.880% (48222/48768)\n",
            "390 391 Loss: 0.033 | Train Acc: 98.874% (49437/50000)\n",
            "0 100 Loss: 0.225 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.306 | Test Acc: 92.000% (1012/1100)\n",
            "20 100 Loss: 0.343 | Test Acc: 91.000% (1911/2100)\n",
            "30 100 Loss: 0.338 | Test Acc: 91.097% (2824/3100)\n",
            "40 100 Loss: 0.348 | Test Acc: 91.024% (3732/4100)\n",
            "50 100 Loss: 0.344 | Test Acc: 91.275% (4655/5100)\n",
            "60 100 Loss: 0.346 | Test Acc: 91.246% (5566/6100)\n",
            "70 100 Loss: 0.337 | Test Acc: 91.338% (6485/7100)\n",
            "80 100 Loss: 0.337 | Test Acc: 91.296% (7395/8100)\n",
            "90 100 Loss: 0.332 | Test Acc: 91.462% (8323/9100)\n",
            "\n",
            "Epoch: 160\n",
            "0 391 Loss: 0.014 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.026 | Train Acc: 98.935% (1393/1408)\n",
            "20 391 Loss: 0.028 | Train Acc: 98.921% (2659/2688)\n",
            "30 391 Loss: 0.028 | Train Acc: 98.866% (3923/3968)\n",
            "40 391 Loss: 0.029 | Train Acc: 98.914% (5191/5248)\n",
            "50 391 Loss: 0.029 | Train Acc: 98.943% (6459/6528)\n",
            "60 391 Loss: 0.029 | Train Acc: 98.911% (7723/7808)\n",
            "70 391 Loss: 0.029 | Train Acc: 98.922% (8990/9088)\n",
            "80 391 Loss: 0.029 | Train Acc: 98.968% (10261/10368)\n",
            "90 391 Loss: 0.028 | Train Acc: 98.996% (11531/11648)\n",
            "100 391 Loss: 0.028 | Train Acc: 99.010% (12800/12928)\n",
            "110 391 Loss: 0.027 | Train Acc: 99.036% (14071/14208)\n",
            "120 391 Loss: 0.028 | Train Acc: 99.032% (15338/15488)\n",
            "130 391 Loss: 0.027 | Train Acc: 99.064% (16611/16768)\n",
            "140 391 Loss: 0.027 | Train Acc: 99.075% (17881/18048)\n",
            "150 391 Loss: 0.027 | Train Acc: 99.058% (19146/19328)\n",
            "160 391 Loss: 0.028 | Train Acc: 99.049% (20412/20608)\n",
            "170 391 Loss: 0.028 | Train Acc: 99.073% (21685/21888)\n",
            "180 391 Loss: 0.028 | Train Acc: 99.059% (22950/23168)\n",
            "190 391 Loss: 0.028 | Train Acc: 99.067% (24220/24448)\n",
            "200 391 Loss: 0.028 | Train Acc: 99.079% (25491/25728)\n",
            "210 391 Loss: 0.028 | Train Acc: 99.067% (26756/27008)\n",
            "220 391 Loss: 0.028 | Train Acc: 99.046% (28018/28288)\n",
            "230 391 Loss: 0.028 | Train Acc: 99.050% (29287/29568)\n",
            "240 391 Loss: 0.028 | Train Acc: 99.063% (30559/30848)\n",
            "250 391 Loss: 0.028 | Train Acc: 99.066% (31828/32128)\n",
            "260 391 Loss: 0.027 | Train Acc: 99.078% (33100/33408)\n",
            "270 391 Loss: 0.027 | Train Acc: 99.077% (34368/34688)\n",
            "280 391 Loss: 0.027 | Train Acc: 99.071% (35634/35968)\n",
            "290 391 Loss: 0.027 | Train Acc: 99.066% (36900/37248)\n",
            "300 391 Loss: 0.027 | Train Acc: 99.073% (38171/38528)\n",
            "310 391 Loss: 0.027 | Train Acc: 99.086% (39444/39808)\n",
            "320 391 Loss: 0.027 | Train Acc: 99.097% (40717/41088)\n",
            "330 391 Loss: 0.027 | Train Acc: 99.094% (41984/42368)\n",
            "340 391 Loss: 0.027 | Train Acc: 99.095% (43253/43648)\n",
            "350 391 Loss: 0.027 | Train Acc: 99.101% (44524/44928)\n",
            "360 391 Loss: 0.027 | Train Acc: 99.100% (45792/46208)\n",
            "370 391 Loss: 0.027 | Train Acc: 99.105% (47063/47488)\n",
            "380 391 Loss: 0.027 | Train Acc: 99.100% (48329/48768)\n",
            "390 391 Loss: 0.027 | Train Acc: 99.100% (49550/50000)\n",
            "0 100 Loss: 0.152 | Test Acc: 95.000% (95/100)\n",
            "10 100 Loss: 0.239 | Test Acc: 93.273% (1026/1100)\n",
            "20 100 Loss: 0.268 | Test Acc: 92.905% (1951/2100)\n",
            "30 100 Loss: 0.279 | Test Acc: 92.871% (2879/3100)\n",
            "40 100 Loss: 0.279 | Test Acc: 92.878% (3808/4100)\n",
            "50 100 Loss: 0.280 | Test Acc: 92.980% (4742/5100)\n",
            "60 100 Loss: 0.273 | Test Acc: 93.082% (5678/6100)\n",
            "70 100 Loss: 0.269 | Test Acc: 93.141% (6613/7100)\n",
            "80 100 Loss: 0.272 | Test Acc: 93.111% (7542/8100)\n",
            "90 100 Loss: 0.266 | Test Acc: 93.297% (8490/9100)\n",
            "\n",
            "Epoch: 161\n",
            "0 391 Loss: 0.023 | Train Acc: 99.219% (127/128)\n",
            "10 391 Loss: 0.023 | Train Acc: 99.503% (1401/1408)\n",
            "20 391 Loss: 0.025 | Train Acc: 99.368% (2671/2688)\n",
            "30 391 Loss: 0.026 | Train Acc: 99.345% (3942/3968)\n",
            "40 391 Loss: 0.024 | Train Acc: 99.409% (5217/5248)\n",
            "50 391 Loss: 0.025 | Train Acc: 99.403% (6489/6528)\n",
            "60 391 Loss: 0.025 | Train Acc: 99.308% (7754/7808)\n",
            "70 391 Loss: 0.025 | Train Acc: 99.296% (9024/9088)\n",
            "80 391 Loss: 0.024 | Train Acc: 99.334% (10299/10368)\n",
            "90 391 Loss: 0.024 | Train Acc: 99.305% (11567/11648)\n",
            "100 391 Loss: 0.024 | Train Acc: 99.257% (12832/12928)\n",
            "110 391 Loss: 0.024 | Train Acc: 99.233% (14099/14208)\n",
            "120 391 Loss: 0.024 | Train Acc: 99.245% (15371/15488)\n",
            "130 391 Loss: 0.024 | Train Acc: 99.243% (16641/16768)\n",
            "140 391 Loss: 0.024 | Train Acc: 99.258% (17914/18048)\n",
            "150 391 Loss: 0.024 | Train Acc: 99.245% (19182/19328)\n",
            "160 391 Loss: 0.025 | Train Acc: 99.224% (20448/20608)\n",
            "170 391 Loss: 0.025 | Train Acc: 99.219% (21717/21888)\n",
            "180 391 Loss: 0.024 | Train Acc: 99.210% (22985/23168)\n",
            "190 391 Loss: 0.025 | Train Acc: 99.182% (24248/24448)\n",
            "200 391 Loss: 0.025 | Train Acc: 99.180% (25517/25728)\n",
            "210 391 Loss: 0.026 | Train Acc: 99.167% (26783/27008)\n",
            "220 391 Loss: 0.026 | Train Acc: 99.159% (28050/28288)\n",
            "230 391 Loss: 0.026 | Train Acc: 99.154% (29318/29568)\n",
            "240 391 Loss: 0.026 | Train Acc: 99.138% (30582/30848)\n",
            "250 391 Loss: 0.027 | Train Acc: 99.125% (31847/32128)\n",
            "260 391 Loss: 0.027 | Train Acc: 99.123% (33115/33408)\n",
            "270 391 Loss: 0.027 | Train Acc: 99.126% (34385/34688)\n",
            "280 391 Loss: 0.028 | Train Acc: 99.108% (35647/35968)\n",
            "290 391 Loss: 0.027 | Train Acc: 99.117% (36919/37248)\n",
            "300 391 Loss: 0.028 | Train Acc: 99.105% (38183/38528)\n",
            "310 391 Loss: 0.028 | Train Acc: 99.093% (39447/39808)\n",
            "320 391 Loss: 0.028 | Train Acc: 99.102% (40719/41088)\n",
            "330 391 Loss: 0.027 | Train Acc: 99.110% (41991/42368)\n",
            "340 391 Loss: 0.027 | Train Acc: 99.113% (43261/43648)\n",
            "350 391 Loss: 0.027 | Train Acc: 99.110% (44528/44928)\n",
            "360 391 Loss: 0.027 | Train Acc: 99.100% (45792/46208)\n",
            "370 391 Loss: 0.027 | Train Acc: 99.095% (47058/47488)\n",
            "380 391 Loss: 0.027 | Train Acc: 99.102% (48330/48768)\n",
            "390 391 Loss: 0.027 | Train Acc: 99.110% (49555/50000)\n",
            "0 100 Loss: 0.172 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.218 | Test Acc: 94.636% (1041/1100)\n",
            "20 100 Loss: 0.251 | Test Acc: 93.571% (1965/2100)\n",
            "30 100 Loss: 0.271 | Test Acc: 93.387% (2895/3100)\n",
            "40 100 Loss: 0.270 | Test Acc: 93.463% (3832/4100)\n",
            "50 100 Loss: 0.266 | Test Acc: 93.588% (4773/5100)\n",
            "60 100 Loss: 0.262 | Test Acc: 93.672% (5714/6100)\n",
            "70 100 Loss: 0.253 | Test Acc: 93.732% (6655/7100)\n",
            "80 100 Loss: 0.253 | Test Acc: 93.765% (7595/8100)\n",
            "90 100 Loss: 0.248 | Test Acc: 93.824% (8538/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 162\n",
            "0 391 Loss: 0.016 | Train Acc: 99.219% (127/128)\n",
            "10 391 Loss: 0.023 | Train Acc: 99.219% (1397/1408)\n",
            "20 391 Loss: 0.024 | Train Acc: 99.219% (2667/2688)\n",
            "30 391 Loss: 0.024 | Train Acc: 99.219% (3937/3968)\n",
            "40 391 Loss: 0.025 | Train Acc: 99.181% (5205/5248)\n",
            "50 391 Loss: 0.024 | Train Acc: 99.203% (6476/6528)\n",
            "60 391 Loss: 0.023 | Train Acc: 99.232% (7748/7808)\n",
            "70 391 Loss: 0.022 | Train Acc: 99.274% (9022/9088)\n",
            "80 391 Loss: 0.022 | Train Acc: 99.257% (10291/10368)\n",
            "90 391 Loss: 0.022 | Train Acc: 99.279% (11564/11648)\n",
            "100 391 Loss: 0.022 | Train Acc: 99.250% (12831/12928)\n",
            "110 391 Loss: 0.022 | Train Acc: 99.282% (14106/14208)\n",
            "120 391 Loss: 0.021 | Train Acc: 99.329% (15384/15488)\n",
            "130 391 Loss: 0.022 | Train Acc: 99.314% (16653/16768)\n",
            "140 391 Loss: 0.021 | Train Acc: 99.330% (17927/18048)\n",
            "150 391 Loss: 0.022 | Train Acc: 99.317% (19196/19328)\n",
            "160 391 Loss: 0.022 | Train Acc: 99.311% (20466/20608)\n",
            "170 391 Loss: 0.022 | Train Acc: 99.306% (21736/21888)\n",
            "180 391 Loss: 0.022 | Train Acc: 99.296% (23005/23168)\n",
            "190 391 Loss: 0.022 | Train Acc: 99.288% (24274/24448)\n",
            "200 391 Loss: 0.022 | Train Acc: 99.296% (25547/25728)\n",
            "210 391 Loss: 0.022 | Train Acc: 99.304% (26820/27008)\n",
            "220 391 Loss: 0.022 | Train Acc: 99.311% (28093/28288)\n",
            "230 391 Loss: 0.022 | Train Acc: 99.293% (29359/29568)\n",
            "240 391 Loss: 0.023 | Train Acc: 99.290% (30629/30848)\n",
            "250 391 Loss: 0.022 | Train Acc: 99.281% (31897/32128)\n",
            "260 391 Loss: 0.023 | Train Acc: 99.270% (33164/33408)\n",
            "270 391 Loss: 0.022 | Train Acc: 99.279% (34438/34688)\n",
            "280 391 Loss: 0.022 | Train Acc: 99.283% (35710/35968)\n",
            "290 391 Loss: 0.022 | Train Acc: 99.278% (36979/37248)\n",
            "300 391 Loss: 0.023 | Train Acc: 99.273% (38248/38528)\n",
            "310 391 Loss: 0.023 | Train Acc: 99.277% (39520/39808)\n",
            "320 391 Loss: 0.023 | Train Acc: 99.287% (40795/41088)\n",
            "330 391 Loss: 0.023 | Train Acc: 99.290% (42067/42368)\n",
            "340 391 Loss: 0.022 | Train Acc: 99.297% (43341/43648)\n",
            "350 391 Loss: 0.022 | Train Acc: 99.292% (44610/44928)\n",
            "360 391 Loss: 0.022 | Train Acc: 99.294% (45882/46208)\n",
            "370 391 Loss: 0.023 | Train Acc: 99.290% (47151/47488)\n",
            "380 391 Loss: 0.022 | Train Acc: 99.291% (48422/48768)\n",
            "390 391 Loss: 0.022 | Train Acc: 99.286% (49643/50000)\n",
            "0 100 Loss: 0.195 | Test Acc: 95.000% (95/100)\n",
            "10 100 Loss: 0.254 | Test Acc: 93.091% (1024/1100)\n",
            "20 100 Loss: 0.250 | Test Acc: 93.143% (1956/2100)\n",
            "30 100 Loss: 0.267 | Test Acc: 93.097% (2886/3100)\n",
            "40 100 Loss: 0.270 | Test Acc: 93.024% (3814/4100)\n",
            "50 100 Loss: 0.270 | Test Acc: 93.216% (4754/5100)\n",
            "60 100 Loss: 0.264 | Test Acc: 93.246% (5688/6100)\n",
            "70 100 Loss: 0.257 | Test Acc: 93.394% (6631/7100)\n",
            "80 100 Loss: 0.254 | Test Acc: 93.481% (7572/8100)\n",
            "90 100 Loss: 0.251 | Test Acc: 93.571% (8515/9100)\n",
            "\n",
            "Epoch: 163\n",
            "0 391 Loss: 0.036 | Train Acc: 98.438% (126/128)\n",
            "10 391 Loss: 0.015 | Train Acc: 99.574% (1402/1408)\n",
            "20 391 Loss: 0.018 | Train Acc: 99.442% (2673/2688)\n",
            "30 391 Loss: 0.017 | Train Acc: 99.471% (3947/3968)\n",
            "40 391 Loss: 0.019 | Train Acc: 99.409% (5217/5248)\n",
            "50 391 Loss: 0.020 | Train Acc: 99.357% (6486/6528)\n",
            "60 391 Loss: 0.020 | Train Acc: 99.360% (7758/7808)\n",
            "70 391 Loss: 0.019 | Train Acc: 99.395% (9033/9088)\n",
            "80 391 Loss: 0.018 | Train Acc: 99.450% (10311/10368)\n",
            "90 391 Loss: 0.017 | Train Acc: 99.476% (11587/11648)\n",
            "100 391 Loss: 0.017 | Train Acc: 99.466% (12859/12928)\n",
            "110 391 Loss: 0.016 | Train Acc: 99.486% (14135/14208)\n",
            "120 391 Loss: 0.016 | Train Acc: 99.509% (15412/15488)\n",
            "130 391 Loss: 0.016 | Train Acc: 99.505% (16685/16768)\n",
            "140 391 Loss: 0.016 | Train Acc: 99.490% (17956/18048)\n",
            "150 391 Loss: 0.016 | Train Acc: 99.488% (19229/19328)\n",
            "160 391 Loss: 0.016 | Train Acc: 99.471% (20499/20608)\n",
            "170 391 Loss: 0.017 | Train Acc: 99.461% (21770/21888)\n",
            "180 391 Loss: 0.016 | Train Acc: 99.456% (23042/23168)\n",
            "190 391 Loss: 0.017 | Train Acc: 99.456% (24315/24448)\n",
            "200 391 Loss: 0.017 | Train Acc: 99.464% (25590/25728)\n",
            "210 391 Loss: 0.017 | Train Acc: 99.463% (26863/27008)\n",
            "220 391 Loss: 0.017 | Train Acc: 99.473% (28139/28288)\n",
            "230 391 Loss: 0.017 | Train Acc: 99.466% (29410/29568)\n",
            "240 391 Loss: 0.017 | Train Acc: 99.465% (30683/30848)\n",
            "250 391 Loss: 0.017 | Train Acc: 99.458% (31954/32128)\n",
            "260 391 Loss: 0.017 | Train Acc: 99.461% (33228/33408)\n",
            "270 391 Loss: 0.017 | Train Acc: 99.464% (34502/34688)\n",
            "280 391 Loss: 0.017 | Train Acc: 99.458% (35773/35968)\n",
            "290 391 Loss: 0.017 | Train Acc: 99.450% (37043/37248)\n",
            "300 391 Loss: 0.017 | Train Acc: 99.452% (38317/38528)\n",
            "310 391 Loss: 0.017 | Train Acc: 99.440% (39585/39808)\n",
            "320 391 Loss: 0.018 | Train Acc: 99.433% (40855/41088)\n",
            "330 391 Loss: 0.018 | Train Acc: 99.422% (42123/42368)\n",
            "340 391 Loss: 0.018 | Train Acc: 99.425% (43397/43648)\n",
            "350 391 Loss: 0.018 | Train Acc: 99.419% (44667/44928)\n",
            "360 391 Loss: 0.018 | Train Acc: 99.422% (45941/46208)\n",
            "370 391 Loss: 0.018 | Train Acc: 99.419% (47212/47488)\n",
            "380 391 Loss: 0.018 | Train Acc: 99.411% (48481/48768)\n",
            "390 391 Loss: 0.018 | Train Acc: 99.418% (49709/50000)\n",
            "0 100 Loss: 0.144 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.234 | Test Acc: 93.364% (1027/1100)\n",
            "20 100 Loss: 0.275 | Test Acc: 92.714% (1947/2100)\n",
            "30 100 Loss: 0.286 | Test Acc: 92.677% (2873/3100)\n",
            "40 100 Loss: 0.282 | Test Acc: 92.805% (3805/4100)\n",
            "50 100 Loss: 0.279 | Test Acc: 92.902% (4738/5100)\n",
            "60 100 Loss: 0.274 | Test Acc: 92.918% (5668/6100)\n",
            "70 100 Loss: 0.265 | Test Acc: 93.141% (6613/7100)\n",
            "80 100 Loss: 0.269 | Test Acc: 93.198% (7549/8100)\n",
            "90 100 Loss: 0.269 | Test Acc: 93.286% (8489/9100)\n",
            "\n",
            "Epoch: 164\n",
            "0 391 Loss: 0.002 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.018 | Train Acc: 99.290% (1398/1408)\n",
            "20 391 Loss: 0.019 | Train Acc: 99.442% (2673/2688)\n",
            "30 391 Loss: 0.022 | Train Acc: 99.269% (3939/3968)\n",
            "40 391 Loss: 0.021 | Train Acc: 99.371% (5215/5248)\n",
            "50 391 Loss: 0.021 | Train Acc: 99.387% (6488/6528)\n",
            "60 391 Loss: 0.019 | Train Acc: 99.436% (7764/7808)\n",
            "70 391 Loss: 0.018 | Train Acc: 99.461% (9039/9088)\n",
            "80 391 Loss: 0.018 | Train Acc: 99.479% (10314/10368)\n",
            "90 391 Loss: 0.017 | Train Acc: 99.493% (11589/11648)\n",
            "100 391 Loss: 0.016 | Train Acc: 99.520% (12866/12928)\n",
            "110 391 Loss: 0.017 | Train Acc: 99.535% (14142/14208)\n",
            "120 391 Loss: 0.016 | Train Acc: 99.542% (15417/15488)\n",
            "130 391 Loss: 0.017 | Train Acc: 99.511% (16686/16768)\n",
            "140 391 Loss: 0.017 | Train Acc: 99.518% (17961/18048)\n",
            "150 391 Loss: 0.017 | Train Acc: 99.529% (19237/19328)\n",
            "160 391 Loss: 0.017 | Train Acc: 99.529% (20511/20608)\n",
            "170 391 Loss: 0.017 | Train Acc: 99.534% (21786/21888)\n",
            "180 391 Loss: 0.017 | Train Acc: 99.525% (23058/23168)\n",
            "190 391 Loss: 0.017 | Train Acc: 99.538% (24335/24448)\n",
            "200 391 Loss: 0.016 | Train Acc: 99.549% (25612/25728)\n",
            "210 391 Loss: 0.017 | Train Acc: 99.541% (26884/27008)\n",
            "220 391 Loss: 0.017 | Train Acc: 99.533% (28156/28288)\n",
            "230 391 Loss: 0.017 | Train Acc: 99.530% (29429/29568)\n",
            "240 391 Loss: 0.017 | Train Acc: 99.527% (30702/30848)\n",
            "250 391 Loss: 0.017 | Train Acc: 99.524% (31975/32128)\n",
            "260 391 Loss: 0.017 | Train Acc: 99.530% (33251/33408)\n",
            "270 391 Loss: 0.017 | Train Acc: 99.524% (34523/34688)\n",
            "280 391 Loss: 0.017 | Train Acc: 99.538% (35802/35968)\n",
            "290 391 Loss: 0.017 | Train Acc: 99.530% (37073/37248)\n",
            "300 391 Loss: 0.017 | Train Acc: 99.530% (38347/38528)\n",
            "310 391 Loss: 0.017 | Train Acc: 99.523% (39618/39808)\n",
            "320 391 Loss: 0.017 | Train Acc: 99.516% (40889/41088)\n",
            "330 391 Loss: 0.017 | Train Acc: 99.521% (42165/42368)\n",
            "340 391 Loss: 0.017 | Train Acc: 99.517% (43437/43648)\n",
            "350 391 Loss: 0.017 | Train Acc: 99.519% (44712/44928)\n",
            "360 391 Loss: 0.017 | Train Acc: 99.524% (45988/46208)\n",
            "370 391 Loss: 0.017 | Train Acc: 99.516% (47258/47488)\n",
            "380 391 Loss: 0.017 | Train Acc: 99.514% (48531/48768)\n",
            "390 391 Loss: 0.017 | Train Acc: 99.512% (49756/50000)\n",
            "0 100 Loss: 0.156 | Test Acc: 94.000% (94/100)\n",
            "10 100 Loss: 0.251 | Test Acc: 94.364% (1038/1100)\n",
            "20 100 Loss: 0.240 | Test Acc: 93.810% (1970/2100)\n",
            "30 100 Loss: 0.268 | Test Acc: 93.710% (2905/3100)\n",
            "40 100 Loss: 0.265 | Test Acc: 93.732% (3843/4100)\n",
            "50 100 Loss: 0.269 | Test Acc: 93.667% (4777/5100)\n",
            "60 100 Loss: 0.264 | Test Acc: 93.705% (5716/6100)\n",
            "70 100 Loss: 0.261 | Test Acc: 93.746% (6656/7100)\n",
            "80 100 Loss: 0.260 | Test Acc: 93.815% (7599/8100)\n",
            "90 100 Loss: 0.259 | Test Acc: 93.868% (8542/9100)\n",
            "\n",
            "Epoch: 165\n",
            "0 391 Loss: 0.089 | Train Acc: 97.656% (125/128)\n",
            "10 391 Loss: 0.024 | Train Acc: 99.148% (1396/1408)\n",
            "20 391 Loss: 0.024 | Train Acc: 99.219% (2667/2688)\n",
            "30 391 Loss: 0.021 | Train Acc: 99.345% (3942/3968)\n",
            "40 391 Loss: 0.021 | Train Acc: 99.333% (5213/5248)\n",
            "50 391 Loss: 0.020 | Train Acc: 99.341% (6485/6528)\n",
            "60 391 Loss: 0.020 | Train Acc: 99.360% (7758/7808)\n",
            "70 391 Loss: 0.019 | Train Acc: 99.384% (9032/9088)\n",
            "80 391 Loss: 0.019 | Train Acc: 99.402% (10306/10368)\n",
            "90 391 Loss: 0.019 | Train Acc: 99.390% (11577/11648)\n",
            "100 391 Loss: 0.021 | Train Acc: 99.312% (12839/12928)\n",
            "110 391 Loss: 0.021 | Train Acc: 99.303% (14109/14208)\n",
            "120 391 Loss: 0.021 | Train Acc: 99.309% (15381/15488)\n",
            "130 391 Loss: 0.021 | Train Acc: 99.308% (16652/16768)\n",
            "140 391 Loss: 0.021 | Train Acc: 99.324% (17926/18048)\n",
            "150 391 Loss: 0.021 | Train Acc: 99.307% (19194/19328)\n",
            "160 391 Loss: 0.021 | Train Acc: 99.326% (20469/20608)\n",
            "170 391 Loss: 0.021 | Train Acc: 99.319% (21739/21888)\n",
            "180 391 Loss: 0.020 | Train Acc: 99.340% (23015/23168)\n",
            "190 391 Loss: 0.020 | Train Acc: 99.354% (24290/24448)\n",
            "200 391 Loss: 0.020 | Train Acc: 99.351% (25561/25728)\n",
            "210 391 Loss: 0.020 | Train Acc: 99.352% (26833/27008)\n",
            "220 391 Loss: 0.020 | Train Acc: 99.350% (28104/28288)\n",
            "230 391 Loss: 0.020 | Train Acc: 99.344% (29374/29568)\n",
            "240 391 Loss: 0.020 | Train Acc: 99.352% (30648/30848)\n",
            "250 391 Loss: 0.020 | Train Acc: 99.349% (31919/32128)\n",
            "260 391 Loss: 0.020 | Train Acc: 99.356% (33193/33408)\n",
            "270 391 Loss: 0.020 | Train Acc: 99.363% (34467/34688)\n",
            "280 391 Loss: 0.020 | Train Acc: 99.366% (35740/35968)\n",
            "290 391 Loss: 0.020 | Train Acc: 99.372% (37014/37248)\n",
            "300 391 Loss: 0.020 | Train Acc: 99.372% (38286/38528)\n",
            "310 391 Loss: 0.020 | Train Acc: 99.374% (39559/39808)\n",
            "320 391 Loss: 0.019 | Train Acc: 99.389% (40837/41088)\n",
            "330 391 Loss: 0.019 | Train Acc: 99.389% (42109/42368)\n",
            "340 391 Loss: 0.019 | Train Acc: 99.393% (43383/43648)\n",
            "350 391 Loss: 0.019 | Train Acc: 99.397% (44657/44928)\n",
            "360 391 Loss: 0.019 | Train Acc: 99.396% (45929/46208)\n",
            "370 391 Loss: 0.019 | Train Acc: 99.389% (47198/47488)\n",
            "380 391 Loss: 0.019 | Train Acc: 99.399% (48475/48768)\n",
            "390 391 Loss: 0.019 | Train Acc: 99.400% (49700/50000)\n",
            "0 100 Loss: 0.194 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.266 | Test Acc: 93.909% (1033/1100)\n",
            "20 100 Loss: 0.276 | Test Acc: 93.429% (1962/2100)\n",
            "30 100 Loss: 0.283 | Test Acc: 93.548% (2900/3100)\n",
            "40 100 Loss: 0.277 | Test Acc: 93.561% (3836/4100)\n",
            "50 100 Loss: 0.275 | Test Acc: 93.627% (4775/5100)\n",
            "60 100 Loss: 0.266 | Test Acc: 93.672% (5714/6100)\n",
            "70 100 Loss: 0.258 | Test Acc: 93.761% (6657/7100)\n",
            "80 100 Loss: 0.252 | Test Acc: 93.889% (7605/8100)\n",
            "90 100 Loss: 0.249 | Test Acc: 93.923% (8547/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 166\n",
            "0 391 Loss: 0.003 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.013 | Train Acc: 99.574% (1402/1408)\n",
            "20 391 Loss: 0.014 | Train Acc: 99.591% (2677/2688)\n",
            "30 391 Loss: 0.013 | Train Acc: 99.597% (3952/3968)\n",
            "40 391 Loss: 0.012 | Train Acc: 99.638% (5229/5248)\n",
            "50 391 Loss: 0.011 | Train Acc: 99.663% (6506/6528)\n",
            "60 391 Loss: 0.011 | Train Acc: 99.680% (7783/7808)\n",
            "70 391 Loss: 0.010 | Train Acc: 99.703% (9061/9088)\n",
            "80 391 Loss: 0.011 | Train Acc: 99.653% (10332/10368)\n",
            "90 391 Loss: 0.011 | Train Acc: 99.648% (11607/11648)\n",
            "100 391 Loss: 0.011 | Train Acc: 99.644% (12882/12928)\n",
            "110 391 Loss: 0.012 | Train Acc: 99.641% (14157/14208)\n",
            "120 391 Loss: 0.011 | Train Acc: 99.645% (15433/15488)\n",
            "130 391 Loss: 0.011 | Train Acc: 99.636% (16707/16768)\n",
            "140 391 Loss: 0.012 | Train Acc: 99.612% (17978/18048)\n",
            "150 391 Loss: 0.012 | Train Acc: 99.612% (19253/19328)\n",
            "160 391 Loss: 0.012 | Train Acc: 99.602% (20526/20608)\n",
            "170 391 Loss: 0.012 | Train Acc: 99.598% (21800/21888)\n",
            "180 391 Loss: 0.012 | Train Acc: 99.607% (23077/23168)\n",
            "190 391 Loss: 0.012 | Train Acc: 99.587% (24347/24448)\n",
            "200 391 Loss: 0.012 | Train Acc: 99.588% (25622/25728)\n",
            "210 391 Loss: 0.012 | Train Acc: 99.593% (26898/27008)\n",
            "220 391 Loss: 0.013 | Train Acc: 99.586% (28171/28288)\n",
            "230 391 Loss: 0.013 | Train Acc: 99.581% (29444/29568)\n",
            "240 391 Loss: 0.013 | Train Acc: 99.572% (30716/30848)\n",
            "250 391 Loss: 0.013 | Train Acc: 99.555% (31985/32128)\n",
            "260 391 Loss: 0.013 | Train Acc: 99.548% (33257/33408)\n",
            "270 391 Loss: 0.013 | Train Acc: 99.545% (34530/34688)\n",
            "280 391 Loss: 0.013 | Train Acc: 99.544% (35804/35968)\n",
            "290 391 Loss: 0.013 | Train Acc: 99.546% (37079/37248)\n",
            "300 391 Loss: 0.013 | Train Acc: 99.535% (38349/38528)\n",
            "310 391 Loss: 0.014 | Train Acc: 99.528% (39620/39808)\n",
            "320 391 Loss: 0.014 | Train Acc: 99.535% (40897/41088)\n",
            "330 391 Loss: 0.014 | Train Acc: 99.526% (42167/42368)\n",
            "340 391 Loss: 0.014 | Train Acc: 99.517% (43437/43648)\n",
            "350 391 Loss: 0.014 | Train Acc: 99.515% (44710/44928)\n",
            "360 391 Loss: 0.014 | Train Acc: 99.517% (45985/46208)\n",
            "370 391 Loss: 0.014 | Train Acc: 99.516% (47258/47488)\n",
            "380 391 Loss: 0.014 | Train Acc: 99.518% (48533/48768)\n",
            "390 391 Loss: 0.014 | Train Acc: 99.510% (49755/50000)\n",
            "0 100 Loss: 0.115 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.258 | Test Acc: 93.364% (1027/1100)\n",
            "20 100 Loss: 0.285 | Test Acc: 93.143% (1956/2100)\n",
            "30 100 Loss: 0.283 | Test Acc: 93.032% (2884/3100)\n",
            "40 100 Loss: 0.280 | Test Acc: 93.049% (3815/4100)\n",
            "50 100 Loss: 0.279 | Test Acc: 93.078% (4747/5100)\n",
            "60 100 Loss: 0.268 | Test Acc: 93.377% (5696/6100)\n",
            "70 100 Loss: 0.262 | Test Acc: 93.535% (6641/7100)\n",
            "80 100 Loss: 0.259 | Test Acc: 93.642% (7585/8100)\n",
            "90 100 Loss: 0.257 | Test Acc: 93.725% (8529/9100)\n",
            "\n",
            "Epoch: 167\n",
            "0 391 Loss: 0.014 | Train Acc: 99.219% (127/128)\n",
            "10 391 Loss: 0.014 | Train Acc: 99.432% (1400/1408)\n",
            "20 391 Loss: 0.016 | Train Acc: 99.405% (2672/2688)\n",
            "30 391 Loss: 0.015 | Train Acc: 99.420% (3945/3968)\n",
            "40 391 Loss: 0.015 | Train Acc: 99.409% (5217/5248)\n",
            "50 391 Loss: 0.014 | Train Acc: 99.464% (6493/6528)\n",
            "60 391 Loss: 0.014 | Train Acc: 99.501% (7769/7808)\n",
            "70 391 Loss: 0.015 | Train Acc: 99.472% (9040/9088)\n",
            "80 391 Loss: 0.015 | Train Acc: 99.479% (10314/10368)\n",
            "90 391 Loss: 0.015 | Train Acc: 99.511% (11591/11648)\n",
            "100 391 Loss: 0.015 | Train Acc: 99.513% (12865/12928)\n",
            "110 391 Loss: 0.015 | Train Acc: 99.500% (14137/14208)\n",
            "120 391 Loss: 0.015 | Train Acc: 99.503% (15411/15488)\n",
            "130 391 Loss: 0.016 | Train Acc: 99.481% (16681/16768)\n",
            "140 391 Loss: 0.016 | Train Acc: 99.479% (17954/18048)\n",
            "150 391 Loss: 0.016 | Train Acc: 99.488% (19229/19328)\n",
            "160 391 Loss: 0.015 | Train Acc: 99.505% (20506/20608)\n",
            "170 391 Loss: 0.015 | Train Acc: 99.493% (21777/21888)\n",
            "180 391 Loss: 0.015 | Train Acc: 99.508% (23054/23168)\n",
            "190 391 Loss: 0.015 | Train Acc: 99.509% (24328/24448)\n",
            "200 391 Loss: 0.015 | Train Acc: 99.514% (25603/25728)\n",
            "210 391 Loss: 0.014 | Train Acc: 99.526% (26880/27008)\n",
            "220 391 Loss: 0.015 | Train Acc: 99.519% (28152/28288)\n",
            "230 391 Loss: 0.015 | Train Acc: 99.520% (29426/29568)\n",
            "240 391 Loss: 0.015 | Train Acc: 99.514% (30698/30848)\n",
            "250 391 Loss: 0.015 | Train Acc: 99.505% (31969/32128)\n",
            "260 391 Loss: 0.015 | Train Acc: 99.503% (33242/33408)\n",
            "270 391 Loss: 0.015 | Train Acc: 99.498% (34514/34688)\n",
            "280 391 Loss: 0.015 | Train Acc: 99.505% (35790/35968)\n",
            "290 391 Loss: 0.015 | Train Acc: 99.503% (37063/37248)\n",
            "300 391 Loss: 0.015 | Train Acc: 99.491% (38332/38528)\n",
            "310 391 Loss: 0.015 | Train Acc: 99.488% (39604/39808)\n",
            "320 391 Loss: 0.015 | Train Acc: 99.491% (40879/41088)\n",
            "330 391 Loss: 0.015 | Train Acc: 99.490% (42152/42368)\n",
            "340 391 Loss: 0.015 | Train Acc: 99.491% (43426/43648)\n",
            "350 391 Loss: 0.015 | Train Acc: 99.495% (44701/44928)\n",
            "360 391 Loss: 0.015 | Train Acc: 99.500% (45977/46208)\n",
            "370 391 Loss: 0.015 | Train Acc: 99.505% (47253/47488)\n",
            "380 391 Loss: 0.015 | Train Acc: 99.506% (48527/48768)\n",
            "390 391 Loss: 0.015 | Train Acc: 99.510% (49755/50000)\n",
            "0 100 Loss: 0.202 | Test Acc: 92.000% (92/100)\n",
            "10 100 Loss: 0.268 | Test Acc: 94.455% (1039/1100)\n",
            "20 100 Loss: 0.267 | Test Acc: 94.143% (1977/2100)\n",
            "30 100 Loss: 0.281 | Test Acc: 93.839% (2909/3100)\n",
            "40 100 Loss: 0.270 | Test Acc: 94.000% (3854/4100)\n",
            "50 100 Loss: 0.269 | Test Acc: 93.863% (4787/5100)\n",
            "60 100 Loss: 0.257 | Test Acc: 93.918% (5729/6100)\n",
            "70 100 Loss: 0.253 | Test Acc: 93.915% (6668/7100)\n",
            "80 100 Loss: 0.251 | Test Acc: 94.062% (7619/8100)\n",
            "90 100 Loss: 0.249 | Test Acc: 94.099% (8563/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 168\n",
            "0 391 Loss: 0.009 | Train Acc: 99.219% (127/128)\n",
            "10 391 Loss: 0.016 | Train Acc: 99.503% (1401/1408)\n",
            "20 391 Loss: 0.016 | Train Acc: 99.442% (2673/2688)\n",
            "30 391 Loss: 0.015 | Train Acc: 99.521% (3949/3968)\n",
            "40 391 Loss: 0.015 | Train Acc: 99.543% (5224/5248)\n",
            "50 391 Loss: 0.014 | Train Acc: 99.571% (6500/6528)\n",
            "60 391 Loss: 0.013 | Train Acc: 99.616% (7778/7808)\n",
            "70 391 Loss: 0.012 | Train Acc: 99.648% (9056/9088)\n",
            "80 391 Loss: 0.013 | Train Acc: 99.633% (10330/10368)\n",
            "90 391 Loss: 0.012 | Train Acc: 99.648% (11607/11648)\n",
            "100 391 Loss: 0.012 | Train Acc: 99.636% (12881/12928)\n",
            "110 391 Loss: 0.013 | Train Acc: 99.634% (14156/14208)\n",
            "120 391 Loss: 0.013 | Train Acc: 99.632% (15431/15488)\n",
            "130 391 Loss: 0.012 | Train Acc: 99.648% (16709/16768)\n",
            "140 391 Loss: 0.012 | Train Acc: 99.651% (17985/18048)\n",
            "150 391 Loss: 0.012 | Train Acc: 99.653% (19261/19328)\n",
            "160 391 Loss: 0.012 | Train Acc: 99.651% (20536/20608)\n",
            "170 391 Loss: 0.012 | Train Acc: 99.657% (21813/21888)\n",
            "180 391 Loss: 0.012 | Train Acc: 99.646% (23086/23168)\n",
            "190 391 Loss: 0.012 | Train Acc: 99.644% (24361/24448)\n",
            "200 391 Loss: 0.012 | Train Acc: 99.635% (25634/25728)\n",
            "210 391 Loss: 0.012 | Train Acc: 99.633% (26909/27008)\n",
            "220 391 Loss: 0.012 | Train Acc: 99.639% (28186/28288)\n",
            "230 391 Loss: 0.012 | Train Acc: 99.642% (29462/29568)\n",
            "240 391 Loss: 0.012 | Train Acc: 99.647% (30739/30848)\n",
            "250 391 Loss: 0.012 | Train Acc: 99.648% (32015/32128)\n",
            "260 391 Loss: 0.011 | Train Acc: 99.650% (33291/33408)\n",
            "270 391 Loss: 0.011 | Train Acc: 99.648% (34566/34688)\n",
            "280 391 Loss: 0.011 | Train Acc: 99.658% (35845/35968)\n",
            "290 391 Loss: 0.011 | Train Acc: 99.662% (37122/37248)\n",
            "300 391 Loss: 0.011 | Train Acc: 99.663% (38398/38528)\n",
            "310 391 Loss: 0.011 | Train Acc: 99.666% (39675/39808)\n",
            "320 391 Loss: 0.011 | Train Acc: 99.671% (40953/41088)\n",
            "330 391 Loss: 0.011 | Train Acc: 99.674% (42230/42368)\n",
            "340 391 Loss: 0.011 | Train Acc: 99.675% (43506/43648)\n",
            "350 391 Loss: 0.011 | Train Acc: 99.679% (44784/44928)\n",
            "360 391 Loss: 0.011 | Train Acc: 99.678% (46059/46208)\n",
            "370 391 Loss: 0.011 | Train Acc: 99.682% (47337/47488)\n",
            "380 391 Loss: 0.011 | Train Acc: 99.682% (48613/48768)\n",
            "390 391 Loss: 0.011 | Train Acc: 99.680% (49840/50000)\n",
            "0 100 Loss: 0.249 | Test Acc: 94.000% (94/100)\n",
            "10 100 Loss: 0.277 | Test Acc: 93.818% (1032/1100)\n",
            "20 100 Loss: 0.267 | Test Acc: 93.905% (1972/2100)\n",
            "30 100 Loss: 0.271 | Test Acc: 93.935% (2912/3100)\n",
            "40 100 Loss: 0.266 | Test Acc: 94.049% (3856/4100)\n",
            "50 100 Loss: 0.262 | Test Acc: 94.216% (4805/5100)\n",
            "60 100 Loss: 0.248 | Test Acc: 94.459% (5762/6100)\n",
            "70 100 Loss: 0.241 | Test Acc: 94.437% (6705/7100)\n",
            "80 100 Loss: 0.244 | Test Acc: 94.370% (7644/8100)\n",
            "90 100 Loss: 0.241 | Test Acc: 94.418% (8592/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 169\n",
            "0 391 Loss: 0.003 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.007 | Train Acc: 99.858% (1406/1408)\n",
            "20 391 Loss: 0.007 | Train Acc: 99.814% (2683/2688)\n",
            "30 391 Loss: 0.009 | Train Acc: 99.748% (3958/3968)\n",
            "40 391 Loss: 0.008 | Train Acc: 99.752% (5235/5248)\n",
            "50 391 Loss: 0.007 | Train Acc: 99.786% (6514/6528)\n",
            "60 391 Loss: 0.008 | Train Acc: 99.744% (7788/7808)\n",
            "70 391 Loss: 0.008 | Train Acc: 99.747% (9065/9088)\n",
            "80 391 Loss: 0.008 | Train Acc: 99.720% (10339/10368)\n",
            "90 391 Loss: 0.009 | Train Acc: 99.725% (11616/11648)\n",
            "100 391 Loss: 0.009 | Train Acc: 99.706% (12890/12928)\n",
            "110 391 Loss: 0.009 | Train Acc: 99.711% (14167/14208)\n",
            "120 391 Loss: 0.010 | Train Acc: 99.697% (15441/15488)\n",
            "130 391 Loss: 0.010 | Train Acc: 99.684% (16715/16768)\n",
            "140 391 Loss: 0.010 | Train Acc: 99.701% (17994/18048)\n",
            "150 391 Loss: 0.010 | Train Acc: 99.695% (19269/19328)\n",
            "160 391 Loss: 0.010 | Train Acc: 99.694% (20545/20608)\n",
            "170 391 Loss: 0.010 | Train Acc: 99.712% (21825/21888)\n",
            "180 391 Loss: 0.010 | Train Acc: 99.706% (23100/23168)\n",
            "190 391 Loss: 0.010 | Train Acc: 99.705% (24376/24448)\n",
            "200 391 Loss: 0.010 | Train Acc: 99.712% (25654/25728)\n",
            "210 391 Loss: 0.010 | Train Acc: 99.704% (26928/27008)\n",
            "220 391 Loss: 0.010 | Train Acc: 99.710% (28206/28288)\n",
            "230 391 Loss: 0.010 | Train Acc: 99.709% (29482/29568)\n",
            "240 391 Loss: 0.010 | Train Acc: 99.708% (30758/30848)\n",
            "250 391 Loss: 0.010 | Train Acc: 99.717% (32037/32128)\n",
            "260 391 Loss: 0.010 | Train Acc: 99.701% (33308/33408)\n",
            "270 391 Loss: 0.010 | Train Acc: 99.703% (34585/34688)\n",
            "280 391 Loss: 0.010 | Train Acc: 99.691% (35857/35968)\n",
            "290 391 Loss: 0.010 | Train Acc: 99.697% (37135/37248)\n",
            "300 391 Loss: 0.010 | Train Acc: 99.702% (38413/38528)\n",
            "310 391 Loss: 0.010 | Train Acc: 99.711% (39693/39808)\n",
            "320 391 Loss: 0.010 | Train Acc: 99.710% (40969/41088)\n",
            "330 391 Loss: 0.010 | Train Acc: 99.707% (42244/42368)\n",
            "340 391 Loss: 0.010 | Train Acc: 99.709% (43521/43648)\n",
            "350 391 Loss: 0.010 | Train Acc: 99.711% (44798/44928)\n",
            "360 391 Loss: 0.010 | Train Acc: 99.708% (46073/46208)\n",
            "370 391 Loss: 0.010 | Train Acc: 99.709% (47350/47488)\n",
            "380 391 Loss: 0.010 | Train Acc: 99.713% (48628/48768)\n",
            "390 391 Loss: 0.010 | Train Acc: 99.714% (49857/50000)\n",
            "0 100 Loss: 0.187 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.240 | Test Acc: 94.636% (1041/1100)\n",
            "20 100 Loss: 0.244 | Test Acc: 94.238% (1979/2100)\n",
            "30 100 Loss: 0.257 | Test Acc: 94.226% (2921/3100)\n",
            "40 100 Loss: 0.252 | Test Acc: 94.244% (3864/4100)\n",
            "50 100 Loss: 0.258 | Test Acc: 94.196% (4804/5100)\n",
            "60 100 Loss: 0.248 | Test Acc: 94.344% (5755/6100)\n",
            "70 100 Loss: 0.241 | Test Acc: 94.437% (6705/7100)\n",
            "80 100 Loss: 0.240 | Test Acc: 94.469% (7652/8100)\n",
            "90 100 Loss: 0.236 | Test Acc: 94.516% (8601/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 170\n",
            "0 391 Loss: 0.005 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.007 | Train Acc: 99.858% (1406/1408)\n",
            "20 391 Loss: 0.008 | Train Acc: 99.740% (2681/2688)\n",
            "30 391 Loss: 0.007 | Train Acc: 99.798% (3960/3968)\n",
            "40 391 Loss: 0.007 | Train Acc: 99.790% (5237/5248)\n",
            "50 391 Loss: 0.007 | Train Acc: 99.770% (6513/6528)\n",
            "60 391 Loss: 0.007 | Train Acc: 99.782% (7791/7808)\n",
            "70 391 Loss: 0.007 | Train Acc: 99.791% (9069/9088)\n",
            "80 391 Loss: 0.007 | Train Acc: 99.778% (10345/10368)\n",
            "90 391 Loss: 0.008 | Train Acc: 99.760% (11620/11648)\n",
            "100 391 Loss: 0.008 | Train Acc: 99.768% (12898/12928)\n",
            "110 391 Loss: 0.008 | Train Acc: 99.761% (14174/14208)\n",
            "120 391 Loss: 0.008 | Train Acc: 99.761% (15451/15488)\n",
            "130 391 Loss: 0.008 | Train Acc: 99.773% (16730/16768)\n",
            "140 391 Loss: 0.007 | Train Acc: 99.784% (18009/18048)\n",
            "150 391 Loss: 0.007 | Train Acc: 99.778% (19285/19328)\n",
            "160 391 Loss: 0.008 | Train Acc: 99.772% (20561/20608)\n",
            "170 391 Loss: 0.008 | Train Acc: 99.762% (21836/21888)\n",
            "180 391 Loss: 0.008 | Train Acc: 99.771% (23115/23168)\n",
            "190 391 Loss: 0.008 | Train Acc: 99.779% (24394/24448)\n",
            "200 391 Loss: 0.008 | Train Acc: 99.771% (25669/25728)\n",
            "210 391 Loss: 0.008 | Train Acc: 99.774% (26947/27008)\n",
            "220 391 Loss: 0.008 | Train Acc: 99.760% (28220/28288)\n",
            "230 391 Loss: 0.008 | Train Acc: 99.750% (29494/29568)\n",
            "240 391 Loss: 0.008 | Train Acc: 99.757% (30773/30848)\n",
            "250 391 Loss: 0.008 | Train Acc: 99.760% (32051/32128)\n",
            "260 391 Loss: 0.007 | Train Acc: 99.761% (33328/33408)\n",
            "270 391 Loss: 0.007 | Train Acc: 99.766% (34607/34688)\n",
            "280 391 Loss: 0.007 | Train Acc: 99.766% (35884/35968)\n",
            "290 391 Loss: 0.007 | Train Acc: 99.766% (37161/37248)\n",
            "300 391 Loss: 0.007 | Train Acc: 99.772% (38440/38528)\n",
            "310 391 Loss: 0.007 | Train Acc: 99.771% (39717/39808)\n",
            "320 391 Loss: 0.007 | Train Acc: 99.774% (40995/41088)\n",
            "330 391 Loss: 0.007 | Train Acc: 99.773% (42272/42368)\n",
            "340 391 Loss: 0.007 | Train Acc: 99.780% (43552/43648)\n",
            "350 391 Loss: 0.007 | Train Acc: 99.784% (44831/44928)\n",
            "360 391 Loss: 0.007 | Train Acc: 99.786% (46109/46208)\n",
            "370 391 Loss: 0.007 | Train Acc: 99.789% (47388/47488)\n",
            "380 391 Loss: 0.007 | Train Acc: 99.793% (48667/48768)\n",
            "390 391 Loss: 0.007 | Train Acc: 99.798% (49899/50000)\n",
            "0 100 Loss: 0.160 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.234 | Test Acc: 94.273% (1037/1100)\n",
            "20 100 Loss: 0.252 | Test Acc: 93.571% (1965/2100)\n",
            "30 100 Loss: 0.263 | Test Acc: 93.871% (2910/3100)\n",
            "40 100 Loss: 0.258 | Test Acc: 93.829% (3847/4100)\n",
            "50 100 Loss: 0.260 | Test Acc: 93.863% (4787/5100)\n",
            "60 100 Loss: 0.250 | Test Acc: 94.049% (5737/6100)\n",
            "70 100 Loss: 0.243 | Test Acc: 94.211% (6689/7100)\n",
            "80 100 Loss: 0.240 | Test Acc: 94.272% (7636/8100)\n",
            "90 100 Loss: 0.238 | Test Acc: 94.308% (8582/9100)\n",
            "\n",
            "Epoch: 171\n",
            "0 391 Loss: 0.002 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.007 | Train Acc: 99.787% (1405/1408)\n",
            "20 391 Loss: 0.008 | Train Acc: 99.814% (2683/2688)\n",
            "30 391 Loss: 0.009 | Train Acc: 99.824% (3961/3968)\n",
            "40 391 Loss: 0.008 | Train Acc: 99.848% (5240/5248)\n",
            "50 391 Loss: 0.007 | Train Acc: 99.847% (6518/6528)\n",
            "60 391 Loss: 0.007 | Train Acc: 99.872% (7798/7808)\n",
            "70 391 Loss: 0.006 | Train Acc: 99.890% (9078/9088)\n",
            "80 391 Loss: 0.006 | Train Acc: 99.894% (10357/10368)\n",
            "90 391 Loss: 0.006 | Train Acc: 99.897% (11636/11648)\n",
            "100 391 Loss: 0.005 | Train Acc: 99.907% (12916/12928)\n",
            "110 391 Loss: 0.005 | Train Acc: 99.894% (14193/14208)\n",
            "120 391 Loss: 0.005 | Train Acc: 99.884% (15470/15488)\n",
            "130 391 Loss: 0.005 | Train Acc: 99.881% (16748/16768)\n",
            "140 391 Loss: 0.005 | Train Acc: 99.884% (18027/18048)\n",
            "150 391 Loss: 0.005 | Train Acc: 99.876% (19304/19328)\n",
            "160 391 Loss: 0.005 | Train Acc: 99.884% (20584/20608)\n",
            "170 391 Loss: 0.006 | Train Acc: 99.877% (21861/21888)\n",
            "180 391 Loss: 0.006 | Train Acc: 99.862% (23136/23168)\n",
            "190 391 Loss: 0.006 | Train Acc: 99.865% (24415/24448)\n",
            "200 391 Loss: 0.006 | Train Acc: 99.868% (25694/25728)\n",
            "210 391 Loss: 0.006 | Train Acc: 99.870% (26973/27008)\n",
            "220 391 Loss: 0.006 | Train Acc: 99.876% (28253/28288)\n",
            "230 391 Loss: 0.006 | Train Acc: 99.871% (29530/29568)\n",
            "240 391 Loss: 0.006 | Train Acc: 99.867% (30807/30848)\n",
            "250 391 Loss: 0.006 | Train Acc: 99.863% (32084/32128)\n",
            "260 391 Loss: 0.006 | Train Acc: 99.859% (33361/33408)\n",
            "270 391 Loss: 0.006 | Train Acc: 99.865% (34641/34688)\n",
            "280 391 Loss: 0.006 | Train Acc: 99.869% (35921/35968)\n",
            "290 391 Loss: 0.006 | Train Acc: 99.871% (37200/37248)\n",
            "300 391 Loss: 0.005 | Train Acc: 99.870% (38478/38528)\n",
            "310 391 Loss: 0.005 | Train Acc: 99.874% (39758/39808)\n",
            "320 391 Loss: 0.005 | Train Acc: 99.876% (41037/41088)\n",
            "330 391 Loss: 0.005 | Train Acc: 99.875% (42315/42368)\n",
            "340 391 Loss: 0.005 | Train Acc: 99.879% (43595/43648)\n",
            "350 391 Loss: 0.005 | Train Acc: 99.875% (44872/44928)\n",
            "360 391 Loss: 0.005 | Train Acc: 99.879% (46152/46208)\n",
            "370 391 Loss: 0.005 | Train Acc: 99.880% (47431/47488)\n",
            "380 391 Loss: 0.005 | Train Acc: 99.879% (48709/48768)\n",
            "390 391 Loss: 0.005 | Train Acc: 99.880% (49940/50000)\n",
            "0 100 Loss: 0.160 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.234 | Test Acc: 93.727% (1031/1100)\n",
            "20 100 Loss: 0.239 | Test Acc: 93.524% (1964/2100)\n",
            "30 100 Loss: 0.245 | Test Acc: 94.000% (2914/3100)\n",
            "40 100 Loss: 0.247 | Test Acc: 94.049% (3856/4100)\n",
            "50 100 Loss: 0.251 | Test Acc: 94.157% (4802/5100)\n",
            "60 100 Loss: 0.240 | Test Acc: 94.311% (5753/6100)\n",
            "70 100 Loss: 0.233 | Test Acc: 94.465% (6707/7100)\n",
            "80 100 Loss: 0.233 | Test Acc: 94.506% (7655/8100)\n",
            "90 100 Loss: 0.231 | Test Acc: 94.582% (8607/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 172\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.005 | Train Acc: 99.787% (1405/1408)\n",
            "20 391 Loss: 0.005 | Train Acc: 99.851% (2684/2688)\n",
            "30 391 Loss: 0.005 | Train Acc: 99.874% (3963/3968)\n",
            "40 391 Loss: 0.005 | Train Acc: 99.886% (5242/5248)\n",
            "50 391 Loss: 0.005 | Train Acc: 99.877% (6520/6528)\n",
            "60 391 Loss: 0.005 | Train Acc: 99.872% (7798/7808)\n",
            "70 391 Loss: 0.005 | Train Acc: 99.868% (9076/9088)\n",
            "80 391 Loss: 0.005 | Train Acc: 99.865% (10354/10368)\n",
            "90 391 Loss: 0.005 | Train Acc: 99.880% (11634/11648)\n",
            "100 391 Loss: 0.005 | Train Acc: 99.884% (12913/12928)\n",
            "110 391 Loss: 0.005 | Train Acc: 99.894% (14193/14208)\n",
            "120 391 Loss: 0.005 | Train Acc: 99.903% (15473/15488)\n",
            "130 391 Loss: 0.005 | Train Acc: 99.899% (16751/16768)\n",
            "140 391 Loss: 0.005 | Train Acc: 99.895% (18029/18048)\n",
            "150 391 Loss: 0.004 | Train Acc: 99.902% (19309/19328)\n",
            "160 391 Loss: 0.004 | Train Acc: 99.903% (20588/20608)\n",
            "170 391 Loss: 0.004 | Train Acc: 99.909% (21868/21888)\n",
            "180 391 Loss: 0.004 | Train Acc: 99.905% (23146/23168)\n",
            "190 391 Loss: 0.004 | Train Acc: 99.906% (24425/24448)\n",
            "200 391 Loss: 0.004 | Train Acc: 99.907% (25704/25728)\n",
            "210 391 Loss: 0.004 | Train Acc: 99.911% (26984/27008)\n",
            "220 391 Loss: 0.004 | Train Acc: 99.915% (28264/28288)\n",
            "230 391 Loss: 0.004 | Train Acc: 99.919% (29544/29568)\n",
            "240 391 Loss: 0.004 | Train Acc: 99.919% (30823/30848)\n",
            "250 391 Loss: 0.004 | Train Acc: 99.916% (32101/32128)\n",
            "260 391 Loss: 0.004 | Train Acc: 99.919% (33381/33408)\n",
            "270 391 Loss: 0.004 | Train Acc: 99.916% (34659/34688)\n",
            "280 391 Loss: 0.004 | Train Acc: 99.911% (35936/35968)\n",
            "290 391 Loss: 0.004 | Train Acc: 99.909% (37214/37248)\n",
            "300 391 Loss: 0.004 | Train Acc: 99.907% (38492/38528)\n",
            "310 391 Loss: 0.004 | Train Acc: 99.907% (39771/39808)\n",
            "320 391 Loss: 0.004 | Train Acc: 99.910% (41051/41088)\n",
            "330 391 Loss: 0.004 | Train Acc: 99.910% (42330/42368)\n",
            "340 391 Loss: 0.004 | Train Acc: 99.913% (43610/43648)\n",
            "350 391 Loss: 0.004 | Train Acc: 99.907% (44886/44928)\n",
            "360 391 Loss: 0.004 | Train Acc: 99.905% (46164/46208)\n",
            "370 391 Loss: 0.004 | Train Acc: 99.907% (47444/47488)\n",
            "380 391 Loss: 0.004 | Train Acc: 99.908% (48723/48768)\n",
            "390 391 Loss: 0.004 | Train Acc: 99.908% (49954/50000)\n",
            "0 100 Loss: 0.222 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.243 | Test Acc: 94.545% (1040/1100)\n",
            "20 100 Loss: 0.248 | Test Acc: 93.905% (1972/2100)\n",
            "30 100 Loss: 0.258 | Test Acc: 93.935% (2912/3100)\n",
            "40 100 Loss: 0.261 | Test Acc: 93.976% (3853/4100)\n",
            "50 100 Loss: 0.257 | Test Acc: 94.118% (4800/5100)\n",
            "60 100 Loss: 0.246 | Test Acc: 94.311% (5753/6100)\n",
            "70 100 Loss: 0.239 | Test Acc: 94.380% (6701/7100)\n",
            "80 100 Loss: 0.236 | Test Acc: 94.395% (7646/8100)\n",
            "90 100 Loss: 0.232 | Test Acc: 94.440% (8594/9100)\n",
            "\n",
            "Epoch: 173\n",
            "0 391 Loss: 0.006 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.004 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.005 | Train Acc: 99.926% (2686/2688)\n",
            "30 391 Loss: 0.006 | Train Acc: 99.874% (3963/3968)\n",
            "40 391 Loss: 0.006 | Train Acc: 99.867% (5241/5248)\n",
            "50 391 Loss: 0.006 | Train Acc: 99.831% (6517/6528)\n",
            "60 391 Loss: 0.006 | Train Acc: 99.834% (7795/7808)\n",
            "70 391 Loss: 0.006 | Train Acc: 99.835% (9073/9088)\n",
            "80 391 Loss: 0.006 | Train Acc: 99.826% (10350/10368)\n",
            "90 391 Loss: 0.006 | Train Acc: 99.845% (11630/11648)\n",
            "100 391 Loss: 0.006 | Train Acc: 99.853% (12909/12928)\n",
            "110 391 Loss: 0.006 | Train Acc: 99.859% (14188/14208)\n",
            "120 391 Loss: 0.005 | Train Acc: 99.858% (15466/15488)\n",
            "130 391 Loss: 0.005 | Train Acc: 99.863% (16745/16768)\n",
            "140 391 Loss: 0.005 | Train Acc: 99.856% (18022/18048)\n",
            "150 391 Loss: 0.005 | Train Acc: 99.855% (19300/19328)\n",
            "160 391 Loss: 0.005 | Train Acc: 99.859% (20579/20608)\n",
            "170 391 Loss: 0.005 | Train Acc: 99.868% (21859/21888)\n",
            "180 391 Loss: 0.005 | Train Acc: 99.866% (23137/23168)\n",
            "190 391 Loss: 0.005 | Train Acc: 99.861% (24414/24448)\n",
            "200 391 Loss: 0.005 | Train Acc: 99.852% (25690/25728)\n",
            "210 391 Loss: 0.005 | Train Acc: 99.856% (26969/27008)\n",
            "220 391 Loss: 0.006 | Train Acc: 99.837% (28242/28288)\n",
            "230 391 Loss: 0.006 | Train Acc: 99.838% (29520/29568)\n",
            "240 391 Loss: 0.005 | Train Acc: 99.844% (30800/30848)\n",
            "250 391 Loss: 0.005 | Train Acc: 99.844% (32078/32128)\n",
            "260 391 Loss: 0.005 | Train Acc: 99.847% (33357/33408)\n",
            "270 391 Loss: 0.005 | Train Acc: 99.850% (34636/34688)\n",
            "280 391 Loss: 0.005 | Train Acc: 99.844% (35912/35968)\n",
            "290 391 Loss: 0.006 | Train Acc: 99.839% (37188/37248)\n",
            "300 391 Loss: 0.006 | Train Acc: 99.839% (38466/38528)\n",
            "310 391 Loss: 0.005 | Train Acc: 99.842% (39745/39808)\n",
            "320 391 Loss: 0.005 | Train Acc: 99.842% (41023/41088)\n",
            "330 391 Loss: 0.005 | Train Acc: 99.844% (42302/42368)\n",
            "340 391 Loss: 0.005 | Train Acc: 99.844% (43580/43648)\n",
            "350 391 Loss: 0.005 | Train Acc: 99.842% (44857/44928)\n",
            "360 391 Loss: 0.005 | Train Acc: 99.846% (46137/46208)\n",
            "370 391 Loss: 0.005 | Train Acc: 99.850% (47417/47488)\n",
            "380 391 Loss: 0.005 | Train Acc: 99.844% (48692/48768)\n",
            "390 391 Loss: 0.005 | Train Acc: 99.848% (49924/50000)\n",
            "0 100 Loss: 0.218 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.252 | Test Acc: 94.545% (1040/1100)\n",
            "20 100 Loss: 0.270 | Test Acc: 93.810% (1970/2100)\n",
            "30 100 Loss: 0.273 | Test Acc: 93.935% (2912/3100)\n",
            "40 100 Loss: 0.271 | Test Acc: 94.098% (3858/4100)\n",
            "50 100 Loss: 0.265 | Test Acc: 94.294% (4809/5100)\n",
            "60 100 Loss: 0.253 | Test Acc: 94.410% (5759/6100)\n",
            "70 100 Loss: 0.244 | Test Acc: 94.577% (6715/7100)\n",
            "80 100 Loss: 0.244 | Test Acc: 94.605% (7663/8100)\n",
            "90 100 Loss: 0.238 | Test Acc: 94.582% (8607/9100)\n",
            "\n",
            "Epoch: 174\n",
            "0 391 Loss: 0.005 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.006 | Train Acc: 99.787% (1405/1408)\n",
            "20 391 Loss: 0.005 | Train Acc: 99.851% (2684/2688)\n",
            "30 391 Loss: 0.006 | Train Acc: 99.824% (3961/3968)\n",
            "40 391 Loss: 0.007 | Train Acc: 99.790% (5237/5248)\n",
            "50 391 Loss: 0.006 | Train Acc: 99.816% (6516/6528)\n",
            "60 391 Loss: 0.006 | Train Acc: 99.846% (7796/7808)\n",
            "70 391 Loss: 0.006 | Train Acc: 99.835% (9073/9088)\n",
            "80 391 Loss: 0.007 | Train Acc: 99.826% (10350/10368)\n",
            "90 391 Loss: 0.006 | Train Acc: 99.837% (11629/11648)\n",
            "100 391 Loss: 0.006 | Train Acc: 99.822% (12905/12928)\n",
            "110 391 Loss: 0.006 | Train Acc: 99.824% (14183/14208)\n",
            "120 391 Loss: 0.006 | Train Acc: 99.826% (15461/15488)\n",
            "130 391 Loss: 0.006 | Train Acc: 99.839% (16741/16768)\n",
            "140 391 Loss: 0.006 | Train Acc: 99.850% (18021/18048)\n",
            "150 391 Loss: 0.006 | Train Acc: 99.845% (19298/19328)\n",
            "160 391 Loss: 0.006 | Train Acc: 99.845% (20576/20608)\n",
            "170 391 Loss: 0.006 | Train Acc: 99.831% (21851/21888)\n",
            "180 391 Loss: 0.006 | Train Acc: 99.836% (23130/23168)\n",
            "190 391 Loss: 0.006 | Train Acc: 99.840% (24409/24448)\n",
            "200 391 Loss: 0.006 | Train Acc: 99.845% (25688/25728)\n",
            "210 391 Loss: 0.006 | Train Acc: 99.837% (26964/27008)\n",
            "220 391 Loss: 0.006 | Train Acc: 99.844% (28244/28288)\n",
            "230 391 Loss: 0.006 | Train Acc: 99.844% (29522/29568)\n",
            "240 391 Loss: 0.006 | Train Acc: 99.848% (30801/30848)\n",
            "250 391 Loss: 0.006 | Train Acc: 99.847% (32079/32128)\n",
            "260 391 Loss: 0.006 | Train Acc: 99.853% (33359/33408)\n",
            "270 391 Loss: 0.005 | Train Acc: 99.859% (34639/34688)\n",
            "280 391 Loss: 0.005 | Train Acc: 99.864% (35919/35968)\n",
            "290 391 Loss: 0.005 | Train Acc: 99.863% (37197/37248)\n",
            "300 391 Loss: 0.005 | Train Acc: 99.862% (38475/38528)\n",
            "310 391 Loss: 0.005 | Train Acc: 99.867% (39755/39808)\n",
            "320 391 Loss: 0.005 | Train Acc: 99.864% (41032/41088)\n",
            "330 391 Loss: 0.005 | Train Acc: 99.868% (42312/42368)\n",
            "340 391 Loss: 0.005 | Train Acc: 99.867% (43590/43648)\n",
            "350 391 Loss: 0.005 | Train Acc: 99.869% (44869/44928)\n",
            "360 391 Loss: 0.005 | Train Acc: 99.872% (46149/46208)\n",
            "370 391 Loss: 0.005 | Train Acc: 99.869% (47426/47488)\n",
            "380 391 Loss: 0.005 | Train Acc: 99.871% (48705/48768)\n",
            "390 391 Loss: 0.005 | Train Acc: 99.870% (49935/50000)\n",
            "0 100 Loss: 0.244 | Test Acc: 94.000% (94/100)\n",
            "10 100 Loss: 0.239 | Test Acc: 94.364% (1038/1100)\n",
            "20 100 Loss: 0.254 | Test Acc: 93.857% (1971/2100)\n",
            "30 100 Loss: 0.257 | Test Acc: 94.000% (2914/3100)\n",
            "40 100 Loss: 0.250 | Test Acc: 94.195% (3862/4100)\n",
            "50 100 Loss: 0.251 | Test Acc: 94.275% (4808/5100)\n",
            "60 100 Loss: 0.239 | Test Acc: 94.492% (5764/6100)\n",
            "70 100 Loss: 0.231 | Test Acc: 94.676% (6722/7100)\n",
            "80 100 Loss: 0.235 | Test Acc: 94.667% (7668/8100)\n",
            "90 100 Loss: 0.229 | Test Acc: 94.725% (8620/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 175\n",
            "0 391 Loss: 0.002 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.002 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.003 | Train Acc: 99.950% (3966/3968)\n",
            "40 391 Loss: 0.003 | Train Acc: 99.962% (5246/5248)\n",
            "50 391 Loss: 0.003 | Train Acc: 99.954% (6525/6528)\n",
            "60 391 Loss: 0.003 | Train Acc: 99.962% (7805/7808)\n",
            "70 391 Loss: 0.003 | Train Acc: 99.956% (9084/9088)\n",
            "80 391 Loss: 0.003 | Train Acc: 99.952% (10363/10368)\n",
            "90 391 Loss: 0.003 | Train Acc: 99.957% (11643/11648)\n",
            "100 391 Loss: 0.003 | Train Acc: 99.961% (12923/12928)\n",
            "110 391 Loss: 0.004 | Train Acc: 99.930% (14198/14208)\n",
            "120 391 Loss: 0.004 | Train Acc: 99.935% (15478/15488)\n",
            "130 391 Loss: 0.003 | Train Acc: 99.940% (16758/16768)\n",
            "140 391 Loss: 0.004 | Train Acc: 99.939% (18037/18048)\n",
            "150 391 Loss: 0.004 | Train Acc: 99.938% (19316/19328)\n",
            "160 391 Loss: 0.004 | Train Acc: 99.927% (20593/20608)\n",
            "170 391 Loss: 0.004 | Train Acc: 99.931% (21873/21888)\n",
            "180 391 Loss: 0.004 | Train Acc: 99.931% (23152/23168)\n",
            "190 391 Loss: 0.004 | Train Acc: 99.922% (24429/24448)\n",
            "200 391 Loss: 0.004 | Train Acc: 99.922% (25708/25728)\n",
            "210 391 Loss: 0.004 | Train Acc: 99.922% (26987/27008)\n",
            "220 391 Loss: 0.004 | Train Acc: 99.922% (28266/28288)\n",
            "230 391 Loss: 0.004 | Train Acc: 99.919% (29544/29568)\n",
            "240 391 Loss: 0.004 | Train Acc: 99.912% (30821/30848)\n",
            "250 391 Loss: 0.004 | Train Acc: 99.916% (32101/32128)\n",
            "260 391 Loss: 0.004 | Train Acc: 99.916% (33380/33408)\n",
            "270 391 Loss: 0.004 | Train Acc: 99.919% (34660/34688)\n",
            "280 391 Loss: 0.004 | Train Acc: 99.917% (35938/35968)\n",
            "290 391 Loss: 0.004 | Train Acc: 99.917% (37217/37248)\n",
            "300 391 Loss: 0.004 | Train Acc: 99.917% (38496/38528)\n",
            "310 391 Loss: 0.004 | Train Acc: 99.920% (39776/39808)\n",
            "320 391 Loss: 0.004 | Train Acc: 99.917% (41054/41088)\n",
            "330 391 Loss: 0.004 | Train Acc: 99.917% (42333/42368)\n",
            "340 391 Loss: 0.004 | Train Acc: 99.918% (43612/43648)\n",
            "350 391 Loss: 0.004 | Train Acc: 99.918% (44891/44928)\n",
            "360 391 Loss: 0.004 | Train Acc: 99.918% (46170/46208)\n",
            "370 391 Loss: 0.004 | Train Acc: 99.920% (47450/47488)\n",
            "380 391 Loss: 0.004 | Train Acc: 99.922% (48730/48768)\n",
            "390 391 Loss: 0.004 | Train Acc: 99.924% (49962/50000)\n",
            "0 100 Loss: 0.250 | Test Acc: 95.000% (95/100)\n",
            "10 100 Loss: 0.226 | Test Acc: 95.182% (1047/1100)\n",
            "20 100 Loss: 0.257 | Test Acc: 94.143% (1977/2100)\n",
            "30 100 Loss: 0.261 | Test Acc: 94.161% (2919/3100)\n",
            "40 100 Loss: 0.253 | Test Acc: 94.317% (3867/4100)\n",
            "50 100 Loss: 0.248 | Test Acc: 94.431% (4816/5100)\n",
            "60 100 Loss: 0.238 | Test Acc: 94.623% (5772/6100)\n",
            "70 100 Loss: 0.231 | Test Acc: 94.775% (6729/7100)\n",
            "80 100 Loss: 0.232 | Test Acc: 94.753% (7675/8100)\n",
            "90 100 Loss: 0.229 | Test Acc: 94.813% (8628/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 176\n",
            "0 391 Loss: 0.005 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.004 | Train Acc: 99.858% (1406/1408)\n",
            "20 391 Loss: 0.004 | Train Acc: 99.888% (2685/2688)\n",
            "30 391 Loss: 0.004 | Train Acc: 99.899% (3964/3968)\n",
            "40 391 Loss: 0.004 | Train Acc: 99.924% (5244/5248)\n",
            "50 391 Loss: 0.003 | Train Acc: 99.939% (6524/6528)\n",
            "60 391 Loss: 0.004 | Train Acc: 99.923% (7802/7808)\n",
            "70 391 Loss: 0.004 | Train Acc: 99.923% (9081/9088)\n",
            "80 391 Loss: 0.004 | Train Acc: 99.923% (10360/10368)\n",
            "90 391 Loss: 0.004 | Train Acc: 99.931% (11640/11648)\n",
            "100 391 Loss: 0.004 | Train Acc: 99.915% (12917/12928)\n",
            "110 391 Loss: 0.004 | Train Acc: 99.916% (14196/14208)\n",
            "120 391 Loss: 0.004 | Train Acc: 99.923% (15476/15488)\n",
            "130 391 Loss: 0.004 | Train Acc: 99.917% (16754/16768)\n",
            "140 391 Loss: 0.004 | Train Acc: 99.911% (18032/18048)\n",
            "150 391 Loss: 0.004 | Train Acc: 99.917% (19312/19328)\n",
            "160 391 Loss: 0.004 | Train Acc: 99.913% (20590/20608)\n",
            "170 391 Loss: 0.004 | Train Acc: 99.904% (21867/21888)\n",
            "180 391 Loss: 0.004 | Train Acc: 99.896% (23144/23168)\n",
            "190 391 Loss: 0.004 | Train Acc: 99.902% (24424/24448)\n",
            "200 391 Loss: 0.004 | Train Acc: 99.899% (25702/25728)\n",
            "210 391 Loss: 0.004 | Train Acc: 99.885% (26977/27008)\n",
            "220 391 Loss: 0.004 | Train Acc: 99.887% (28256/28288)\n",
            "230 391 Loss: 0.004 | Train Acc: 99.888% (29535/29568)\n",
            "240 391 Loss: 0.004 | Train Acc: 99.890% (30814/30848)\n",
            "250 391 Loss: 0.004 | Train Acc: 99.891% (32093/32128)\n",
            "260 391 Loss: 0.004 | Train Acc: 99.895% (33373/33408)\n",
            "270 391 Loss: 0.004 | Train Acc: 99.893% (34651/34688)\n",
            "280 391 Loss: 0.004 | Train Acc: 99.892% (35929/35968)\n",
            "290 391 Loss: 0.004 | Train Acc: 99.895% (37209/37248)\n",
            "300 391 Loss: 0.004 | Train Acc: 99.899% (38489/38528)\n",
            "310 391 Loss: 0.004 | Train Acc: 99.897% (39767/39808)\n",
            "320 391 Loss: 0.004 | Train Acc: 99.895% (41045/41088)\n",
            "330 391 Loss: 0.004 | Train Acc: 99.894% (42323/42368)\n",
            "340 391 Loss: 0.004 | Train Acc: 99.895% (43602/43648)\n",
            "350 391 Loss: 0.004 | Train Acc: 99.893% (44880/44928)\n",
            "360 391 Loss: 0.004 | Train Acc: 99.892% (46158/46208)\n",
            "370 391 Loss: 0.004 | Train Acc: 99.893% (47437/47488)\n",
            "380 391 Loss: 0.004 | Train Acc: 99.887% (48713/48768)\n",
            "390 391 Loss: 0.004 | Train Acc: 99.884% (49942/50000)\n",
            "0 100 Loss: 0.165 | Test Acc: 95.000% (95/100)\n",
            "10 100 Loss: 0.222 | Test Acc: 95.000% (1045/1100)\n",
            "20 100 Loss: 0.251 | Test Acc: 94.238% (1979/2100)\n",
            "30 100 Loss: 0.259 | Test Acc: 94.355% (2925/3100)\n",
            "40 100 Loss: 0.251 | Test Acc: 94.439% (3872/4100)\n",
            "50 100 Loss: 0.248 | Test Acc: 94.569% (4823/5100)\n",
            "60 100 Loss: 0.240 | Test Acc: 94.705% (5777/6100)\n",
            "70 100 Loss: 0.231 | Test Acc: 94.761% (6728/7100)\n",
            "80 100 Loss: 0.232 | Test Acc: 94.654% (7667/8100)\n",
            "90 100 Loss: 0.228 | Test Acc: 94.769% (8624/9100)\n",
            "\n",
            "Epoch: 177\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.003 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.003 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.003 | Train Acc: 99.975% (3967/3968)\n",
            "40 391 Loss: 0.003 | Train Acc: 99.962% (5246/5248)\n",
            "50 391 Loss: 0.003 | Train Acc: 99.954% (6525/6528)\n",
            "60 391 Loss: 0.003 | Train Acc: 99.936% (7803/7808)\n",
            "70 391 Loss: 0.003 | Train Acc: 99.945% (9083/9088)\n",
            "80 391 Loss: 0.003 | Train Acc: 99.942% (10362/10368)\n",
            "90 391 Loss: 0.003 | Train Acc: 99.948% (11642/11648)\n",
            "100 391 Loss: 0.003 | Train Acc: 99.946% (12921/12928)\n",
            "110 391 Loss: 0.003 | Train Acc: 99.944% (14200/14208)\n",
            "120 391 Loss: 0.003 | Train Acc: 99.948% (15480/15488)\n",
            "130 391 Loss: 0.003 | Train Acc: 99.946% (16759/16768)\n",
            "140 391 Loss: 0.003 | Train Acc: 99.950% (18039/18048)\n",
            "150 391 Loss: 0.003 | Train Acc: 99.953% (19319/19328)\n",
            "160 391 Loss: 0.003 | Train Acc: 99.956% (20599/20608)\n",
            "170 391 Loss: 0.003 | Train Acc: 99.954% (21878/21888)\n",
            "180 391 Loss: 0.003 | Train Acc: 99.948% (23156/23168)\n",
            "190 391 Loss: 0.003 | Train Acc: 99.947% (24435/24448)\n",
            "200 391 Loss: 0.003 | Train Acc: 99.946% (25714/25728)\n",
            "210 391 Loss: 0.003 | Train Acc: 99.948% (26994/27008)\n",
            "220 391 Loss: 0.003 | Train Acc: 99.947% (28273/28288)\n",
            "230 391 Loss: 0.003 | Train Acc: 99.949% (29553/29568)\n",
            "240 391 Loss: 0.003 | Train Acc: 99.951% (30833/30848)\n",
            "250 391 Loss: 0.003 | Train Acc: 99.953% (32113/32128)\n",
            "260 391 Loss: 0.003 | Train Acc: 99.955% (33393/33408)\n",
            "270 391 Loss: 0.003 | Train Acc: 99.957% (34673/34688)\n",
            "280 391 Loss: 0.003 | Train Acc: 99.950% (35950/35968)\n",
            "290 391 Loss: 0.003 | Train Acc: 99.949% (37229/37248)\n",
            "300 391 Loss: 0.003 | Train Acc: 99.951% (38509/38528)\n",
            "310 391 Loss: 0.003 | Train Acc: 99.952% (39789/39808)\n",
            "320 391 Loss: 0.003 | Train Acc: 99.954% (41069/41088)\n",
            "330 391 Loss: 0.003 | Train Acc: 99.955% (42349/42368)\n",
            "340 391 Loss: 0.003 | Train Acc: 99.956% (43629/43648)\n",
            "350 391 Loss: 0.003 | Train Acc: 99.958% (44909/44928)\n",
            "360 391 Loss: 0.003 | Train Acc: 99.957% (46188/46208)\n",
            "370 391 Loss: 0.003 | Train Acc: 99.958% (47468/47488)\n",
            "380 391 Loss: 0.003 | Train Acc: 99.959% (48748/48768)\n",
            "390 391 Loss: 0.003 | Train Acc: 99.958% (49979/50000)\n",
            "0 100 Loss: 0.175 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.213 | Test Acc: 95.091% (1046/1100)\n",
            "20 100 Loss: 0.241 | Test Acc: 94.238% (1979/2100)\n",
            "30 100 Loss: 0.244 | Test Acc: 94.516% (2930/3100)\n",
            "40 100 Loss: 0.239 | Test Acc: 94.683% (3882/4100)\n",
            "50 100 Loss: 0.237 | Test Acc: 94.765% (4833/5100)\n",
            "60 100 Loss: 0.228 | Test Acc: 94.934% (5791/6100)\n",
            "70 100 Loss: 0.219 | Test Acc: 94.986% (6744/7100)\n",
            "80 100 Loss: 0.221 | Test Acc: 94.938% (7690/8100)\n",
            "90 100 Loss: 0.217 | Test Acc: 95.033% (8648/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 178\n",
            "0 391 Loss: 0.000 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.002 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.002 | Train Acc: 99.963% (2687/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 99.950% (3966/3968)\n",
            "40 391 Loss: 0.002 | Train Acc: 99.943% (5245/5248)\n",
            "50 391 Loss: 0.002 | Train Acc: 99.954% (6525/6528)\n",
            "60 391 Loss: 0.002 | Train Acc: 99.949% (7804/7808)\n",
            "70 391 Loss: 0.003 | Train Acc: 99.934% (9082/9088)\n",
            "80 391 Loss: 0.003 | Train Acc: 99.923% (10360/10368)\n",
            "90 391 Loss: 0.003 | Train Acc: 99.931% (11640/11648)\n",
            "100 391 Loss: 0.003 | Train Acc: 99.930% (12919/12928)\n",
            "110 391 Loss: 0.003 | Train Acc: 99.923% (14197/14208)\n",
            "120 391 Loss: 0.003 | Train Acc: 99.929% (15477/15488)\n",
            "130 391 Loss: 0.003 | Train Acc: 99.928% (16756/16768)\n",
            "140 391 Loss: 0.003 | Train Acc: 99.922% (18034/18048)\n",
            "150 391 Loss: 0.003 | Train Acc: 99.917% (19312/19328)\n",
            "160 391 Loss: 0.003 | Train Acc: 99.913% (20590/20608)\n",
            "170 391 Loss: 0.003 | Train Acc: 99.913% (21869/21888)\n",
            "180 391 Loss: 0.003 | Train Acc: 99.918% (23149/23168)\n",
            "190 391 Loss: 0.003 | Train Acc: 99.918% (24428/24448)\n",
            "200 391 Loss: 0.003 | Train Acc: 99.922% (25708/25728)\n",
            "210 391 Loss: 0.003 | Train Acc: 99.922% (26987/27008)\n",
            "220 391 Loss: 0.003 | Train Acc: 99.922% (28266/28288)\n",
            "230 391 Loss: 0.003 | Train Acc: 99.922% (29545/29568)\n",
            "240 391 Loss: 0.003 | Train Acc: 99.925% (30825/30848)\n",
            "250 391 Loss: 0.003 | Train Acc: 99.928% (32105/32128)\n",
            "260 391 Loss: 0.003 | Train Acc: 99.931% (33385/33408)\n",
            "270 391 Loss: 0.003 | Train Acc: 99.931% (34664/34688)\n",
            "280 391 Loss: 0.003 | Train Acc: 99.928% (35942/35968)\n",
            "290 391 Loss: 0.003 | Train Acc: 99.928% (37221/37248)\n",
            "300 391 Loss: 0.003 | Train Acc: 99.920% (38497/38528)\n",
            "310 391 Loss: 0.003 | Train Acc: 99.922% (39777/39808)\n",
            "320 391 Loss: 0.003 | Train Acc: 99.925% (41057/41088)\n",
            "330 391 Loss: 0.003 | Train Acc: 99.922% (42335/42368)\n",
            "340 391 Loss: 0.003 | Train Acc: 99.924% (43615/43648)\n",
            "350 391 Loss: 0.003 | Train Acc: 99.922% (44893/44928)\n",
            "360 391 Loss: 0.003 | Train Acc: 99.922% (46172/46208)\n",
            "370 391 Loss: 0.003 | Train Acc: 99.922% (47451/47488)\n",
            "380 391 Loss: 0.003 | Train Acc: 99.924% (48731/48768)\n",
            "390 391 Loss: 0.003 | Train Acc: 99.924% (49962/50000)\n",
            "0 100 Loss: 0.200 | Test Acc: 95.000% (95/100)\n",
            "10 100 Loss: 0.218 | Test Acc: 95.091% (1046/1100)\n",
            "20 100 Loss: 0.241 | Test Acc: 94.333% (1981/2100)\n",
            "30 100 Loss: 0.249 | Test Acc: 94.452% (2928/3100)\n",
            "40 100 Loss: 0.248 | Test Acc: 94.561% (3877/4100)\n",
            "50 100 Loss: 0.247 | Test Acc: 94.667% (4828/5100)\n",
            "60 100 Loss: 0.237 | Test Acc: 94.836% (5785/6100)\n",
            "70 100 Loss: 0.230 | Test Acc: 94.831% (6733/7100)\n",
            "80 100 Loss: 0.233 | Test Acc: 94.827% (7681/8100)\n",
            "90 100 Loss: 0.227 | Test Acc: 94.879% (8634/9100)\n",
            "\n",
            "Epoch: 179\n",
            "0 391 Loss: 0.002 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.001 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 100.000% (3968/3968)\n",
            "40 391 Loss: 0.002 | Train Acc: 100.000% (5248/5248)\n",
            "50 391 Loss: 0.002 | Train Acc: 100.000% (6528/6528)\n",
            "60 391 Loss: 0.002 | Train Acc: 99.987% (7807/7808)\n",
            "70 391 Loss: 0.002 | Train Acc: 99.978% (9086/9088)\n",
            "80 391 Loss: 0.002 | Train Acc: 99.981% (10366/10368)\n",
            "90 391 Loss: 0.003 | Train Acc: 99.966% (11644/11648)\n",
            "100 391 Loss: 0.002 | Train Acc: 99.969% (12924/12928)\n",
            "110 391 Loss: 0.003 | Train Acc: 99.965% (14203/14208)\n",
            "120 391 Loss: 0.003 | Train Acc: 99.961% (15482/15488)\n",
            "130 391 Loss: 0.003 | Train Acc: 99.964% (16762/16768)\n",
            "140 391 Loss: 0.003 | Train Acc: 99.961% (18041/18048)\n",
            "150 391 Loss: 0.003 | Train Acc: 99.959% (19320/19328)\n",
            "160 391 Loss: 0.002 | Train Acc: 99.961% (20600/20608)\n",
            "170 391 Loss: 0.002 | Train Acc: 99.959% (21879/21888)\n",
            "180 391 Loss: 0.002 | Train Acc: 99.961% (23159/23168)\n",
            "190 391 Loss: 0.002 | Train Acc: 99.959% (24438/24448)\n",
            "200 391 Loss: 0.002 | Train Acc: 99.957% (25717/25728)\n",
            "210 391 Loss: 0.002 | Train Acc: 99.952% (26995/27008)\n",
            "220 391 Loss: 0.002 | Train Acc: 99.954% (28275/28288)\n",
            "230 391 Loss: 0.002 | Train Acc: 99.956% (29555/29568)\n",
            "240 391 Loss: 0.002 | Train Acc: 99.955% (30834/30848)\n",
            "250 391 Loss: 0.002 | Train Acc: 99.953% (32113/32128)\n",
            "260 391 Loss: 0.002 | Train Acc: 99.946% (33390/33408)\n",
            "270 391 Loss: 0.002 | Train Acc: 99.945% (34669/34688)\n",
            "280 391 Loss: 0.002 | Train Acc: 99.947% (35949/35968)\n",
            "290 391 Loss: 0.002 | Train Acc: 99.949% (37229/37248)\n",
            "300 391 Loss: 0.002 | Train Acc: 99.948% (38508/38528)\n",
            "310 391 Loss: 0.002 | Train Acc: 99.950% (39788/39808)\n",
            "320 391 Loss: 0.003 | Train Acc: 99.949% (41067/41088)\n",
            "330 391 Loss: 0.003 | Train Acc: 99.948% (42346/42368)\n",
            "340 391 Loss: 0.003 | Train Acc: 99.950% (43626/43648)\n",
            "350 391 Loss: 0.003 | Train Acc: 99.949% (44905/44928)\n",
            "360 391 Loss: 0.003 | Train Acc: 99.948% (46184/46208)\n",
            "370 391 Loss: 0.003 | Train Acc: 99.949% (47464/47488)\n",
            "380 391 Loss: 0.003 | Train Acc: 99.951% (48744/48768)\n",
            "390 391 Loss: 0.003 | Train Acc: 99.950% (49975/50000)\n",
            "0 100 Loss: 0.132 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.212 | Test Acc: 95.091% (1046/1100)\n",
            "20 100 Loss: 0.242 | Test Acc: 94.381% (1982/2100)\n",
            "30 100 Loss: 0.248 | Test Acc: 94.484% (2929/3100)\n",
            "40 100 Loss: 0.245 | Test Acc: 94.512% (3875/4100)\n",
            "50 100 Loss: 0.242 | Test Acc: 94.627% (4826/5100)\n",
            "60 100 Loss: 0.234 | Test Acc: 94.803% (5783/6100)\n",
            "70 100 Loss: 0.225 | Test Acc: 94.789% (6730/7100)\n",
            "80 100 Loss: 0.227 | Test Acc: 94.741% (7674/8100)\n",
            "90 100 Loss: 0.223 | Test Acc: 94.879% (8634/9100)\n",
            "\n",
            "Epoch: 180\n",
            "0 391 Loss: 0.000 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.002 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.002 | Train Acc: 99.963% (2687/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 99.950% (3966/3968)\n",
            "40 391 Loss: 0.002 | Train Acc: 99.962% (5246/5248)\n",
            "50 391 Loss: 0.002 | Train Acc: 99.969% (6526/6528)\n",
            "60 391 Loss: 0.002 | Train Acc: 99.962% (7805/7808)\n",
            "70 391 Loss: 0.002 | Train Acc: 99.945% (9083/9088)\n",
            "80 391 Loss: 0.002 | Train Acc: 99.952% (10363/10368)\n",
            "90 391 Loss: 0.002 | Train Acc: 99.957% (11643/11648)\n",
            "100 391 Loss: 0.002 | Train Acc: 99.961% (12923/12928)\n",
            "110 391 Loss: 0.002 | Train Acc: 99.965% (14203/14208)\n",
            "120 391 Loss: 0.002 | Train Acc: 99.961% (15482/15488)\n",
            "130 391 Loss: 0.002 | Train Acc: 99.964% (16762/16768)\n",
            "140 391 Loss: 0.002 | Train Acc: 99.967% (18042/18048)\n",
            "150 391 Loss: 0.002 | Train Acc: 99.969% (19322/19328)\n",
            "160 391 Loss: 0.002 | Train Acc: 99.951% (20598/20608)\n",
            "170 391 Loss: 0.002 | Train Acc: 99.950% (21877/21888)\n",
            "180 391 Loss: 0.002 | Train Acc: 99.948% (23156/23168)\n",
            "190 391 Loss: 0.002 | Train Acc: 99.951% (24436/24448)\n",
            "200 391 Loss: 0.002 | Train Acc: 99.953% (25716/25728)\n",
            "210 391 Loss: 0.003 | Train Acc: 99.948% (26994/27008)\n",
            "220 391 Loss: 0.002 | Train Acc: 99.951% (28274/28288)\n",
            "230 391 Loss: 0.003 | Train Acc: 99.946% (29552/29568)\n",
            "240 391 Loss: 0.002 | Train Acc: 99.945% (30831/30848)\n",
            "250 391 Loss: 0.002 | Train Acc: 99.947% (32111/32128)\n",
            "260 391 Loss: 0.002 | Train Acc: 99.949% (33391/33408)\n",
            "270 391 Loss: 0.002 | Train Acc: 99.948% (34670/34688)\n",
            "280 391 Loss: 0.002 | Train Acc: 99.950% (35950/35968)\n",
            "290 391 Loss: 0.002 | Train Acc: 99.949% (37229/37248)\n",
            "300 391 Loss: 0.002 | Train Acc: 99.951% (38509/38528)\n",
            "310 391 Loss: 0.002 | Train Acc: 99.952% (39789/39808)\n",
            "320 391 Loss: 0.002 | Train Acc: 99.954% (41069/41088)\n",
            "330 391 Loss: 0.002 | Train Acc: 99.955% (42349/42368)\n",
            "340 391 Loss: 0.002 | Train Acc: 99.954% (43628/43648)\n",
            "350 391 Loss: 0.003 | Train Acc: 99.944% (44903/44928)\n",
            "360 391 Loss: 0.003 | Train Acc: 99.946% (46183/46208)\n",
            "370 391 Loss: 0.003 | Train Acc: 99.947% (47463/47488)\n",
            "380 391 Loss: 0.003 | Train Acc: 99.949% (48743/48768)\n",
            "390 391 Loss: 0.003 | Train Acc: 99.948% (49974/50000)\n",
            "0 100 Loss: 0.141 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.207 | Test Acc: 95.182% (1047/1100)\n",
            "20 100 Loss: 0.239 | Test Acc: 94.524% (1985/2100)\n",
            "30 100 Loss: 0.250 | Test Acc: 94.484% (2929/3100)\n",
            "40 100 Loss: 0.245 | Test Acc: 94.610% (3879/4100)\n",
            "50 100 Loss: 0.244 | Test Acc: 94.686% (4829/5100)\n",
            "60 100 Loss: 0.236 | Test Acc: 94.836% (5785/6100)\n",
            "70 100 Loss: 0.227 | Test Acc: 94.845% (6734/7100)\n",
            "80 100 Loss: 0.230 | Test Acc: 94.802% (7679/8100)\n",
            "90 100 Loss: 0.225 | Test Acc: 94.890% (8635/9100)\n",
            "\n",
            "Epoch: 181\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.002 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.002 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 99.975% (3967/3968)\n",
            "40 391 Loss: 0.002 | Train Acc: 99.943% (5245/5248)\n",
            "50 391 Loss: 0.002 | Train Acc: 99.939% (6524/6528)\n",
            "60 391 Loss: 0.002 | Train Acc: 99.949% (7804/7808)\n",
            "70 391 Loss: 0.002 | Train Acc: 99.956% (9084/9088)\n",
            "80 391 Loss: 0.002 | Train Acc: 99.961% (10364/10368)\n",
            "90 391 Loss: 0.002 | Train Acc: 99.966% (11644/11648)\n",
            "100 391 Loss: 0.002 | Train Acc: 99.961% (12923/12928)\n",
            "110 391 Loss: 0.002 | Train Acc: 99.965% (14203/14208)\n",
            "120 391 Loss: 0.002 | Train Acc: 99.961% (15482/15488)\n",
            "130 391 Loss: 0.002 | Train Acc: 99.958% (16761/16768)\n",
            "140 391 Loss: 0.002 | Train Acc: 99.961% (18041/18048)\n",
            "150 391 Loss: 0.002 | Train Acc: 99.964% (19321/19328)\n",
            "160 391 Loss: 0.002 | Train Acc: 99.961% (20600/20608)\n",
            "170 391 Loss: 0.002 | Train Acc: 99.959% (21879/21888)\n",
            "180 391 Loss: 0.002 | Train Acc: 99.961% (23159/23168)\n",
            "190 391 Loss: 0.002 | Train Acc: 99.959% (24438/24448)\n",
            "200 391 Loss: 0.002 | Train Acc: 99.961% (25718/25728)\n",
            "210 391 Loss: 0.002 | Train Acc: 99.963% (26998/27008)\n",
            "220 391 Loss: 0.002 | Train Acc: 99.961% (28277/28288)\n",
            "230 391 Loss: 0.002 | Train Acc: 99.963% (29557/29568)\n",
            "240 391 Loss: 0.002 | Train Acc: 99.964% (30837/30848)\n",
            "250 391 Loss: 0.002 | Train Acc: 99.966% (32117/32128)\n",
            "260 391 Loss: 0.002 | Train Acc: 99.967% (33397/33408)\n",
            "270 391 Loss: 0.002 | Train Acc: 99.965% (34676/34688)\n",
            "280 391 Loss: 0.002 | Train Acc: 99.967% (35956/35968)\n",
            "290 391 Loss: 0.002 | Train Acc: 99.968% (37236/37248)\n",
            "300 391 Loss: 0.002 | Train Acc: 99.966% (38515/38528)\n",
            "310 391 Loss: 0.002 | Train Acc: 99.967% (39795/39808)\n",
            "320 391 Loss: 0.002 | Train Acc: 99.968% (41075/41088)\n",
            "330 391 Loss: 0.002 | Train Acc: 99.969% (42355/42368)\n",
            "340 391 Loss: 0.002 | Train Acc: 99.970% (43635/43648)\n",
            "350 391 Loss: 0.002 | Train Acc: 99.971% (44915/44928)\n",
            "360 391 Loss: 0.002 | Train Acc: 99.970% (46194/46208)\n",
            "370 391 Loss: 0.002 | Train Acc: 99.966% (47472/47488)\n",
            "380 391 Loss: 0.002 | Train Acc: 99.965% (48751/48768)\n",
            "390 391 Loss: 0.002 | Train Acc: 99.966% (49983/50000)\n",
            "0 100 Loss: 0.134 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.209 | Test Acc: 95.182% (1047/1100)\n",
            "20 100 Loss: 0.235 | Test Acc: 94.381% (1982/2100)\n",
            "30 100 Loss: 0.243 | Test Acc: 94.581% (2932/3100)\n",
            "40 100 Loss: 0.238 | Test Acc: 94.659% (3881/4100)\n",
            "50 100 Loss: 0.238 | Test Acc: 94.765% (4833/5100)\n",
            "60 100 Loss: 0.229 | Test Acc: 94.967% (5793/6100)\n",
            "70 100 Loss: 0.220 | Test Acc: 95.042% (6748/7100)\n",
            "80 100 Loss: 0.223 | Test Acc: 94.914% (7688/8100)\n",
            "90 100 Loss: 0.218 | Test Acc: 95.022% (8647/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 182\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.002 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.002 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 99.975% (3967/3968)\n",
            "40 391 Loss: 0.002 | Train Acc: 99.962% (5246/5248)\n",
            "50 391 Loss: 0.002 | Train Acc: 99.969% (6526/6528)\n",
            "60 391 Loss: 0.002 | Train Acc: 99.962% (7805/7808)\n",
            "70 391 Loss: 0.002 | Train Acc: 99.967% (9085/9088)\n",
            "80 391 Loss: 0.002 | Train Acc: 99.952% (10363/10368)\n",
            "90 391 Loss: 0.002 | Train Acc: 99.957% (11643/11648)\n",
            "100 391 Loss: 0.002 | Train Acc: 99.961% (12923/12928)\n",
            "110 391 Loss: 0.002 | Train Acc: 99.965% (14203/14208)\n",
            "120 391 Loss: 0.002 | Train Acc: 99.968% (15483/15488)\n",
            "130 391 Loss: 0.002 | Train Acc: 99.970% (16763/16768)\n",
            "140 391 Loss: 0.002 | Train Acc: 99.972% (18043/18048)\n",
            "150 391 Loss: 0.002 | Train Acc: 99.974% (19323/19328)\n",
            "160 391 Loss: 0.002 | Train Acc: 99.976% (20603/20608)\n",
            "170 391 Loss: 0.002 | Train Acc: 99.977% (21883/21888)\n",
            "180 391 Loss: 0.002 | Train Acc: 99.978% (23163/23168)\n",
            "190 391 Loss: 0.002 | Train Acc: 99.975% (24442/24448)\n",
            "200 391 Loss: 0.002 | Train Acc: 99.973% (25721/25728)\n",
            "210 391 Loss: 0.002 | Train Acc: 99.974% (27001/27008)\n",
            "220 391 Loss: 0.002 | Train Acc: 99.975% (28281/28288)\n",
            "230 391 Loss: 0.002 | Train Acc: 99.976% (29561/29568)\n",
            "240 391 Loss: 0.002 | Train Acc: 99.971% (30839/30848)\n",
            "250 391 Loss: 0.002 | Train Acc: 99.963% (32116/32128)\n",
            "260 391 Loss: 0.002 | Train Acc: 99.958% (33394/33408)\n",
            "270 391 Loss: 0.002 | Train Acc: 99.957% (34673/34688)\n",
            "280 391 Loss: 0.002 | Train Acc: 99.958% (35953/35968)\n",
            "290 391 Loss: 0.002 | Train Acc: 99.960% (37233/37248)\n",
            "300 391 Loss: 0.002 | Train Acc: 99.961% (38513/38528)\n",
            "310 391 Loss: 0.002 | Train Acc: 99.962% (39793/39808)\n",
            "320 391 Loss: 0.002 | Train Acc: 99.963% (41073/41088)\n",
            "330 391 Loss: 0.002 | Train Acc: 99.962% (42352/42368)\n",
            "340 391 Loss: 0.002 | Train Acc: 99.963% (43632/43648)\n",
            "350 391 Loss: 0.002 | Train Acc: 99.958% (44909/44928)\n",
            "360 391 Loss: 0.002 | Train Acc: 99.959% (46189/46208)\n",
            "370 391 Loss: 0.002 | Train Acc: 99.960% (47469/47488)\n",
            "380 391 Loss: 0.002 | Train Acc: 99.959% (48748/48768)\n",
            "390 391 Loss: 0.002 | Train Acc: 99.960% (49980/50000)\n",
            "0 100 Loss: 0.139 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.214 | Test Acc: 95.182% (1047/1100)\n",
            "20 100 Loss: 0.233 | Test Acc: 94.476% (1984/2100)\n",
            "30 100 Loss: 0.242 | Test Acc: 94.581% (2932/3100)\n",
            "40 100 Loss: 0.236 | Test Acc: 94.732% (3884/4100)\n",
            "50 100 Loss: 0.239 | Test Acc: 94.784% (4834/5100)\n",
            "60 100 Loss: 0.230 | Test Acc: 94.951% (5792/6100)\n",
            "70 100 Loss: 0.222 | Test Acc: 95.028% (6747/7100)\n",
            "80 100 Loss: 0.223 | Test Acc: 94.963% (7692/8100)\n",
            "90 100 Loss: 0.218 | Test Acc: 95.044% (8649/9100)\n",
            "\n",
            "Epoch: 183\n",
            "0 391 Loss: 0.002 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.002 | Train Acc: 99.963% (2687/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 99.975% (3967/3968)\n",
            "40 391 Loss: 0.001 | Train Acc: 99.981% (5247/5248)\n",
            "50 391 Loss: 0.001 | Train Acc: 99.985% (6527/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 99.987% (7807/7808)\n",
            "70 391 Loss: 0.002 | Train Acc: 99.978% (9086/9088)\n",
            "80 391 Loss: 0.002 | Train Acc: 99.981% (10366/10368)\n",
            "90 391 Loss: 0.002 | Train Acc: 99.983% (11646/11648)\n",
            "100 391 Loss: 0.002 | Train Acc: 99.977% (12925/12928)\n",
            "110 391 Loss: 0.002 | Train Acc: 99.979% (14205/14208)\n",
            "120 391 Loss: 0.002 | Train Acc: 99.981% (15485/15488)\n",
            "130 391 Loss: 0.002 | Train Acc: 99.982% (16765/16768)\n",
            "140 391 Loss: 0.002 | Train Acc: 99.983% (18045/18048)\n",
            "150 391 Loss: 0.002 | Train Acc: 99.984% (19325/19328)\n",
            "160 391 Loss: 0.002 | Train Acc: 99.981% (20604/20608)\n",
            "170 391 Loss: 0.002 | Train Acc: 99.982% (21884/21888)\n",
            "180 391 Loss: 0.002 | Train Acc: 99.978% (23163/23168)\n",
            "190 391 Loss: 0.002 | Train Acc: 99.980% (24443/24448)\n",
            "200 391 Loss: 0.002 | Train Acc: 99.977% (25722/25728)\n",
            "210 391 Loss: 0.002 | Train Acc: 99.974% (27001/27008)\n",
            "220 391 Loss: 0.002 | Train Acc: 99.975% (28281/28288)\n",
            "230 391 Loss: 0.002 | Train Acc: 99.976% (29561/29568)\n",
            "240 391 Loss: 0.002 | Train Acc: 99.977% (30841/30848)\n",
            "250 391 Loss: 0.002 | Train Acc: 99.978% (32121/32128)\n",
            "260 391 Loss: 0.002 | Train Acc: 99.979% (33401/33408)\n",
            "270 391 Loss: 0.002 | Train Acc: 99.980% (34681/34688)\n",
            "280 391 Loss: 0.002 | Train Acc: 99.981% (35961/35968)\n",
            "290 391 Loss: 0.002 | Train Acc: 99.981% (37241/37248)\n",
            "300 391 Loss: 0.002 | Train Acc: 99.982% (38521/38528)\n",
            "310 391 Loss: 0.002 | Train Acc: 99.982% (39801/39808)\n",
            "320 391 Loss: 0.002 | Train Acc: 99.983% (41081/41088)\n",
            "330 391 Loss: 0.002 | Train Acc: 99.983% (42361/42368)\n",
            "340 391 Loss: 0.002 | Train Acc: 99.984% (43641/43648)\n",
            "350 391 Loss: 0.002 | Train Acc: 99.984% (44921/44928)\n",
            "360 391 Loss: 0.002 | Train Acc: 99.985% (46201/46208)\n",
            "370 391 Loss: 0.002 | Train Acc: 99.983% (47480/47488)\n",
            "380 391 Loss: 0.002 | Train Acc: 99.984% (48760/48768)\n",
            "390 391 Loss: 0.002 | Train Acc: 99.982% (49991/50000)\n",
            "0 100 Loss: 0.110 | Test Acc: 98.000% (98/100)\n",
            "10 100 Loss: 0.217 | Test Acc: 95.364% (1049/1100)\n",
            "20 100 Loss: 0.235 | Test Acc: 94.667% (1988/2100)\n",
            "30 100 Loss: 0.242 | Test Acc: 94.774% (2938/3100)\n",
            "40 100 Loss: 0.237 | Test Acc: 94.756% (3885/4100)\n",
            "50 100 Loss: 0.237 | Test Acc: 94.863% (4838/5100)\n",
            "60 100 Loss: 0.229 | Test Acc: 95.016% (5796/6100)\n",
            "70 100 Loss: 0.221 | Test Acc: 95.113% (6753/7100)\n",
            "80 100 Loss: 0.222 | Test Acc: 95.037% (7698/8100)\n",
            "90 100 Loss: 0.217 | Test Acc: 95.110% (8655/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 184\n",
            "0 391 Loss: 0.000 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.002 | Train Acc: 99.929% (1407/1408)\n",
            "20 391 Loss: 0.002 | Train Acc: 99.963% (2687/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 99.975% (3967/3968)\n",
            "40 391 Loss: 0.001 | Train Acc: 99.981% (5247/5248)\n",
            "50 391 Loss: 0.001 | Train Acc: 99.985% (6527/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 99.987% (7807/7808)\n",
            "70 391 Loss: 0.001 | Train Acc: 99.989% (9087/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 99.990% (10367/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 99.991% (11647/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 99.992% (12927/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 99.986% (14206/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 99.987% (15486/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 99.988% (16766/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.989% (18046/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.990% (19326/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.985% (20605/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.986% (21885/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.987% (23165/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.988% (24445/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.988% (25725/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.985% (27004/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.982% (28283/28288)\n",
            "230 391 Loss: 0.002 | Train Acc: 99.980% (29562/29568)\n",
            "240 391 Loss: 0.002 | Train Acc: 99.977% (30841/30848)\n",
            "250 391 Loss: 0.002 | Train Acc: 99.978% (32121/32128)\n",
            "260 391 Loss: 0.002 | Train Acc: 99.979% (33401/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.980% (34681/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.981% (35961/35968)\n",
            "290 391 Loss: 0.002 | Train Acc: 99.979% (37240/37248)\n",
            "300 391 Loss: 0.002 | Train Acc: 99.979% (38520/38528)\n",
            "310 391 Loss: 0.002 | Train Acc: 99.980% (39800/39808)\n",
            "320 391 Loss: 0.002 | Train Acc: 99.981% (41080/41088)\n",
            "330 391 Loss: 0.002 | Train Acc: 99.979% (42359/42368)\n",
            "340 391 Loss: 0.002 | Train Acc: 99.979% (43639/43648)\n",
            "350 391 Loss: 0.002 | Train Acc: 99.978% (44918/44928)\n",
            "360 391 Loss: 0.002 | Train Acc: 99.978% (46198/46208)\n",
            "370 391 Loss: 0.002 | Train Acc: 99.979% (47478/47488)\n",
            "380 391 Loss: 0.002 | Train Acc: 99.977% (48757/48768)\n",
            "390 391 Loss: 0.002 | Train Acc: 99.978% (49989/50000)\n",
            "0 100 Loss: 0.134 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.215 | Test Acc: 95.273% (1048/1100)\n",
            "20 100 Loss: 0.242 | Test Acc: 94.571% (1986/2100)\n",
            "30 100 Loss: 0.246 | Test Acc: 94.548% (2931/3100)\n",
            "40 100 Loss: 0.240 | Test Acc: 94.732% (3884/4100)\n",
            "50 100 Loss: 0.240 | Test Acc: 94.804% (4835/5100)\n",
            "60 100 Loss: 0.231 | Test Acc: 94.984% (5794/6100)\n",
            "70 100 Loss: 0.222 | Test Acc: 95.099% (6752/7100)\n",
            "80 100 Loss: 0.224 | Test Acc: 94.988% (7694/8100)\n",
            "90 100 Loss: 0.219 | Test Acc: 95.055% (8650/9100)\n",
            "\n",
            "Epoch: 185\n",
            "0 391 Loss: 0.000 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.002 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.002 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 100.000% (3968/3968)\n",
            "40 391 Loss: 0.002 | Train Acc: 100.000% (5248/5248)\n",
            "50 391 Loss: 0.002 | Train Acc: 100.000% (6528/6528)\n",
            "60 391 Loss: 0.002 | Train Acc: 100.000% (7808/7808)\n",
            "70 391 Loss: 0.002 | Train Acc: 100.000% (9088/9088)\n",
            "80 391 Loss: 0.002 | Train Acc: 100.000% (10368/10368)\n",
            "90 391 Loss: 0.002 | Train Acc: 99.983% (11646/11648)\n",
            "100 391 Loss: 0.002 | Train Acc: 99.985% (12926/12928)\n",
            "110 391 Loss: 0.002 | Train Acc: 99.986% (14206/14208)\n",
            "120 391 Loss: 0.002 | Train Acc: 99.981% (15485/15488)\n",
            "130 391 Loss: 0.002 | Train Acc: 99.976% (16764/16768)\n",
            "140 391 Loss: 0.002 | Train Acc: 99.978% (18044/18048)\n",
            "150 391 Loss: 0.002 | Train Acc: 99.979% (19324/19328)\n",
            "160 391 Loss: 0.002 | Train Acc: 99.981% (20604/20608)\n",
            "170 391 Loss: 0.002 | Train Acc: 99.982% (21884/21888)\n",
            "180 391 Loss: 0.002 | Train Acc: 99.983% (23164/23168)\n",
            "190 391 Loss: 0.002 | Train Acc: 99.984% (24444/24448)\n",
            "200 391 Loss: 0.002 | Train Acc: 99.984% (25724/25728)\n",
            "210 391 Loss: 0.002 | Train Acc: 99.985% (27004/27008)\n",
            "220 391 Loss: 0.002 | Train Acc: 99.982% (28283/28288)\n",
            "230 391 Loss: 0.002 | Train Acc: 99.983% (29563/29568)\n",
            "240 391 Loss: 0.002 | Train Acc: 99.984% (30843/30848)\n",
            "250 391 Loss: 0.002 | Train Acc: 99.984% (32123/32128)\n",
            "260 391 Loss: 0.002 | Train Acc: 99.985% (33403/33408)\n",
            "270 391 Loss: 0.002 | Train Acc: 99.986% (34683/34688)\n",
            "280 391 Loss: 0.002 | Train Acc: 99.986% (35963/35968)\n",
            "290 391 Loss: 0.002 | Train Acc: 99.984% (37242/37248)\n",
            "300 391 Loss: 0.002 | Train Acc: 99.982% (38521/38528)\n",
            "310 391 Loss: 0.002 | Train Acc: 99.982% (39801/39808)\n",
            "320 391 Loss: 0.002 | Train Acc: 99.983% (41081/41088)\n",
            "330 391 Loss: 0.002 | Train Acc: 99.981% (42360/42368)\n",
            "340 391 Loss: 0.002 | Train Acc: 99.982% (43640/43648)\n",
            "350 391 Loss: 0.002 | Train Acc: 99.982% (44920/44928)\n",
            "360 391 Loss: 0.002 | Train Acc: 99.983% (46200/46208)\n",
            "370 391 Loss: 0.002 | Train Acc: 99.983% (47480/47488)\n",
            "380 391 Loss: 0.002 | Train Acc: 99.984% (48760/48768)\n",
            "390 391 Loss: 0.002 | Train Acc: 99.984% (49992/50000)\n",
            "0 100 Loss: 0.137 | Test Acc: 98.000% (98/100)\n",
            "10 100 Loss: 0.212 | Test Acc: 94.727% (1042/1100)\n",
            "20 100 Loss: 0.234 | Test Acc: 94.381% (1982/2100)\n",
            "30 100 Loss: 0.241 | Test Acc: 94.516% (2930/3100)\n",
            "40 100 Loss: 0.233 | Test Acc: 94.634% (3880/4100)\n",
            "50 100 Loss: 0.234 | Test Acc: 94.706% (4830/5100)\n",
            "60 100 Loss: 0.226 | Test Acc: 94.836% (5785/6100)\n",
            "70 100 Loss: 0.218 | Test Acc: 94.930% (6740/7100)\n",
            "80 100 Loss: 0.219 | Test Acc: 94.901% (7687/8100)\n",
            "90 100 Loss: 0.214 | Test Acc: 94.989% (8644/9100)\n",
            "\n",
            "Epoch: 186\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.004 | Train Acc: 99.929% (1407/1408)\n",
            "20 391 Loss: 0.003 | Train Acc: 99.963% (2687/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 99.975% (3967/3968)\n",
            "40 391 Loss: 0.002 | Train Acc: 99.981% (5247/5248)\n",
            "50 391 Loss: 0.002 | Train Acc: 99.985% (6527/6528)\n",
            "60 391 Loss: 0.002 | Train Acc: 99.987% (7807/7808)\n",
            "70 391 Loss: 0.002 | Train Acc: 99.989% (9087/9088)\n",
            "80 391 Loss: 0.002 | Train Acc: 99.990% (10367/10368)\n",
            "90 391 Loss: 0.002 | Train Acc: 99.983% (11646/11648)\n",
            "100 391 Loss: 0.002 | Train Acc: 99.977% (12925/12928)\n",
            "110 391 Loss: 0.002 | Train Acc: 99.979% (14205/14208)\n",
            "120 391 Loss: 0.002 | Train Acc: 99.981% (15485/15488)\n",
            "130 391 Loss: 0.002 | Train Acc: 99.982% (16765/16768)\n",
            "140 391 Loss: 0.002 | Train Acc: 99.983% (18045/18048)\n",
            "150 391 Loss: 0.002 | Train Acc: 99.984% (19325/19328)\n",
            "160 391 Loss: 0.002 | Train Acc: 99.985% (20605/20608)\n",
            "170 391 Loss: 0.002 | Train Acc: 99.982% (21884/21888)\n",
            "180 391 Loss: 0.002 | Train Acc: 99.983% (23164/23168)\n",
            "190 391 Loss: 0.002 | Train Acc: 99.984% (24444/24448)\n",
            "200 391 Loss: 0.002 | Train Acc: 99.984% (25724/25728)\n",
            "210 391 Loss: 0.002 | Train Acc: 99.978% (27002/27008)\n",
            "220 391 Loss: 0.002 | Train Acc: 99.979% (28282/28288)\n",
            "230 391 Loss: 0.002 | Train Acc: 99.980% (29562/29568)\n",
            "240 391 Loss: 0.002 | Train Acc: 99.981% (30842/30848)\n",
            "250 391 Loss: 0.002 | Train Acc: 99.981% (32122/32128)\n",
            "260 391 Loss: 0.002 | Train Acc: 99.979% (33401/33408)\n",
            "270 391 Loss: 0.002 | Train Acc: 99.980% (34681/34688)\n",
            "280 391 Loss: 0.002 | Train Acc: 99.981% (35961/35968)\n",
            "290 391 Loss: 0.002 | Train Acc: 99.979% (37240/37248)\n",
            "300 391 Loss: 0.002 | Train Acc: 99.979% (38520/38528)\n",
            "310 391 Loss: 0.002 | Train Acc: 99.980% (39800/39808)\n",
            "320 391 Loss: 0.002 | Train Acc: 99.981% (41080/41088)\n",
            "330 391 Loss: 0.002 | Train Acc: 99.981% (42360/42368)\n",
            "340 391 Loss: 0.002 | Train Acc: 99.982% (43640/43648)\n",
            "350 391 Loss: 0.002 | Train Acc: 99.980% (44919/44928)\n",
            "360 391 Loss: 0.002 | Train Acc: 99.981% (46199/46208)\n",
            "370 391 Loss: 0.002 | Train Acc: 99.981% (47479/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.982% (48759/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.982% (49991/50000)\n",
            "0 100 Loss: 0.118 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.206 | Test Acc: 95.364% (1049/1100)\n",
            "20 100 Loss: 0.229 | Test Acc: 94.619% (1987/2100)\n",
            "30 100 Loss: 0.235 | Test Acc: 94.839% (2940/3100)\n",
            "40 100 Loss: 0.231 | Test Acc: 94.878% (3890/4100)\n",
            "50 100 Loss: 0.231 | Test Acc: 94.980% (4844/5100)\n",
            "60 100 Loss: 0.223 | Test Acc: 95.082% (5800/6100)\n",
            "70 100 Loss: 0.215 | Test Acc: 95.155% (6756/7100)\n",
            "80 100 Loss: 0.217 | Test Acc: 95.099% (7703/8100)\n",
            "90 100 Loss: 0.212 | Test Acc: 95.176% (8661/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 187\n",
            "0 391 Loss: 0.002 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.003 | Train Acc: 99.858% (1406/1408)\n",
            "20 391 Loss: 0.002 | Train Acc: 99.926% (2686/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 99.950% (3966/3968)\n",
            "40 391 Loss: 0.002 | Train Acc: 99.943% (5245/5248)\n",
            "50 391 Loss: 0.002 | Train Acc: 99.954% (6525/6528)\n",
            "60 391 Loss: 0.002 | Train Acc: 99.949% (7804/7808)\n",
            "70 391 Loss: 0.002 | Train Acc: 99.956% (9084/9088)\n",
            "80 391 Loss: 0.002 | Train Acc: 99.961% (10364/10368)\n",
            "90 391 Loss: 0.002 | Train Acc: 99.957% (11643/11648)\n",
            "100 391 Loss: 0.002 | Train Acc: 99.961% (12923/12928)\n",
            "110 391 Loss: 0.002 | Train Acc: 99.965% (14203/14208)\n",
            "120 391 Loss: 0.002 | Train Acc: 99.968% (15483/15488)\n",
            "130 391 Loss: 0.002 | Train Acc: 99.964% (16762/16768)\n",
            "140 391 Loss: 0.002 | Train Acc: 99.967% (18042/18048)\n",
            "150 391 Loss: 0.002 | Train Acc: 99.964% (19321/19328)\n",
            "160 391 Loss: 0.002 | Train Acc: 99.966% (20601/20608)\n",
            "170 391 Loss: 0.002 | Train Acc: 99.968% (21881/21888)\n",
            "180 391 Loss: 0.002 | Train Acc: 99.970% (23161/23168)\n",
            "190 391 Loss: 0.002 | Train Acc: 99.971% (24441/24448)\n",
            "200 391 Loss: 0.002 | Train Acc: 99.973% (25721/25728)\n",
            "210 391 Loss: 0.002 | Train Acc: 99.974% (27001/27008)\n",
            "220 391 Loss: 0.002 | Train Acc: 99.975% (28281/28288)\n",
            "230 391 Loss: 0.002 | Train Acc: 99.976% (29561/29568)\n",
            "240 391 Loss: 0.002 | Train Acc: 99.977% (30841/30848)\n",
            "250 391 Loss: 0.002 | Train Acc: 99.978% (32121/32128)\n",
            "260 391 Loss: 0.002 | Train Acc: 99.979% (33401/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.980% (34681/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.978% (35960/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.979% (37240/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.979% (38520/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.980% (39800/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.981% (41080/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.981% (42360/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.982% (43640/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.982% (44920/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.981% (46199/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.981% (47479/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.979% (48758/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.980% (49990/50000)\n",
            "0 100 Loss: 0.144 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.204 | Test Acc: 95.091% (1046/1100)\n",
            "20 100 Loss: 0.232 | Test Acc: 94.571% (1986/2100)\n",
            "30 100 Loss: 0.237 | Test Acc: 94.710% (2936/3100)\n",
            "40 100 Loss: 0.233 | Test Acc: 94.780% (3886/4100)\n",
            "50 100 Loss: 0.233 | Test Acc: 94.804% (4835/5100)\n",
            "60 100 Loss: 0.225 | Test Acc: 94.984% (5794/6100)\n",
            "70 100 Loss: 0.215 | Test Acc: 95.099% (6752/7100)\n",
            "80 100 Loss: 0.217 | Test Acc: 95.074% (7701/8100)\n",
            "90 100 Loss: 0.212 | Test Acc: 95.198% (8663/9100)\n",
            "\n",
            "Epoch: 188\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.001 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.001 | Train Acc: 100.000% (3968/3968)\n",
            "40 391 Loss: 0.001 | Train Acc: 100.000% (5248/5248)\n",
            "50 391 Loss: 0.001 | Train Acc: 100.000% (6528/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 100.000% (7808/7808)\n",
            "70 391 Loss: 0.001 | Train Acc: 100.000% (9088/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 100.000% (10368/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 100.000% (11648/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 100.000% (12928/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 100.000% (14208/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 100.000% (15488/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 100.000% (16768/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 100.000% (18048/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.995% (19327/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.990% (20606/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.991% (21886/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.991% (23166/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.992% (24446/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.992% (25726/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.989% (27005/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.989% (28285/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.990% (29565/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.990% (30845/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.991% (32125/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.991% (33405/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.991% (34685/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.992% (35965/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.992% (37245/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.992% (38525/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.992% (39805/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.993% (41085/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.991% (42364/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.991% (43644/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.989% (44923/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.989% (46203/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.989% (47483/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.990% (48763/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.990% (49995/50000)\n",
            "0 100 Loss: 0.139 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.205 | Test Acc: 95.091% (1046/1100)\n",
            "20 100 Loss: 0.229 | Test Acc: 94.667% (1988/2100)\n",
            "30 100 Loss: 0.237 | Test Acc: 94.903% (2942/3100)\n",
            "40 100 Loss: 0.233 | Test Acc: 94.927% (3892/4100)\n",
            "50 100 Loss: 0.234 | Test Acc: 94.961% (4843/5100)\n",
            "60 100 Loss: 0.226 | Test Acc: 95.049% (5798/6100)\n",
            "70 100 Loss: 0.217 | Test Acc: 95.113% (6753/7100)\n",
            "80 100 Loss: 0.218 | Test Acc: 95.074% (7701/8100)\n",
            "90 100 Loss: 0.213 | Test Acc: 95.176% (8661/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 189\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.001 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.001 | Train Acc: 100.000% (3968/3968)\n",
            "40 391 Loss: 0.001 | Train Acc: 100.000% (5248/5248)\n",
            "50 391 Loss: 0.001 | Train Acc: 100.000% (6528/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 100.000% (7808/7808)\n",
            "70 391 Loss: 0.001 | Train Acc: 100.000% (9088/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 100.000% (10368/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 100.000% (11648/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 99.992% (12927/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 99.993% (14207/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 99.994% (15487/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 99.994% (16767/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.983% (18045/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.984% (19325/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.985% (20605/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.982% (21884/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.983% (23164/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.984% (24444/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.984% (25724/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.985% (27004/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.986% (28284/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.986% (29564/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.984% (30843/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.984% (32123/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.982% (33402/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.980% (34681/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.981% (35961/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.981% (37241/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.982% (38521/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.982% (39801/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.983% (41081/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.983% (42361/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.984% (43641/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.984% (44921/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.985% (46201/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.985% (47481/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.986% (48761/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.986% (49993/50000)\n",
            "0 100 Loss: 0.120 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.208 | Test Acc: 95.182% (1047/1100)\n",
            "20 100 Loss: 0.229 | Test Acc: 94.667% (1988/2100)\n",
            "30 100 Loss: 0.238 | Test Acc: 94.903% (2942/3100)\n",
            "40 100 Loss: 0.234 | Test Acc: 94.902% (3891/4100)\n",
            "50 100 Loss: 0.236 | Test Acc: 94.961% (4843/5100)\n",
            "60 100 Loss: 0.226 | Test Acc: 95.115% (5802/6100)\n",
            "70 100 Loss: 0.217 | Test Acc: 95.141% (6755/7100)\n",
            "80 100 Loss: 0.219 | Test Acc: 95.086% (7702/8100)\n",
            "90 100 Loss: 0.214 | Test Acc: 95.154% (8659/9100)\n",
            "\n",
            "Epoch: 190\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.002 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.002 | Train Acc: 99.963% (2687/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 99.975% (3967/3968)\n",
            "40 391 Loss: 0.002 | Train Acc: 99.981% (5247/5248)\n",
            "50 391 Loss: 0.002 | Train Acc: 99.985% (6527/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 99.987% (7807/7808)\n",
            "70 391 Loss: 0.001 | Train Acc: 99.989% (9087/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 99.990% (10367/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 99.991% (11647/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 99.992% (12927/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 99.993% (14207/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 99.994% (15487/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 99.988% (16766/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.989% (18046/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.990% (19326/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.990% (20606/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.991% (21886/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.991% (23166/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.992% (24446/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.992% (25726/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.993% (27006/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.993% (28286/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.993% (29566/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.994% (30846/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.994% (32126/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.994% (33406/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.994% (34686/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.994% (35966/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.995% (37246/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.995% (38526/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.992% (39805/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.993% (41085/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.993% (42365/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.991% (43644/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.991% (44924/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.991% (46204/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.992% (47484/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.992% (48764/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.992% (49996/50000)\n",
            "0 100 Loss: 0.138 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.208 | Test Acc: 95.000% (1045/1100)\n",
            "20 100 Loss: 0.232 | Test Acc: 94.571% (1986/2100)\n",
            "30 100 Loss: 0.240 | Test Acc: 94.774% (2938/3100)\n",
            "40 100 Loss: 0.234 | Test Acc: 94.878% (3890/4100)\n",
            "50 100 Loss: 0.236 | Test Acc: 94.902% (4840/5100)\n",
            "60 100 Loss: 0.226 | Test Acc: 95.066% (5799/6100)\n",
            "70 100 Loss: 0.217 | Test Acc: 95.141% (6755/7100)\n",
            "80 100 Loss: 0.219 | Test Acc: 95.062% (7700/8100)\n",
            "90 100 Loss: 0.214 | Test Acc: 95.154% (8659/9100)\n",
            "\n",
            "Epoch: 191\n",
            "0 391 Loss: 0.002 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.001 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.001 | Train Acc: 99.975% (3967/3968)\n",
            "40 391 Loss: 0.001 | Train Acc: 99.981% (5247/5248)\n",
            "50 391 Loss: 0.001 | Train Acc: 99.985% (6527/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 99.987% (7807/7808)\n",
            "70 391 Loss: 0.001 | Train Acc: 99.978% (9086/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 99.971% (10365/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 99.974% (11645/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 99.977% (12925/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 99.979% (14205/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 99.981% (15485/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 99.976% (16764/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.978% (18044/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.979% (19324/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.981% (20604/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.982% (21884/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.978% (23163/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.980% (24443/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.981% (25723/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.981% (27003/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.982% (28283/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.980% (29562/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.981% (30842/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.978% (32121/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.979% (33401/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.980% (34681/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.981% (35961/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.981% (37241/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.982% (38521/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.982% (39801/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.983% (41081/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.983% (42361/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.984% (43641/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.984% (44921/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.985% (46201/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.983% (47480/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.982% (48759/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.982% (49991/50000)\n",
            "0 100 Loss: 0.130 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.208 | Test Acc: 95.273% (1048/1100)\n",
            "20 100 Loss: 0.230 | Test Acc: 94.667% (1988/2100)\n",
            "30 100 Loss: 0.238 | Test Acc: 94.871% (2941/3100)\n",
            "40 100 Loss: 0.234 | Test Acc: 94.902% (3891/4100)\n",
            "50 100 Loss: 0.235 | Test Acc: 94.941% (4842/5100)\n",
            "60 100 Loss: 0.226 | Test Acc: 95.016% (5796/6100)\n",
            "70 100 Loss: 0.217 | Test Acc: 95.070% (6750/7100)\n",
            "80 100 Loss: 0.219 | Test Acc: 95.012% (7696/8100)\n",
            "90 100 Loss: 0.214 | Test Acc: 95.121% (8656/9100)\n",
            "\n",
            "Epoch: 192\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.002 | Train Acc: 99.963% (2687/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 99.975% (3967/3968)\n",
            "40 391 Loss: 0.002 | Train Acc: 99.981% (5247/5248)\n",
            "50 391 Loss: 0.001 | Train Acc: 99.985% (6527/6528)\n",
            "60 391 Loss: 0.002 | Train Acc: 99.974% (7806/7808)\n",
            "70 391 Loss: 0.002 | Train Acc: 99.967% (9085/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 99.971% (10365/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 99.974% (11645/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 99.977% (12925/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 99.979% (14205/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 99.981% (15485/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 99.982% (16765/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.983% (18045/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.984% (19325/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.981% (20604/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.982% (21884/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.983% (23164/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.984% (24444/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.984% (25724/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.985% (27004/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.986% (28284/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.986% (29564/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.987% (30844/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.988% (32124/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.988% (33404/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.986% (34683/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.981% (35961/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.981% (37241/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.982% (38521/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.982% (39801/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.983% (41081/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.983% (42361/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.984% (43641/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.984% (44921/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.985% (46201/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.985% (47481/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.986% (48761/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.986% (49993/50000)\n",
            "0 100 Loss: 0.133 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.205 | Test Acc: 95.091% (1046/1100)\n",
            "20 100 Loss: 0.230 | Test Acc: 94.667% (1988/2100)\n",
            "30 100 Loss: 0.239 | Test Acc: 94.839% (2940/3100)\n",
            "40 100 Loss: 0.234 | Test Acc: 94.902% (3891/4100)\n",
            "50 100 Loss: 0.235 | Test Acc: 94.882% (4839/5100)\n",
            "60 100 Loss: 0.227 | Test Acc: 95.016% (5796/6100)\n",
            "70 100 Loss: 0.218 | Test Acc: 95.113% (6753/7100)\n",
            "80 100 Loss: 0.219 | Test Acc: 95.074% (7701/8100)\n",
            "90 100 Loss: 0.214 | Test Acc: 95.176% (8661/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 193\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.001 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.001 | Train Acc: 100.000% (3968/3968)\n",
            "40 391 Loss: 0.001 | Train Acc: 100.000% (5248/5248)\n",
            "50 391 Loss: 0.001 | Train Acc: 100.000% (6528/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 100.000% (7808/7808)\n",
            "70 391 Loss: 0.001 | Train Acc: 100.000% (9088/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 99.990% (10367/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 99.983% (11646/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 99.985% (12926/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 99.986% (14206/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 99.987% (15486/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 99.988% (16766/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.983% (18045/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.984% (19325/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.985% (20605/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.986% (21885/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.983% (23164/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.984% (24444/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.984% (25724/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.985% (27004/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.986% (28284/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.986% (29564/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.987% (30844/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.988% (32124/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.988% (33404/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.986% (34683/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.986% (35963/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.987% (37243/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.987% (38523/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.985% (39802/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.985% (41082/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.986% (42362/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.986% (43642/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.987% (44922/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.987% (46202/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.987% (47482/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.988% (48762/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.988% (49994/50000)\n",
            "0 100 Loss: 0.129 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.203 | Test Acc: 95.273% (1048/1100)\n",
            "20 100 Loss: 0.228 | Test Acc: 94.762% (1990/2100)\n",
            "30 100 Loss: 0.236 | Test Acc: 94.935% (2943/3100)\n",
            "40 100 Loss: 0.232 | Test Acc: 94.951% (3893/4100)\n",
            "50 100 Loss: 0.234 | Test Acc: 94.980% (4844/5100)\n",
            "60 100 Loss: 0.225 | Test Acc: 95.098% (5801/6100)\n",
            "70 100 Loss: 0.216 | Test Acc: 95.141% (6755/7100)\n",
            "80 100 Loss: 0.217 | Test Acc: 95.074% (7701/8100)\n",
            "90 100 Loss: 0.213 | Test Acc: 95.176% (8661/9100)\n",
            "\n",
            "Epoch: 194\n",
            "0 391 Loss: 0.000 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.001 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.002 | Train Acc: 99.975% (3967/3968)\n",
            "40 391 Loss: 0.002 | Train Acc: 99.962% (5246/5248)\n",
            "50 391 Loss: 0.002 | Train Acc: 99.969% (6526/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 99.974% (7806/7808)\n",
            "70 391 Loss: 0.001 | Train Acc: 99.978% (9086/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 99.981% (10366/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 99.983% (11646/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 99.985% (12926/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 99.986% (14206/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 99.987% (15486/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 99.988% (16766/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.989% (18046/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.990% (19326/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.990% (20606/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.991% (21886/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.987% (23165/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.988% (24445/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.988% (25725/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.989% (27005/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.989% (28285/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.986% (29564/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.987% (30844/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.988% (32124/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.988% (33404/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.988% (34684/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.989% (35964/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.989% (37244/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.990% (38524/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.990% (39804/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.990% (41084/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.991% (42364/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.991% (43644/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.991% (44924/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.991% (46204/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.992% (47484/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.992% (48764/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.992% (49996/50000)\n",
            "0 100 Loss: 0.137 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.204 | Test Acc: 95.364% (1049/1100)\n",
            "20 100 Loss: 0.231 | Test Acc: 94.619% (1987/2100)\n",
            "30 100 Loss: 0.238 | Test Acc: 94.839% (2940/3100)\n",
            "40 100 Loss: 0.233 | Test Acc: 94.927% (3892/4100)\n",
            "50 100 Loss: 0.234 | Test Acc: 94.980% (4844/5100)\n",
            "60 100 Loss: 0.225 | Test Acc: 95.131% (5803/6100)\n",
            "70 100 Loss: 0.215 | Test Acc: 95.197% (6759/7100)\n",
            "80 100 Loss: 0.217 | Test Acc: 95.123% (7705/8100)\n",
            "90 100 Loss: 0.212 | Test Acc: 95.242% (8667/9100)\n",
            "Saving..\n",
            "\n",
            "Epoch: 195\n",
            "0 391 Loss: 0.000 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.001 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.001 | Train Acc: 100.000% (3968/3968)\n",
            "40 391 Loss: 0.001 | Train Acc: 100.000% (5248/5248)\n",
            "50 391 Loss: 0.001 | Train Acc: 100.000% (6528/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 100.000% (7808/7808)\n",
            "70 391 Loss: 0.001 | Train Acc: 100.000% (9088/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 100.000% (10368/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 100.000% (11648/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 100.000% (12928/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 100.000% (14208/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 100.000% (15488/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 100.000% (16768/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.994% (18047/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.995% (19327/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.995% (20607/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.995% (21887/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.996% (23167/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.992% (24446/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.992% (25726/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.993% (27006/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.993% (28286/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.993% (29566/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.994% (30846/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.994% (32126/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.994% (33406/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.991% (34685/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.992% (35965/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.992% (37245/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.992% (38525/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.992% (39805/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.993% (41085/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.993% (42365/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.991% (43644/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.991% (44924/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.991% (46204/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.992% (47484/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.990% (48763/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.988% (49994/50000)\n",
            "0 100 Loss: 0.131 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.202 | Test Acc: 95.091% (1046/1100)\n",
            "20 100 Loss: 0.229 | Test Acc: 94.667% (1988/2100)\n",
            "30 100 Loss: 0.237 | Test Acc: 94.903% (2942/3100)\n",
            "40 100 Loss: 0.232 | Test Acc: 94.927% (3892/4100)\n",
            "50 100 Loss: 0.234 | Test Acc: 94.980% (4844/5100)\n",
            "60 100 Loss: 0.225 | Test Acc: 95.066% (5799/6100)\n",
            "70 100 Loss: 0.216 | Test Acc: 95.141% (6755/7100)\n",
            "80 100 Loss: 0.218 | Test Acc: 95.086% (7702/8100)\n",
            "90 100 Loss: 0.213 | Test Acc: 95.187% (8662/9100)\n",
            "\n",
            "Epoch: 196\n",
            "0 391 Loss: 0.000 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.002 | Train Acc: 99.929% (1407/1408)\n",
            "20 391 Loss: 0.001 | Train Acc: 99.963% (2687/2688)\n",
            "30 391 Loss: 0.001 | Train Acc: 99.975% (3967/3968)\n",
            "40 391 Loss: 0.001 | Train Acc: 99.981% (5247/5248)\n",
            "50 391 Loss: 0.001 | Train Acc: 99.985% (6527/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 99.987% (7807/7808)\n",
            "70 391 Loss: 0.001 | Train Acc: 99.967% (9085/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 99.971% (10365/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 99.974% (11645/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 99.977% (12925/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 99.979% (14205/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 99.981% (15485/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 99.982% (16765/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.978% (18044/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.974% (19323/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.976% (20603/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.977% (21883/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.978% (23163/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.980% (24443/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.981% (25723/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.981% (27003/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.982% (28283/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.983% (29563/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.984% (30843/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.984% (32123/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.985% (33403/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.986% (34683/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.986% (35963/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.987% (37243/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.987% (38523/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.987% (39803/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.988% (41083/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.988% (42363/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.989% (43643/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.989% (44923/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.989% (46203/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.989% (47483/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.990% (48763/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.990% (49995/50000)\n",
            "0 100 Loss: 0.133 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.199 | Test Acc: 95.091% (1046/1100)\n",
            "20 100 Loss: 0.229 | Test Acc: 94.571% (1986/2100)\n",
            "30 100 Loss: 0.239 | Test Acc: 94.839% (2940/3100)\n",
            "40 100 Loss: 0.234 | Test Acc: 94.854% (3889/4100)\n",
            "50 100 Loss: 0.236 | Test Acc: 94.902% (4840/5100)\n",
            "60 100 Loss: 0.228 | Test Acc: 95.000% (5795/6100)\n",
            "70 100 Loss: 0.218 | Test Acc: 95.085% (6751/7100)\n",
            "80 100 Loss: 0.219 | Test Acc: 95.012% (7696/8100)\n",
            "90 100 Loss: 0.214 | Test Acc: 95.132% (8657/9100)\n",
            "\n",
            "Epoch: 197\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.001 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.001 | Train Acc: 100.000% (3968/3968)\n",
            "40 391 Loss: 0.001 | Train Acc: 100.000% (5248/5248)\n",
            "50 391 Loss: 0.002 | Train Acc: 99.969% (6526/6528)\n",
            "60 391 Loss: 0.002 | Train Acc: 99.974% (7806/7808)\n",
            "70 391 Loss: 0.002 | Train Acc: 99.967% (9085/9088)\n",
            "80 391 Loss: 0.002 | Train Acc: 99.961% (10364/10368)\n",
            "90 391 Loss: 0.002 | Train Acc: 99.966% (11644/11648)\n",
            "100 391 Loss: 0.002 | Train Acc: 99.969% (12924/12928)\n",
            "110 391 Loss: 0.002 | Train Acc: 99.972% (14204/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 99.974% (15484/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 99.976% (16764/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.978% (18044/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.979% (19324/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.981% (20604/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.982% (21884/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.983% (23164/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.984% (24444/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.984% (25724/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.985% (27004/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.986% (28284/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.986% (29564/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.987% (30844/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.988% (32124/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.988% (33404/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.988% (34684/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.989% (35964/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.989% (37244/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.987% (38523/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.982% (39801/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.983% (41081/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.983% (42361/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.984% (43641/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.984% (44921/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.985% (46201/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.985% (47481/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.986% (48761/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.986% (49993/50000)\n",
            "0 100 Loss: 0.135 | Test Acc: 96.000% (96/100)\n",
            "10 100 Loss: 0.205 | Test Acc: 94.909% (1044/1100)\n",
            "20 100 Loss: 0.230 | Test Acc: 94.619% (1987/2100)\n",
            "30 100 Loss: 0.237 | Test Acc: 94.742% (2937/3100)\n",
            "40 100 Loss: 0.232 | Test Acc: 94.805% (3887/4100)\n",
            "50 100 Loss: 0.234 | Test Acc: 94.863% (4838/5100)\n",
            "60 100 Loss: 0.225 | Test Acc: 95.000% (5795/6100)\n",
            "70 100 Loss: 0.216 | Test Acc: 95.042% (6748/7100)\n",
            "80 100 Loss: 0.217 | Test Acc: 94.988% (7694/8100)\n",
            "90 100 Loss: 0.212 | Test Acc: 95.099% (8654/9100)\n",
            "\n",
            "Epoch: 198\n",
            "0 391 Loss: 0.002 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.001 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.001 | Train Acc: 100.000% (3968/3968)\n",
            "40 391 Loss: 0.001 | Train Acc: 100.000% (5248/5248)\n",
            "50 391 Loss: 0.001 | Train Acc: 100.000% (6528/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 100.000% (7808/7808)\n",
            "70 391 Loss: 0.001 | Train Acc: 100.000% (9088/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 99.990% (10367/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 99.991% (11647/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 99.985% (12926/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 99.986% (14206/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 99.987% (15486/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 99.982% (16765/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.983% (18045/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.984% (19325/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.985% (20605/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.982% (21884/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.983% (23164/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.980% (24443/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.981% (25723/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.981% (27003/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.982% (28283/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.983% (29563/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.984% (30843/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.984% (32123/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.985% (33403/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.986% (34683/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.986% (35963/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.987% (37243/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.987% (38523/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.987% (39803/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.985% (41082/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.983% (42361/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.984% (43641/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.984% (44921/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.985% (46201/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.985% (47481/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.986% (48761/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.984% (49992/50000)\n",
            "0 100 Loss: 0.137 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.206 | Test Acc: 95.091% (1046/1100)\n",
            "20 100 Loss: 0.230 | Test Acc: 94.619% (1987/2100)\n",
            "30 100 Loss: 0.238 | Test Acc: 94.839% (2940/3100)\n",
            "40 100 Loss: 0.233 | Test Acc: 94.854% (3889/4100)\n",
            "50 100 Loss: 0.234 | Test Acc: 94.843% (4837/5100)\n",
            "60 100 Loss: 0.224 | Test Acc: 95.033% (5797/6100)\n",
            "70 100 Loss: 0.216 | Test Acc: 95.099% (6752/7100)\n",
            "80 100 Loss: 0.217 | Test Acc: 95.062% (7700/8100)\n",
            "90 100 Loss: 0.212 | Test Acc: 95.143% (8658/9100)\n",
            "\n",
            "Epoch: 199\n",
            "0 391 Loss: 0.001 | Train Acc: 100.000% (128/128)\n",
            "10 391 Loss: 0.001 | Train Acc: 100.000% (1408/1408)\n",
            "20 391 Loss: 0.001 | Train Acc: 100.000% (2688/2688)\n",
            "30 391 Loss: 0.001 | Train Acc: 100.000% (3968/3968)\n",
            "40 391 Loss: 0.001 | Train Acc: 100.000% (5248/5248)\n",
            "50 391 Loss: 0.001 | Train Acc: 100.000% (6528/6528)\n",
            "60 391 Loss: 0.001 | Train Acc: 100.000% (7808/7808)\n",
            "70 391 Loss: 0.001 | Train Acc: 100.000% (9088/9088)\n",
            "80 391 Loss: 0.001 | Train Acc: 100.000% (10368/10368)\n",
            "90 391 Loss: 0.001 | Train Acc: 100.000% (11648/11648)\n",
            "100 391 Loss: 0.001 | Train Acc: 99.992% (12927/12928)\n",
            "110 391 Loss: 0.001 | Train Acc: 99.993% (14207/14208)\n",
            "120 391 Loss: 0.001 | Train Acc: 99.987% (15486/15488)\n",
            "130 391 Loss: 0.001 | Train Acc: 99.982% (16765/16768)\n",
            "140 391 Loss: 0.001 | Train Acc: 99.983% (18045/18048)\n",
            "150 391 Loss: 0.001 | Train Acc: 99.984% (19325/19328)\n",
            "160 391 Loss: 0.001 | Train Acc: 99.985% (20605/20608)\n",
            "170 391 Loss: 0.001 | Train Acc: 99.986% (21885/21888)\n",
            "180 391 Loss: 0.001 | Train Acc: 99.987% (23165/23168)\n",
            "190 391 Loss: 0.001 | Train Acc: 99.988% (24445/24448)\n",
            "200 391 Loss: 0.001 | Train Acc: 99.984% (25724/25728)\n",
            "210 391 Loss: 0.001 | Train Acc: 99.985% (27004/27008)\n",
            "220 391 Loss: 0.001 | Train Acc: 99.986% (28284/28288)\n",
            "230 391 Loss: 0.001 | Train Acc: 99.986% (29564/29568)\n",
            "240 391 Loss: 0.001 | Train Acc: 99.987% (30844/30848)\n",
            "250 391 Loss: 0.001 | Train Acc: 99.988% (32124/32128)\n",
            "260 391 Loss: 0.001 | Train Acc: 99.988% (33404/33408)\n",
            "270 391 Loss: 0.001 | Train Acc: 99.986% (34683/34688)\n",
            "280 391 Loss: 0.001 | Train Acc: 99.986% (35963/35968)\n",
            "290 391 Loss: 0.001 | Train Acc: 99.987% (37243/37248)\n",
            "300 391 Loss: 0.001 | Train Acc: 99.987% (38523/38528)\n",
            "310 391 Loss: 0.001 | Train Acc: 99.987% (39803/39808)\n",
            "320 391 Loss: 0.001 | Train Acc: 99.985% (41082/41088)\n",
            "330 391 Loss: 0.001 | Train Acc: 99.986% (42362/42368)\n",
            "340 391 Loss: 0.001 | Train Acc: 99.986% (43642/43648)\n",
            "350 391 Loss: 0.001 | Train Acc: 99.987% (44922/44928)\n",
            "360 391 Loss: 0.001 | Train Acc: 99.987% (46202/46208)\n",
            "370 391 Loss: 0.001 | Train Acc: 99.987% (47482/47488)\n",
            "380 391 Loss: 0.001 | Train Acc: 99.988% (48762/48768)\n",
            "390 391 Loss: 0.001 | Train Acc: 99.988% (49994/50000)\n",
            "0 100 Loss: 0.133 | Test Acc: 97.000% (97/100)\n",
            "10 100 Loss: 0.204 | Test Acc: 95.091% (1046/1100)\n",
            "20 100 Loss: 0.227 | Test Acc: 94.524% (1985/2100)\n",
            "30 100 Loss: 0.236 | Test Acc: 94.742% (2937/3100)\n",
            "40 100 Loss: 0.231 | Test Acc: 94.829% (3888/4100)\n",
            "50 100 Loss: 0.233 | Test Acc: 94.882% (4839/5100)\n",
            "60 100 Loss: 0.224 | Test Acc: 95.033% (5797/6100)\n",
            "70 100 Loss: 0.215 | Test Acc: 95.127% (6754/7100)\n",
            "80 100 Loss: 0.216 | Test Acc: 95.049% (7699/8100)\n",
            "90 100 Loss: 0.211 | Test Acc: 95.165% (8660/9100)\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHES = 200\n",
        "for epoch in range(start_epoch, start_epoch+NUM_EPOCHES):\n",
        "    train(epoch, train_acc_hist)\n",
        "    test(epoch, test_acc_hist)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result\n",
        "\n",
        "Print out the best test result in all epochs and plot the train-test accuracy."
      ],
      "metadata": {
        "id": "_v5G6lSjVWh0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wru9C7PBLlsa",
        "outputId": "607b841f-d01b-4c99-bd8d-9c8d099a8496"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "95.25"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EfhlKFJRrzP",
        "outputId": "8ef79400-966b-40c1-e1af-7ba9588905e7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGwCAYAAABPSaTdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACK5UlEQVR4nO3dd3hUVfrA8e9MyqT3DkkIEHqvgiioFFFAxAZYcLFjWXQVF101ioKyK/hT1oJSVcSKa6cJKCBKL6FDILQQQnqblLm/P06mpZFAJpOE9/M888zMvXfunMkk3Jf3vOccnaZpGkIIIYQQTZTe2Q0QQgghhHAkCXaEEEII0aRJsCOEEEKIJk2CHSGEEEI0aRLsCCGEEKJJk2BHCCGEEE2aBDtCCCGEaNJcnd2AhsBkMnH69Gl8fX3R6XTObo4QQgghakDTNHJycoiKikKvrzp/I8EOcPr0aaKjo53dDCGEEEJchBMnTtC8efMq90uwA/j6+gLqh+Xn5+fk1gghhBCiJrKzs4mOjrZcx6siwQ5Yuq78/Pwk2BFCCCEamQuVoEiBshBCCCGaNAl2hBBCCNGkSbAjhBBCiCZNanZqobS0lOLiYmc3Q4gacXd3r3YophBCXC4k2KkBTdNISUkhMzPT2U0Rosb0ej1xcXG4u7s7uylCCOFUEuzUgDnQCQsLw8vLSyYeFA2eeaLMM2fOEBMTI7+zQojLmgQ7F1BaWmoJdIKDg53dHCFqLDQ0lNOnT1NSUoKbm5uzmyOEEE4jHfoXYK7R8fLycnJLhKgdc/dVaWmpk1sihBDOJcFODUk3gGhs5HdWCCEUCXaEEEII0aQ5Ndj57bffGDlyJFFRUeh0Or799lu7/ZqmkZCQQFRUFJ6engwaNIjExES7Y4xGI48//jghISF4e3szatQoTp48WY+fQgghhBANmVODnby8PLp27cqcOXMq3T9z5kxmzZrFnDlz2Lx5MxEREQwZMoScnBzLMZMnT2bZsmUsXbqU9evXk5uby4gRI6ROwQEGDRrE5MmTnd2MRu+FF17gwQcfvKRzPP300zzxxBN11CIhhGjitAYC0JYtW2Z5bjKZtIiICO3111+3bCssLNT8/f21999/X9M0TcvMzNTc3Ny0pUuXWo45deqUptfrtV9++aXG752VlaUBWlZWVoV9BQUF2t69e7WCgoKL+FTOAVR7mzBhwkWd9/z581p2dnadtHHDhg2aXq/Xhg0bVifnayxSUlI0X19fLSkpybLtk08+0Zo3b64FBgZqTz/9tN3xSUlJWnx8fIXfzbNnz2o+Pj7a0aNHq3yvxvi7K4S4NKWlJi2nsFhLzzXa3TLyrLfMvCLL7XyuUTuXU6idzS7QUrIKtNOZ+dqJ9DztRHqediojXzuTWaCdzSrQzmZXccuqeEup4pZnLK7zz1vd9dtWgx16npSUREpKCkOHDrVsMxgMDBw4kI0bN/LQQw+xdetWiouL7Y6JioqiU6dObNy4kWHDhlV6bqPRiNFotDzPzs523AdxgjNnzlgef/7557z44oscOHDAss3T09Pu+OLi4hoNTQ4KCqqzNs6fP5/HH3+cjz76iOTkZGJiYurs3LVV089fF+bNm0e/fv1o0aIFAGlpadx///0sXLiQli1bcuONNzJo0CBuvPFGAB555BFef/11/Pz87M4TFhbG0KFDef/993njjTfqpe1CiNrJLyrhcGouB1JyOJSay7kcIzmFaoRvVIAn4X4eGEtM5BtLSM0xkpJVSKmmEeTtjotOR1JaHqczC/AyuODv6UZJqUausQRjiQmTpqFpqtzDpKGeA0UlJud+6GpMv7kz4/s659/6BlugnJKSAkB4eLjd9vDwcMu+lJQU3N3dCQwMrPKYysyYMQN/f3/LLTo6usbt0jSN/KISp9w0TatRGyMiIiw3f39/dDqd5XlhYSEBAQF88cUXDBo0CA8PDz755BPOnz/PuHHjaN68OV5eXnTu3JnPPvvM7rzlu7FatGjB9OnTmThxIr6+vsTExDB37twLti8vL48vvviCRx55hBEjRrBw4cIKx3z33Xf06tULDw8PQkJCGDNmjGWf0WhkypQpREdHYzAYiI+PZ968eQAsXLiQgIAAu3N9++23diOTEhIS6NatG/Pnz6dly5YYDAY0TeOXX35hwIABBAQEEBwczIgRIzhy5IjduU6ePMnYsWMJCgrC29ubXr168eeff3Ls2DH0ej1btmyxO/6dd94hNjbW8t0tXbqUUaNGWfYfPXoUf39/7rjjDnr37s0111zD3r17AViyZAnu7u52n93WqFGjKnxHQgjnOJCSw3+WH2Diws3c8t5Grpr5Kx1fWs6oORt45qtdzP3tKMu2n2LVvlRW7Utl8R/H+ffyA7y9+hAfrU/iu52n+etYOluPZ7By71l+SUzhwNkccowlnM02cvBsLkfT8kjNMZJVUExOYQm5xhLyikopKC7FWGK6pEBHpwMXvQ43Fx3urnp1c9Hjqtfhoteh04G+iptL2TG2N9dKbs4cINpgMztm5YfPapp2wSG1Fzpm6tSpPPXUU5bn2dnZNQ54CopL6fDi8hodW9f2vjIML/e6+cqeffZZ3nzzTRYsWIDBYKCwsJCePXvy7LPP4ufnx48//sjdd99Ny5Yt6du3b5XnefPNN5k2bRrPPfccX331FY888ghXX3017dq1q/I1n3/+OW3btqVt27bcddddPP7447zwwguW7+zHH39kzJgxPP/883z88ccUFRXx448/Wl5/zz338Mcff/D222/TtWtXkpKSSEtLq9XnP3z4MF988QVff/01Li4ugArCnnrqKTp37kxeXh4vvvgiN998Mzt27ECv15Obm8vAgQNp1qwZ3333HREREWzbtg2TyUSLFi0YPHgwCxYsoFevXpb3WbBgAffeey86nY6MjAz27Nljtz8+Pp78/Hy2b99ObGwsmzdvZuLEiaSnp/Piiy+yZs2aKj9Dnz59OHHiBMePHyc2NrZWn18IUT1N0zh2Pp8Nh9PYdTKTvWey0TToHhNA80Avdp/MIvF0FsWlGsWlJlJzjJWeJ8THnTbhvrQJ9yUqwANfDzdKTRqnMwtIzTHi4abH292VEB8DEf4euOp1nM8rorjURIsQb6IDPSksNpFVUIyrXoePhysGVz16nQ69zhyEqHudToeHqx4vd1fcXHR218HK/rOsAS42r23KGmywExERAajsTWRkpGV7amqqJdsTERFBUVERGRkZdtmd1NRU+vfvX+W5DQYDBoPBQS1vHCZPnlwhY/D0009bHj/++OP88ssvfPnll9UGOzfccAOTJk0CVAA1e/Zs1q5dW22wM2/ePO666y4Arr/+enJzc1m9ejWDBw8G4LXXXmPs2LG8/PLLltd07doVgIMHD/LFF1+wcuVKy/EtW7aszUcHoKioiI8//pjQ0FDLtltuuaVCO8PCwti7dy+dOnViyZIlnDt3js2bN1u69Fq3bm05/v777+fhhx9m1qxZGAwGdu7cyY4dO/jmm28AOH78OJqmERUVZXlNYGAgixYt4p577qGgoIB77rmHYcOGMXHiRB5//HGSkpIYNWoUxcXFJCQkcOutt1pe26xZMwCOHTsmwY4QdcBk0jiYmsOy7af4afcZTqQXVDgm8XTlZQ9uLjoGtgljUNtQQnzcCfI20CrUm2CfhnKtadrBzIU02GAnLi6OiIgIVq5cSffu3QF1gVq3bp2lRqFnz564ubmxcuVKbr/9dkDVq+zZs4eZM2c6pF2ebi7sfaXyWiBH83RzqbNz2WYXQM2y+/rrr/P5559z6tQpS12Tt7d3tefp0qWL5bG5uyw1NbXK4w8cOMBff/1lCQBcXV254447mD9/viV42bFjBw888EClr9+xYwcuLi4MHDiwRp+zKrGxsXaBDsCRI0d44YUX2LRpE2lpaZhMKiWcnJxMp06d2LFjB927d6+ydmn06NE89thjLFu2jLFjxzJ//nyuueYaS31OQYH6h9PDw8PudTfffDM333yz5fnatWvZvXs3c+bMoXXr1nz22WdERETQp08frr76asLCwgBr7VV+fv4l/SyEuJwcPZfLqn1naR3mQ/foQJYnpjBvfRJHzuViKpf8cHPR0TM2kN4tgugY5YdJg63HMziTVUDHKH+6Rwfg46Euo7HB3vh7yrIsDZVTg53c3FwOHz5seZ6UlMSOHTsICgoiJiaGyZMnM336dOLj44mPj2f69Ol4eXkxfvx4APz9/bnvvvv4xz/+QXBwMEFBQTz99NN07tzZcuGsazqdrs66kpypfBDz5ptvMnv2bN566y06d+6Mt7c3kydPpqioqNrzlC/s1el0liChMvPmzaOkpMSSlQCVXnVzc7Nk6MoXUNuqbh+olb7Lp2vNS37YqiyIGzlyJNHR0Xz44YdERUVhMpno1KmT5Wdwofd2d3fn7rvvZsGCBYwZM4YlS5bw1ltvWfaHhIQAkJGRUSHQMjMajUyaNIlPPvmEw4cPU1JSYgns2rRpw59//snIkSMBSE9PB6jyXEJczjRNY/epLH7dn4oOHT1iA9h5IpO3Vx+mqLTqf6PcXHRc2y6M0d2aMbBtaIV/72/oHFnFK0VD5tSr9pYtW7jmmmssz811NBMmTGDhwoVMmTKFgoICJk2aREZGBn379mXFihX4+vpaXjN79mxcXV25/fbbKSgo4LrrrmPhwoWWOgxRM7///js33XSTpXvJZDJx6NAh2rdvX2fvUVJSwuLFi3nzzTftRtCB6kL69NNPeeyxx+jSpQurV6/mb3/7W4VzdO7cGZPJxLp16yoNaENDQ8nJySEvL88S0OzYseOCbTt//jz79u3jgw8+4KqrrgJg/fr1dsd06dKFjz76iPT09CqzO/fffz+dOnXi3Xffpbi42K6rsFWrVvj5+bF3717atGlT6eunTZvG8OHD6dGjB9u3b6ekpMSyr7i42G7+qD179uDm5kbHjh0v+PmEaOpMJo09p7PYeOQ8O09ksi05g7PZldfRdI8J4FyOkZMZBUT6e3DfgDhu6ByJwVWPt8EVjzrMoouGwanBzqBBg6odYaTT6UhISCAhIaHKYzw8PHjnnXd45513HNDCy0fr1q35+uuv2bhxI4GBgcyaNYuUlJQ6DXZ++OEHMjIyuO+++/D397fbd+uttzJv3jwee+wxXnrpJa677jpatWrF2LFjKSkp4eeff2bKlCm0aNGCCRMmMHHiREuB8vHjx0lNTeX222+nb9++eHl58dxzz/H444/z119/VTraq7zAwECCg4OZO3cukZGRJCcn889//tPumHHjxjF9+nRGjx7NjBkziIyMZPv27URFRdGvXz8A2rdvzxVXXMGzzz7LxIkT7bJBer2ewYMHs379ekaPHl2hDYmJiXz++eeW4Kxdu3bo9XrmzZtHREQE+/fvp3fv3pbjf//9d6666qoLZpyEaArS84rYeCQNH4MrgV7uRAV4EuLjzrbkTL7aepKVe1NIy7XPRHu5uzCwTSgGVz1bjmeg08E/hrTlpm6qbu58XhH+nm64uTTYgcmijjT+/hhRJ1544QWSkpIYNmwYXl5ePPjgg4wePZqsrKw6e4958+YxePDgCoEOqMzO9OnT2bZtG4MGDeLLL79k2rRpljlmrr76asux7733Hs899xyTJk3i/PnzxMTE8NxzzwFqLqBPPvmEZ555hrlz5zJ48GASEhIuOGOxXq9n6dKlPPHEE3Tq1Im2bdvy9ttvM2jQIMsx7u7urFixgn/84x/ccMMNlJSU0KFDB/773//aneu+++5j48aNTJw4scL7PPjgg9x3333MnDkTvd76D6ymaTz44IPMnj3bkpHy9PRk4cKFPProoxiNRubMmWPX/ffZZ5/ZFXEL0VRk5hexYu9ZOkX50yHKjzxjCePmbuLA2Ry749xd9HZdUt7uLvRvHUKv2EC6NA+ge0xAtVmakAZTPCwcTafVdPKWJiw7Oxt/f3+ysrIqTN5WWFhIUlIScXFxFQpLhajMa6+9xtKlS9m9e3eFfZqmccUVVzB58mTGjRt30e/x448/8swzz7Br1y5cXSv/P4v87orG5kR6PrNWHuTH3WcoKjHh7qLnjVs7syLxLD/vScHf041mAZ6k5xVxNqcQTQMPNz03dI5kTPfm9IkLwt1VsjSXk+qu37YksyNEHcnNzWXfvn288847TJs2rdJjdDodc+fOZdeuXZf0Xnl5eSxYsKDKQEeIxmZ5YgrPfLmT7EJVpxbi405abhFPfr4TUIXD8+/tTc9YNc2IsaSUlKxCgn0M+Bjk70BUT35DhKgjjz32GJ999hmjR4+utAvLrGvXrpZ5gy6WeaoFIRqLwuJS1h9KY9+ZbI6m5RHma6B7TADZBSWs2JvCqn1qyopu0QEkjOpI52b+zFy+nw/WHQVg2k2dLIEOgMHVhdjg6qfGEMJMgh0h6sjChQtrVAwtxOWiqMREZkERv+xJ4d01R0jJLqz2+AeuiuOZYe0sXVFTh7enf6sQ8owlMuRbXBIJdoQQQtSJrPxilvyVzNbj6ew8mcW5cksoRPh50K9VMK3DfDiZUcCOE5m4u+i4pl0YwzpG0D6yYs3FwDYyj5S4dBLsCCGEqLWcwmI2HD7PuZxCIv09OZ1VwOyVB8nIt5/EU6eDmCAv7h8Qx+29ozG4yhw2ov5JsCOEEKJKmqZxMqOAv5LS2XEikzNZhZzNLmTfmWxKyq+vAMSH+TC2Twzdov1pGeKDn6cbLvrLe10m4XwS7AghxGVuz6ksPt98Ag0Nb4MrPu6ueBtcSU7PZ82BVI6fr3z9tZYh3rQK8yElq5DC4lLuuiKWO/vG4CqT9IkGRoIdIYS4TGXmFzFr5UE+2XS8wiKYtlz1Ojo396d3iyBig70I8THQPsKPmGCv+musEJdAgh0hHOTqq6/m4YcftixcezF69+7N1KlT7dbYEqI2zmQV8OfRdDpG+dE6zAeAkxkFfLLpOJ9sOk5ekVpv7cbOkbQO8yHPWEJu2c3f042BbUK5snUI3jKXjWjE5Le3CdLpqu8fNy+0ejFatGjB5MmTmTx5co2Onz59Oi+88AKvvfZahbWmmrIffviBlJQUxo4da9n21FNPsXDhQnx8fJg5c6bdvi+++IKPP/6Y77//3u48L7zwAk8//TSjR4+2W15CiAs5eDaHD9Yd5X87Tllqa6KDPCkoKrVbQ6pdhC8vjuhA/9YhzmqqEA4nwU4TdObMGcvjzz//nBdffJEDBw5YttXnwpELFixgypQpzJ8/3+nBTlFREe7u7vXyXm+//TZ/+9vfLAHK999/z5IlS1ixYgWHDh3ib3/7G0OGDCE4OJjMzEyef/55Vq9eXeE8N954Iw888ADLly9n+PDh9dJ20Thpmsa5XCN7TmXx6aZkVu9PtexrE+7DsbR8TqQXAOCi19EzNpCHrm7Jte3CLvgfJCEaO/mvYhMUERFhufn7+6PT6ey2/fbbb/Ts2RMPDw9atmzJyy+/TElJieX1CQkJxMTEYDAYiIqK4oknngDUKvXHjx/nySefRKfTXfAfyHXr1lFQUMArr7xCXl4ev/32m91+k8nEG2+8QevWrTEYDMTExPDaa69Z9p88eZKxY8cSFBSEt7c3vXr14s8//wTg3nvvrbBy+OTJk+0W7hw0aBCPPfYYTz31FCEhIQwZMgSAWbNm0blzZ7y9vYmOjmbSpEnk5ubanWvDhg0MHDgQLy8vAgMDGTZsGBkZGSxevJjg4GCMRvv5Q2655RbuueceANLS0li1ahWjRo2y7N+3bx+DBg2iV69ejBs3Dj8/P44eVTPDTpkyhUmTJhETE1PhZ+ji4sINN9zAZ599Vu3PWlyezLMSP7dsN32nr6bPa6uZuHALq/enotPB8E4RLJvUnxVPDmTbi0NYcG9vvpnUn8SXh/HFQ/24rn24BDri4pQUQYnxwsc1EJLZqS1Ng+LKRyY4nJuXmrTiEixfvpy77rqLt99+m6uuuoojR45YVgR/6aWX+Oqrr5g9ezZLly6lY8eOpKSksHOnWpvmm2++oWvXrjz44IM88MADF3yvefPmMW7cONzc3Bg3bhzz5s2zW7186tSpfPjhh8yePZsBAwZw5swZ9u/fD6h1pgYOHEizZs347rvviIiIYNu2bZhMpqrerlKLFi3ikUceYcOGDZjXvNXr9bz99tu0aNGCpKQkJk2axJQpU3j33XcB2LFjB9dddx0TJ07k7bffxtXVlTVr1lBaWsptt93GE088wXfffcdtt90GqODmhx9+4JdffgFg/fr1eHl50b59e0s7unbtyty5c8nIyODo0aMUFBTQunVr1q9fz7Zt23jvvfeq/Ax9+vRh5syZtfrcoulIzyvi6LlcUnOMpOUaOVd2f+hsLrtOZtmt+q3XQYsQb/q1DOa+AXG0DPWx7PMxuHJNuzBnfAThaCVG0LmAiysUF0DOGTCW/QdO7wr+zdT14/Aq2P0V5KWCi3vZ9awASovA4AOGskkdTSWg06vXoEFOCuSlqW16F8g7p7ahgU8E+EWCuw+4e4N/NAS3UufOT1Ovyz+v7q9+BuIHO+VHJMFObRXnw/Qo57z3c6fVL9MlMNfOTJgwAYCWLVsybdo0pkyZwksvvURycjIREREMHjwYNzc3YmJi6NOnDwBBQUG4uLjg6+tLREREte+TnZ3N119/zcaNGwG46667uPLKK3nnnXfw8/MjJyeH//u//2POnDmWtrRq1YoBAwYAsGTJEs6dO8fmzZsJCgoCoHXr1rX+vK1bt64QKNjWG8XFxTFt2jQeeeQRS7Azc+ZMevXqZXkO0LFjR8vj8ePHs2DBAkuw8+mnn9K8eXNLVunYsWOEh4fb1dgMGzaMu+66i969e+Pp6cmiRYvw9vbmkUceYeHChbz33nu88847hISEMHfuXLv3a9asGcnJyZhMJqnbaWJ2nczkjV/2079VCPf0i8XXww1Qi1zO+fUwn/2VbFdfU5kQHwNDOoQzvFMEvVsE4ekuk/Y1CbmpsPMzSPodwtpBy0Hg1xzQ1L5zByBlJ5zYDGllZQp6NzAVV36+6vZdUjtT1K0mMpLq/v1rSIKdy8zWrVvZvHmzXXdRaWkphYWF5Ofnc9ttt/HWW2/RsmVLrr/+em644QZGjhxZ69W1lyxZQsuWLS0LXnbr1o2WLVuydOlSHnzwQfbt24fRaOS6666r9PU7duyge/fulkDnYvXq1avCtjVr1jB9+nT27t1LdnY2JSUlFBYWkpeXh7e3Nzt27LAEMpV54IEH6N27N6dOnaJZs2YsWLCAe++919IdUFBQgIeHR4XXJSQkkJCQYPfcHFS++uqr7N69mx9++IF77rmHrVu3Wo7z9PTEZDJhNBrrtd5KONa5HCMPLN7C2WwjGw6f54N1RxjcIZwWwd58v/M0h1KtXavNAjyJ9Pcg1NdAiI+6NQv0pFdsILHBXtIV1RjkpYGpFHzDrduK8uD0Dji5Wd1ObYWCTHD3gsIslWEBOLwSNr5z4fcwBzOunuDhr3oCSgqhIEPt8w6DLrdDZDf1XNPUe7m4q7YUZqnXu7ipthbnq2N8I8G7rIDdVAJeQRAQC+ggK1lleYrywJgDGcdUUKPTg1eIep1XsLqP6l4nP8qLIcFObbl5qQyLs977EplMJl5++eVKhzJ7eHgQHR3NgQMHWLlyJatWrWLSpEn8+9//Zt26dbi5udX4febPn09iYqJdkGQymZg3bx4PPvjgBS/aF9qv1+st3VJmxcUV/9fi7W2fCTt+/Dg33HADDz/8MNOmTSMoKIj169dz3333WV5/offu3r07Xbt2ZfHixQwbNozdu3fbjaIKCQkhIyOj2nPs37+fTz/9lO3btzN//nyuvvpqQkNDuf3225k4cSLZ2dn4+amUcnp6Ol5eXhLoNGKapnHkXB5rD6RSatK4snUIr/64l7PZRloEe+Gi13HkXB7fbDtleU2IjzsvjezIte3CZNh3Y5GZDPt/hGPrIfs0dLgJOt8Gf/wX/nwftFKI7guh7eD0djibqLaVV6IKyWnWCzqMgrSD6pzGHLXdw1+dI7QdRPeBqB4qQCnKA4OvNdAxM+aqriv/GNXVVZe8g+v2fA4if0G1pdNdcleSM/Xo0YMDBw5U2yXk6enJqFGjGDVqFI8++ijt2rVj9+7d9OjRA3d3d0pLK/njtLF79262bNnC2rVr7TIzmZmZXH311ezZs4f4+Hg8PT1ZvXo1999/f4VzdOnShY8++oj09PRKszuhoaHs2bPHbtuOHTsuGJBt2bKFkpIS3nzzTUuX0BdffFHhvVevXs3LL79c5Xnuv/9+Zs+ezalTpxg8eDDR0dGWfd27dyclJYWMjAwCAwMrvFbTNB588EHefPNNfHx8KC0ttQRa5nvb2qQ9e/bQo0ePaj+XaFjyjCXMXnmQpZtPoGkabq56MvMrCcbdXfhoQm/iQrxZdzCVxFPZHE3LI8THnUmDWhPoXT+jB8UlKi2GjW/D2jeg1KZo9/Q2WPWS/bEn/lQ3M98oiO4NzctuvhFQXAhuHhDYonbt8KoiE27wUbfLmAQ7l5kXX3yRESNGEB0dzW233YZer2fXrl3s3r2bV199lYULF1JaWkrfvn3x8vLi448/xtPTk9jYWEDNs/Pbb78xduxYDAYDISEV5+aYN28effr0sStGNuvXrx/z5s1j9uzZPPvss0yZMgV3d3euvPJKzp07R2JiIvfddx/jxo1j+vTpjB49mhkzZhAZGcn27duJioqiX79+XHvttfz73/9m8eLF9OvXj08++YQ9e/bQvXv1adJWrVpRUlLCO++8w8iRI9mwYQPvv/++3TFTp06lc+fOTJo0iYcffhh3d3fWrFnDbbfdZvm8d955J08//TQffvghixcvtnt99+7dCQ0NZcOGDYwYMaJCGz788EPCwsIso7WuvPJKEhIS2LRpEz///DMdOnQgICDAcvzvv//O0KFDq/1cwnnyjCWcyzGSkV9ESlYhx87n88mm45zKLLAeVFSKu4ueK1oF46rXsfFIGkUlJv59W1fLRH/Xtgvn2nbhVbyLaLDSk+Dzu+HsbvU8ui+0uV5lV/76EM7tU10+N74J4R1h3w+Qc1p1JTXvrYqHheNpQsvKytIALSsrq8K+goICbe/evVpBQYETWnbpFixYoPn7+9tt++WXX7T+/ftrnp6emp+fn9anTx9t7ty5mqZp2rJly7S+fftqfn5+mre3t3bFFVdoq1atsrz2jz/+0Lp06aIZDAatsl8fo9GoBQcHazNnzqy0PW+++aYWEhKiGY1GrbS0VHv11Ve12NhYzc3NTYuJidGmT59uOfbYsWPaLbfcovn5+WleXl5ar169tD///NOy/8UXX9TCw8M1f39/7cknn9Qee+wxbeDAgZb9AwcO1P7+979XaMOsWbO0yMhIzdPTUxs2bJi2ePFiDdAyMjIsx6xdu1br37+/ZjAYtICAAG3YsGF2+zVN0+6++24tKChIKywsrPAe//znP7WxY8dW2J6SkqLFxsZqp06dstv+8ssva0FBQVq7du3sPuPJkyc1Nzc37cSJE5X+PKvT2H9361tGnlE7mZFf5f7SUpP2+V/J2ivfJ2q/7jurHU7N0Z79aqfWauqPWuyzP1S49Z+xWluZmKIlncvV9p7O0nIKiy3nKigq0c5my/fS4OSc1bRDKzWtpLjivnOHNO3PuZr263RNW/Wyph1coWmHV2va6y007SU/db/jM00zmayvMZk0LWWPphXJd+0o1V2/bek0rVzhw2UoOzsbf39/srKyLHUSZoWFhSQlJREXF1dp0am4fA0ZMoT27dvz9ttvV9h39uxZOnbsyNatWy1ZsYvxzDPPkJWVxdy5c2v9WvndrbkT6fnc/O5G0vOM/GNoWx4Z2Aq9zUrdZ7IKeObLXaw/nFbp630Mrvh7uhHqayA22IsOkX7cdUWs1No0JnlpMHcQZJ2AkDZw7QvQfqQqXTi9HeZfr4p9KxPZDcYtVUOwRb2q7vptS/4Shail9PR0VqxYwa+//sqcOXMqPSY8PJx58+aRnJx8ScFOWFgYTz/99EW/XlxYdmEx9y3aTFquqrX49/IDbDySRt+4YFxddPx5NJ0/jp6nqMSEh5ue4Z0i2XgkjbPZRq6KD2Hy4Hh6xl7aqEFRj7Z/Cn99ACNmQ7OealtpMXxxjwp0QBUEf3E3tB4Cg1+CpXeqQCeis+p6KimCpN/USKQOo2H0e2pUk2iwJNgRopZ69OhBRkYGb7zxBm3btq3yuJtuuumS3+uZZ5655HMI2Hcmm4y8Inq1CMLdVU+esYStxzNISsvj+52nOXg2lzBfAw9c1ZL/rDjAhsPn2XD4vN05uscE8J/butIq1AeTSSO3qAQ/j5qPUBT16GwifP0A+DeHdjdA3NWqbub3N2FN2bQbWxdag53lz8HxDeDuC/d8Cwd/gQ1vqyHfh1eqY4Lj4d4fVS0OqCHZBRngGXjJk70Kx5NgR4haOnbsmLObIGrodGYBM37ez/c71XQRvh6utAn3ZXe5mYc93VyYN6E3nZv7M7BtKMu2nyIzv4g8Yykdo/y4rn0YrUJ9LPPZ6PU6CXTqW4kRsk9BUMvqjysthmUPQWqiuh1arra7uKuZgs2O/6Hus07BX2XdxLd8CM17qVunW+Gb+yFlt5pZeNxn1kAHVIBT1egn0eBIsCOEaFRMJs1ST1NSauLX/akkp+dj0jRyC0s4kVFAcno+yen5nMtRXVM6HQR5uXM+r4itx9UcSM0DPekQ6UdssBc3dWtGp2bqQtYm3Jdnr2/nnA8nqrbsYUj8Bm7+ALqOrfq4Df+nAhTPQLhiEhxcrp6XGgEdXPs8/PoqnD8Euefg6Br1uua9oa3NYrth7eD+X2H3l2oyvJB4h3484VgS7NSQ1HGLxqax/84WFpfy9baTaBrc2DmSXGMJr/64l9X7Umkd5kO36ADWHTzHmawqikbL9IkL4sURHWgf6cfW4xkcO59H7xZBtJCZh+vXmZ2wZgYMfRVCys3ztf8nOPAjDJsBHpUUmZ7cqgIdgJ+nQNzAisXAJUZVR7PuDfX8+jeg6x0wcIqaDTjjmFq/yTcc9nwDqXvhxCY48qs6vtW1Fd/X1R2633lJH1s0DBLsXIB5krr8/HyZwVY0KkVFKmXv4tI41koqKTVx7HweRSUax8/nMf3nfZxIV3PVvPx9InqdDmOJ6nran5LD/hQ1m2ywtzv9W4fgptdhcHMhJsjL7ubvZe1u6hMXRJ846XpwilUJKrDwjYCRb1m3m0zw41Nq8Ur/GBj0rKqHObQSAqIhrD38Oq3sYJ1a0uCHJ1W3kjlYXT0N/phjHS3VeohaFsFM76IWpzSLuUIFO8c2wJGyzE5lwY5oMiTYuQAXFxcCAgJITU0FwMtL/jcoGj6TycS5c+fw8vKq9bpmjlZYXEpRqYmSUo1zOUZOZuSz/nAa3+88XWHRywg/D4J93Ek8nQ1oXNEyiGeGteNMVgE7kjNpH+nHjV0i8XBrHAHdZSv3HBxdpx6f3Gy/79QWFegAbP4IBkyGxG9h2YNqJe+uY1VXk94N7vgEPr8LDv4Mu75QmZvkTfD7f9TrvUOh1XUqe1Tdv9Mx/WHLfNi5RAVPBj9rsbJokhrWv4INlHmFb3PAI0RjoNfriYmJcWpwfj7XyMYj59menEni6SyS0vJIzTFWebyXuwveBlfcXfSM6BrJE9fG421wZX9KNvlFpXSPDij7PIGM6BJVfx9EXJq931rXgErdC4XZ1u6qfd9Zj8tLha2L1KgpUK/Z8al63OMeaHu96pZa8xr89AzE9odVZcu6dL8bRr1Ts5FRMVeoe/PCl3FXq7WlRJMlwU4N6HQ6IiMjCQsLq3SxSSEaInd3d8v6X45kLCklt7CEYB+DZZumaXy19SQvf7+XXGNJla/193SjeaAnbcN9GdE1kqviQ3FzqdjmdhFVTxYmGoE9X1sfaya1unera1R31d6yYCe6r1oz6pdn1TGBcXD1M/DLVNUNdXXZfFMDnlJFx6e2wMIbIfM4uBhg0NSaDwEPiAb/aOu8Oq2uqbvPKhokCXZqwcXFpdHUPwhRF/KMJeQVlRDma52BOb+ohL2ns9l1MosNh9PYeOQ8BcWltI/045q2oZSaNHafymLjETVPTeswH65sFUyX5gG0CvOhRbAXXu6uuOh1uOilS9ipTCYozHTsEOrME5D8B6BTmZjjG1RXVqtrIGWXClZcPeHWBfBOT+uK39e/rjI5HUer4mNzG11cYcxceP8q9VqAvg/Wfo2pmCtgtznYkXqdpk6CHSEEAOl5Rfy6P5XcwmJKTBp/JaWz7uA5jCUmOkb50TcumN2nMtmenEmJqeJIr31nstl3Jtvy3M1Fx1ND2vLg1S0lqGmI8tNhye1wapsKLPo+WP3xyX/C1/ep7h7fKOh8C/SaeOH3SVym7mP7q9mGj2+wrvq973t13/o6Fax0Gw9b5qkC4zbD1D53b3WzFdwKhr8B3z2m6m0GPFXjj20R008NKw9sceG5e0SjJ8GOEJcZk0njYGoOfxw5z6kM9b/okxkFrN5/luLSyoerJ57OLisSVsJ8DXRp7k/3mECuaRtGhL8Hv+5PZXNSOj4eroT6GhjcPozWYb718plELWWdgo9vhrQD6vnPz0B+mloLKjcVIrqAT6j1+OIC+PZha7dP+lFI3gixV0Jo1bOIk3NWrfwN0OkWaNZDPT65WQ0HN3dhdSibbXzIKxDeQR17oS6p7neBwQcCYi4uM9XlDjjxl/W9RZMmC4FS84XEhGgMTCaNLcczWHcwlT+OnCcjv5g7+8Zwd79Yft6dwr+XH+BUZkGlr+0Y5UdciDc6nY4WwV4M7xRJhL8HKxJT2HUqi45RflzVOpSYYFkHqNEqLYF3r1CT6vlGQYdR8Of79scExMCD66xBxKoEWD8bfCPh5vfVxH1HfoW2N8K4JZW/T0EGLLhRzWIcEAMPrwc3L3g9BorzVcHxtsXg6gH/OACeAY781KKJqun1W4IdJNgRjVdGXhHzNyTx4+4z+Hm4EenvwbbkDM5mVxzx5O3uQl6RGhHj6eZCrxaBtI/0Q6/T4enmwpAO4XSIkt//Su1cCn5RatROQ2EyqQxL+hHVzeMZCKHt4UJF6cl/wvyhYPCHR9arQOSvD9UIJ72rWuTSmAVtroexn6kVv+cNUSOjxn6m1po6d0AFTJoJJi5X5zi3H+IGqfcvLYGFN6juKp9wmPiLtato4Qg49ru1PTe+Cb3vd9RPSTRxTWbV85ycHF544QWWLVtGamoq3bt35//+7//o3bs3oEZ9vPzyy8ydO5eMjAz69u3Lf//7Xzp27OjklgtxcUpKTTzz1S42HE7jytYhDGwTSrifB74eruQaSziXYyQpLY8DKTmsPZBqCWAAdpT1Mvh6uHJduzD6tw6hqMTEW6sOkZZrxMvdhUevac19A+Jkbpqayjiu1loy+MGUJFUg62y/vgZ/fqCCElt9HoIbZlb/2sOr1H3ra1WQAtDnAXUDOLMLPhqsFsOcNwROb1NBTYfRKtAB1XXV/W7Ytgg+GwsFmYCmuqGu/Dsc+EkFOgY/uHuZfU1M897WYKfjzdDrvkv4QQhRMw3gr7Z6999/P3v27OHjjz8mKiqKTz75hMGDB7N3716aNWvGzJkzmTVrFgsXLqRNmza8+uqrDBkyhAMHDuDrK/UComHSNI3fDqXxw87TJKfnk5ZrZFDbMJ64Lp5Xvt/Lsu2nAFi2/ZTlcVU6RPrx0MCWuLvoOZVZQGywN1e3CcHgag1mRndvxup9Z7miZTDhfh7VnE1UYK5TMWar0UPmupNLkXNWzTdzMUOezx+B3/4NaKoLKDhezReTlazqaC7EEuwMrnx/ZBcVMH3/dzW8GyB+GNw4y/64QVPVxH4FGdZtG9+BPg9au8X6PADh5f7j2eoaWD9LDS0f+basGC7qRYPuxiooKMDX15f//e9/3HjjjZbt3bp1Y8SIEUybNo2oqCgmT57Ms88+C4DRaCQ8PJw33niDhx56qNLzGo1GjEZrmj87O5vo6GjpxhIOk5pTyJ9H0zmXY8RYYmJ5Ygo7TmRWOM7TzYWC4lJc9Dr+dWN7UrIL2XIsg8z8InIKS/AxuBLia6B5oCftInzp0jyAvnFBMqu3I+35Gr4qG3U0bDr0e7T25yguVKOY9GUB6Gfj1VpQYz60X9YA1Bw0+RkQX0Uw8sOTavbf1kPUkgkubpC6T3UrefjDP5Ptj89JUbUxPf+mAot/twY0eGp/xfWlzDRNTeyXcUwFL5FdKj8u6Tc4vUMtoPnxGBVw9bwXti5Usx8/uUd1/5V3dB2EdwLv4MrPK0QNNYlurJKSEkpLS/HwsP+fqKenJ+vXrycpKYmUlBSGDh1q2WcwGBg4cCAbN26sMtiZMWMGL7/8skPbLi4fOYXFLP3rBHtOZ3EsLY8Sk0bLUB9CfNw5lVHA4XO5HD2XV+F1Hm56xvaOoXtMAHqdjtkrD3I0TR0385Yu3NKzeX1/FFGZXJuZ049vvHCw8/OzcOBnuH8V+ISpupr3roTOt8Got9Uxp7aq+/Vvqe3mYNWYC4tugqIcuOc7aDnQ/tx5abCjrCB4wGTrrL/+0eq+MEvdPPytr/n2EVVMfHwDdLsT0FSgUVWgA6o95kn8qhN3tbWO6con4KenVaADapRTZYEOVPxcQjhYgw52fH196devH9OmTaN9+/aEh4fz2Wef8eeffxIfH09KSgoA4eHhdq8LDw/n+PHjVZ536tSpPPWUdV4Gc2ZHiJJSE/tTcvA2uOKq17E/JYeDZ3NoGeLNte3DKDVp/LDzDImnswj396C0VGP+hiQy8u1n1rYdpg3q2tEh0o8WId4YXPU0D/Ti7itiCfW1zjo8tGM432w7RaiPgcEd7H+nhYOYTLDiX9C8F3QaU/kxuWetj5M3qaxHVZm0k1usXTiHVqjh0YdWqdFHB34C3gZjDuSqf7tITVSBSOvr1PMDP6lAB1TQ9PB6+xqhvz5Ui11G9VDDvs0MPuAVDPnn1SR+EWXBzuHV1lW9j66FtEPqcVVdWJei+11qxfG8c+p534fr/j2EuEgNOtgB+Pjjj5k4cSLNmjXDxcWFHj16MH78eLZt22Y5pnwKX9O0atP6BoMBg8FQ5X5xecoqKOaOD/6wrKZdnr+nG6UmrdLlD1qGenNrz+a0DPHBRa/j6Llc0nKNNA/0IjbYi27RAQR4uVf7/gZXF8b1iamTzyLKHF0Hwa2rnl339DbY9F81BLvKYOec9XF+mgoYQttUPE7T1BBtszO7oDuQslM9zzsHeeetNUBmf8yxBju7PrduP7dPLYx5RVnQkHEc/pqrHl/5RMWAyz+6LNhJhohOah6blS+pfYEtVJdUdln9lyOCHTdPuGISrH4ZorpDdJ+6fw8hLlKDD3ZatWrFunXryMvLIzs7m8jISO644w7i4uIsC3SmpKQQGWlNyaamplbI9ojLy8q9Z5m3/ig9YwO5+4oWRPjbd4VqmsbRtDx2ncwkPsyXNuG+PPzxVvan5ODhpsdVr6ewuJRWoT7Eh/uw5VgGKdmFAMQGezG4fTgZeUVk5BdxfacIbunRHFe7NZ3k98/pDq+GT8aoLMiDayo/Jqcsw5JzRg25dq0kILXN7IAqAq4s2Dm82n5IdcoudX9ml3Vb2gHIPq0eB7ZQgcmRXyFlj+ryMmdh+j2mgqA108E7BNx94H+ToCBdFSS3G1nx/QNi4MwOdU5QxcNnd6sh5vethE9uUW1y91HrUDlC/yfUfDmtrpXCY9GgNPhgx8zb2xtvb28yMjJYvnw5M2fOtAQ8K1eupHv37gAUFRWxbt063njjDSe3WDjSsbQ8gnzc8fNwq7B9xs/7WJ6oLlCbjqbzwbqjDGwTSr9Wwfh7uvHH0fNsPHzeErwABHq5kZFfjLe7C18+3J8OUX52GcJSk8bW4xnodNAzJhC9LH/QcKTsUfPDhLWz375tsbo/vQ3SkyAoruJr89PKHmiQc1oFIOWZg53IrnBmp+rK6nmv/TGlJdasTushcHglpOxWhcmp+6zHndtvrQFqMQCK8tRyCt8+DG1vUEO8m/VSQ7iTflPBydc2Q7Mju8HYJZUPfzcPIzcHO3/8V90PmKwCqZv+q4K/LndUHtTVBRfXmi0hIUQ9a/DBzvLly9E0jbZt23L48GGeeeYZ2rZty9/+9jd0Oh2TJ09m+vTpxMfHEx8fz/Tp0/Hy8mL8+PHObrq4BFkFxfx+6Bx+Hm50jQ7A39Ma1Lz+837eX3cEgAg/D+LDfWgd5sPRc3msO6i6HFz1Osb3jWH/mRz+OpbO6v2prN6favce7i562kf6si8lh4z8Ylz0Ov57Zw/LxHq2XaEueh194hy4WKKjpO5TxbJXTAK3JjjkvCAT5g9Thbr/OACuZd3TBRnqc5vt/xH6P1bx9Xk2XVRZJ1WwU1ygJs2L7KqyE+bgpMNoFewcr2R49+9vWrMoo9+DtzpDUa6qwTHZ1HOdO6C6mkB1r7UfBcfWq8AoZbfa3uV2NWrrrq9h49twZK06d+fb1FBt9ypmrw6IVfeZx1UQlZqonncdp+4ju8DThyTjIi5LDT7YycrKYurUqZw8eZKgoCBuueUWXnvtNdzc1MVvypQpFBQUMGnSJMukgitWrJA5dhohY0kpaw+c49vtp1i9P5WiEpNlX/eYAKYMa8e+M9mWQAcgJbuQlOxCfj+k/oeu08GgNqE8O7wd7SJU0LLvTDa/HTzHH0fPk1NYQp+4IK5sFUKvFoF4uLmQXVjMmv2pRPh50LdlExsKu+Jfal4Vn3DofqezW3PpSorg7B5VE6LTqcxHUa7ad26/ClBAZUtKbWaR3v+DCna2LVazId++WHUP5aVZj8kqq2dZ8S9VKzN2iZpF2BwQtRsBv05TwUTaIQiJV9uT/4R1r6vHN/5HrSkV3lHNUbP9E/v2n9uvRkuB6o4KbgX3/giLRqoMks4FOpbVDvmEwdBX1ePiwgsHq7aZnbOJKkvkHQa+EdZjJNARl6kGH+zcfvvt3H777VXu1+l0JCQkkJCQUH+NEnWqpNTEu2uPMG99ElkF1v8Ftwr1ptSkcex8PtuTMxn34SbLvinXt+XOvrEcTs3lcGoOB8/m4uXuwq09mxMbbL9CcvtIP9pH+vHQwFaVvr+fhxs3dauigLWxS92v7lN2AU0g2Pn9P2rEz8i3oecE+3qYlN3WYGfnUnXf92E1Oip5k8qg/PCUyrQc/EWNHrILdsoKh09uVvcn/oLmfdQyCehUN1jLQaqu5psH1DIJBZnwzf0qsOh8u3XOnMiuKtgx1+A0763Om7pfZV1AZXZAzUZ874/w9f1qSLbtApxmNcnK2QY7Z8qKoqO6SYAjBI0g2BFNg6ZpfLzpOEv+TKZr8wAGtg0lOtALDY1pP+xl8zE1C2u4n4GbujVjdLdmtI/0RafTcTa7kP+uOcynfyZTatKY0C+WRwa2QqfT0TM2kJ6xgU7+dA1UcQFkn1SPzyY6ty11xTz778FfVLCTUi7YATXD8Ik/QaeHAU+qQOfMDlgy1tqllH1G3dt2Y2WfUiOqzpdlDs8fttbreAWrrrKRb8MHV6n1or68VwVE+WmqC+nG/1jPZZmEr2zO1s63qWDHPORcp7evIQqJh4fWXcIPBggwz7WTqep9QNX4CCEk2BGXTtM0tiVncjg1p5JRSVBYXMpz3+zmm7JlD/an5PD5Fvvhtz4GV6aN7siors1wKVf8G+7nwSs3deJvV8Zx6GwO17UPlxmDbZ3ZCfu+h94PgK/NKLD0JOvjs3uqnx/mQgqz4OByNVGcq5OmbSgpUsXIoIIZTauY2QHY/aW6b3Wt6sJpP0IFO0U2UwrkmIMd28zOSRXcmLvFbIMdn7Kfa0A0jH4fPrujbN4c1AR9ty20n8jPnGEyazFADW/PKRuJFRBT9z9Hg69aDLQgwxoUlm+HEJcpCXZErZWaNH7afYZjaXkUFJfy26Fz7DmlJtHLNZZy3wD1P9b9Kdks/esEP+0+Q2qOERe9jkevaU2esYSNR86TkVdEflEJnZr58/qYLsQEV1F4WSYuxJu4EO9qj7msFGSqGpIt81U3SnoS3DrPuj/9iM2xGeoCX9WMttUpLYZPb4cTm1RgNey1S276RUnda63DyT+vAri0g9b9KbtVAHRwuXreYbS6bzcCfi2rfQlpo15jDnbyy9XsnD9sfZ5+1HqcT5h1e9vr1bpQv/1HrdY9OKFiN1NYBzVCzFQCLgb1vqFtrcGOuQurrgXEqO+6OF89j+rmmPcRopGRYEdc0NurD/HdztOM6dGMa9qGkfBdIn8mpVd67DfbTnLfgDiS0vIY/d8NFBarIuMQHwNvj+tG/1Yh9dn0pu3Le+GozfwxB35WXVdunuq57YUbVFfWxQQ7K19SgQ7A5nmqa8i7ht/jzqWQnw5XPHLptSOnt9k/37JA1dN4Bqo6GGM2nNqmupjAOnFeaDu1LlRBhpo48It7VBBjMlXM7Nj+zEqL1IzIYB/sAAz6Jwx4quoh3K4GCG2vRlGFd1BdYKFtrd9XcPzF/QwuJCDGWq/jFQx+TbQWTYhakmBHVOv9dUeYtVL973nmLweY+csBALzcXbixcyTeBldigrwY1DaUobN/I/F0NofO5rD4j+MUFpvo1MyPJwe3YUC8/Src4hKdTVQXTp0L3P0N/O8xVWB7eBW0L5tw7vyRiq+JH1Lz98hPV+swbSqbr8U3UgUJm96F61688OtLjPC/R1V2IyBGdSfVhDEX1s9WAYybp5pduMUAaxDjYlAZHnMRcmQ3NdnemZ2wYTZq7afO1rWfdDoY+ZZ6bD5HToqqbdFKbd43y7rfzDzMvHywAxeeqyayqwp2Isrqd0LbWvcFV14sf8nMw89B/Vyku1cIQIIdUU5JqYmNR9SEe0lpeby3Vl0wx/WJZtvxTA6czaFHTACz7+hWYdTToLZhrNp3loUbj/HNNlWf89wN7ZtuNsdUqgpBY/vXfx3L5o/UffsRaoRQh5vUjLuJy6zBTvpRdR/aXi09UFWRcmmxOjakjbo45qfDNw+W1X2UFdheOVmNKPr8TrU+U1R31X3mGwU3zan8opp5QgU6ACtfhPihNZvMbtdSNerK7M/34clEOFUWiHS5TQ3pLi4b1RTZRXVrndkJ+35Q26oK6nzLAqDcs9bZk821NoVZankJW2kquLfU7NTGlU+ogKr/E+p5qM2kh47sxjKTLiwhLCTYEQDkF5WwaONxPv7jGKezCu32PTSwJVOHt0fT1DDwmCCvCkXEAKO7R7Fq31k+/VPN4No+0o9+TW3eGlsrX1QBRtsb1JwsjvhftKap7hX/5tbzF2bDzrI1lHrfr+473qzacuAXa1eWObPTYRSsqybYWZWgXtvmepWx+foB64R04Z2g42i48kk1giisg6qd+fwu6+t7TlDrIOWkqFl7+z6k2pt5zHpM+hEVHJnXeaqOuQg5pr+qcck4ptaESt2rtvd92H7+mogu1on6zMFZVcGOd6jKhmml1vN5h4Krhwp2MsqKupv1tK5MDhcX7IS2hbGf2jy3CXZCHNSNZV79HKQ4WQgbEuxcpo6fz6O4VKNVqDcn0gt48OMtlgUwg7zd6dLcH293V3rGBvK3K1sAak6j6gqEB7cPx8fgalko874BcRceNXX+iCrkDIyt/riG5mwibHpPPT7wk5rp9sq/1/37/DFHTXI35iOV0QDVfVOcByFtocVValuznupCl3UCDq2EVtdYhzm3H6nmpkk7UPn6T0fK6kgO/qJuoC7ud30NEZ3tjx34LHw5QXUl+TdTGaGdS1Ww89PTalRYSSHc8G+1cCWAm5cqmF07XXUTaaWqoFozqeLhzrfav4d5eYXe96lz/e9R+H2Wep13mArAQtpasy4RXewLjQ3+an6cyuhd1GfLOW0dveUVorI7Z/dYj2szvFywU0k3Vm15BcFVT6vP5N/80s9XGdvMjgw7F8JCgp3LjKZpLP7jONN+2EuJSaNlqDfnc4vIKigm1NfAlGFtGdk1Cg+32tfXeLi5cH2nCL7aepIQHwMju0ZW/4K8NPhgoJr+fvIex63XU9c0DX58Wl18A2LVjLqrXlYX2Nh+dfteWxep+2O/qWBH06xdWL3vt2Z7dDr7rixz8OgVrIIDg7+qSUk7qFbENisuULP6giqaPX8I/JrDhO8qryvpOBr8V6vC13P74OObIfEb6POAtQvJHKxklgU73car2pfUvaqLyta+H1SXmPm9NM36+rD2ENRKFUibg5lmPdRnjemrgh03L/Va2yH3rQZVvnaUmW+EfbDjHWKfuXH1hLirwXbt0IvJ7FTmuhfq5jxVCW6tvhvPQPvAR4jLnP7Ch4imIruwmH98sZOXvkukxKSh18HRc3lkFRTTLTqAHx4fwG29oi8q0DF7eGBL2kX48sKI9hcuSD60Us19knvWeuFxBpMJVr9izdRcyK4v1MrXbl5q5tvOt6nA56enq35NiREW36SCoppKO6SCD7DOmZOToi7yOhfoOtb+eHOGZN/3cGyDehzUSgUH4R3V8/JdWSl7rBmTRzaoZRQeXFN9AW3zXqr4N26gqoEpyIDP78bShWQe0WTO7AS1hHFL1dIHQ6bBsOlw/RsQfYWa5G/Fv6znzj6tgjK9qwq+3DzsF5aMUgv+EjtA3Ud2U9kaD39rcW7rCxRhm0ek2QY7/jajloJbVexmqqtgx9HcPODxrfDgWilOFsKGZHYuA5qm8e2OU7z2437SctV8N1OHt+OO3tH8uj+V87lFjO8bc0lBjlnrMF9+mXx1zQ4+tNz6OPkPaN7zkt//ohxfrxZyRAfd7waDT9XHapq1ePbqp9Ukc8Nnwu6vVDdITor9WkRmJ/6Eo2tVke3gl2rWrv0/Wh9nHFP35kAiMBY8/OyPj+oOsVfC8Q2wtmytJnPQEt5RBWintkDXO6yvMY8+iuqmiqw73FSztoEKMjrfChvfsQZloEZsFWZbMzsBsaq9/R+3f32ra+Ddfqob8Mga9fxcWVYnuLU109f7PjU6y1QMUT3Uts63qjqdVtdYzzfsNTi0omK3WHnm7yevbIFP71D7WpfgVqrLyStYvYfeDTwCavxjcTrz1ANCCAvJ7DRxpzILuGf+Xzz5+U7Sco20DPFmyf19uf+qlviWrQk1cUBcnQQ6tVJaDIdXW5+b53GpK+ePqC6yvf+78LGWYlfNfpK6yqTuVce4GNSMxaAujOZiUPM0/eWZMyrGbJVJqgnbYCfrpMoOmYOdqkbzDHiy7H3Mi02WBTtxZbU9m+epNaLMzuxQ9xdb39HFJrsU1V1liEAFP+bMTlX1WKFtVfcXwC9T1eg2cxeWbTGvbwTcMFMFoubgRu8C/Sapri6z9iNh1DsXvtiXD0a9Q+3nozH/bM33PmGgl38qhWjM5C+4iSopNfHxH8cYNvs3fj+UhsFVzzPD2vLz5KsaxsreyZvUhZ+yVHty2fT/dWX3V+pC/sNT1oUXbZUUqfvCLPuAyFy/UpXEZeq+9WD7zEpcWTbLPHR53/fw8RjIKVtuwNJ9pNkvW2CWd97+8+ectS5I6eKuXpd5whrsBFXRzdR6sKrRMTMf136UWqhSK4UvJqhzAZzeoe7N3UO1FdHJGihdOVkNXweVMSoom3gyoIpgB1TBs4e/yugcXWNTr9PB/rheE9UQdxe3i2unLd9yEyt6BZfrxjIHO2VdWXVRnCyEcCoJdpqgNftTuf7/fueF/yWSayyhZ2wgP/39Kh69prXjJ/YrKYLtn9rPTFsZcxdWh5vUxTwv1TovTHn56ZB7rvJ9VTGfKz9NZTNs/fEuzGgGf34Ae75Ro2PMzBfbymiaNdjpeLP9vpYD1X3SOpWB+eEpOLIadpRljczDnEEFWLY2fwT/aW1fu3LwZ0BTQYg5gMhIsg4nr6qmRqdTQYeZ+TidDkb+nxpdlZ8GX01UQaA5uLuUOVnGLoG7vlHFy+ZaF3PWzjOwYnebLa8gVfMEsOtL68/JNmNT1yrL7PhGYQm8zcFOiDmz00jqdYQQVZJgpwnJM5Yw5aud/G3hZg6n5hLo5cYrN3Xki4f60Sq0mjqUupS4DP43CZaOrz5Tc3CFuu8wyppVOPFnxeNKS1R31Hv91FpQNWUbOG182z67s3WhWgrg5ylqbSmwXuCqy+yc3aMyKy4GtT6SrZh+qrYj64RaM8lcD5K8SXVb2QZRtp8j8duykV0m+6HO+8sWmWx3IwS2KPtMSRfuxgIViDXvrTITtt1B7l5wx6fg7gMn/4LV06zFyb4XGDlXHf9mapZjsAZm5gxXdVkds863q/t930Nq2c+/fGanLpVfMsM7RNUHtR2uMmHmzFinW1WmzNzVJoRotCTYaQLyi0r4fHMyN7z9O19sOYlOB/cPiGPdlGu4p1+LSicAdBhzUeqJP61ztpSXccw6oqjVdRDdV21PrqRuJ/0oZCVD3jnY9XnN22FeBNPNW712y3zr+czzs4AqQNW5wDXPq+fVBTvmrE78ELXCtC13bxVggP3svyf+VO9pXpgRrJmdE3/BNw9gGcWUZ5O9Mnd7xQ20BjvnD1snvasu2HFxhYnL4bHNFWd2Doy1Zn7+LBt9FtW97kbumDM75tmNazJ/UnQfFRQV50FJgQomg+Lqpj2VqSyzAypD9dgWFRSCKj6/62vrGltCiEZLgp1G7n87TtH3tdU8+/Vujp/PJ8rfg88euIJ/jeiAn0cd1DfUlmUmW9RwblNpxWPMk9hF9wXPAJUVgcqDnXM2GZEtC2pW11OQaW2HeV6TDW9DcaE1o9TiKmsxb4ebVFABkJms1mY6tQ0Wj1ZBkqlUDQM3z1pcvgvLzNyVpZnUXC1uXiqwSfzG/jhzsPPXhyrDZJ64z9z1p2nWzJBvhPXCn7ROLb/g6nHhBR71LlUHMP0mgY/NBb8ulxUoP2S7Jpkdnc7alQUQ2ka131E8AtTPUL05eAZZ2yGFyEI0SfKX3YjtOJHJM1/uIsdYQkyQF/8c3o6fJ1/NFfVRgJx7TtWmlJdvsxp66l5VKFzeib/UfWx/dW/O7KQdsH89wDmbLMy5fZUHROWZsx/eYWriPf9oFTzsXGLNNrUdDoMT4JE/4Kb/gnew9X/4aQdUoHZ0DfzwJMzpBe9eAdkn1Wy7ba6v9G0tRcqg5sAxZ3q2LrQ/zhzsmAOaruPLtmeqUWrGbBUEgXq/wLJgx5x1Cmp1aRdld2+45jnr87qcadc/2iaQoOYzY3e53frYkV1YoIIac7edZ2D1ExAKIZoECXYaqfO5RiZ9spWiUhPDOoaz9ulBPDywFf6e9ZDNyToJszvAp5XMZ2IegRNaVmBq26VjdrIs2Ikum9LfO9ha63HkV/tjzbUurmXDibcuuHD7zPU6QS3V6J1+j6rn62dbh12bA5bwDtZuC3N9y7EN1iHkHv7qfKYSiB8G962oeh6eZr1UlkDnAlc8Ys1YZZ+yP84c7JizT8Gt1LpT5m3mDI+7j2pb+S6dulgxu9udKsj0CoGYKy79fGZ6F/sutoAWNXtdaFvr6uCOLE42Mwc75gBXCNGkSbDTCJlMGpM/38HprEJahnjz79u6oq/PupwTf6rMw/E/VCbCljkz0/chdZ92SBUZ2+43F9maMx+ghkYD7P7S/nzmzI553anEbytmf8o7XxbsmIOCHveo/8FnJquJ6YJbVx4wmC+yG99RhbuR3eCJHSoDdPcyuPOL6gMNV3f4289w30p18S4fRJgzNJZgJ0Pde4dYu1Ly0iDXZrI7UNkSnU23Tl2smO3iqmZ/fmqfGhFVl2y7smqzZMGI2dB1HPSYULftqYyfOdgJcfx7CSGcToKdRmjxH8f4/VAaHm563rurZ/3X5phHzJiKrUOhzSyZnbZlF2ib+hOwzh0THG9/ke1SNqvv4VXWzEZpiXVm3m7j1f/8S42wbVH17bNkdsqCC3dv6wSAUHU3VGhbdW9ub6dbVBsHPAmtrq3+Pc3C2llngm7ey5qxATW7MViDHfPPyjPIGtjkp1kLlc3bXNzsF46si2DHfF5HrEcWfJHBTvNecPP7dR98VcZXgh0hLicS7DQyR8/l8vovKth47ob2tI3wvcArHMC2aNh2/hiwZl28Q63zk+Scse43Dy83d2GZhbZRo4JMJWruG1C1N6VFakSVf7TqGgI1P455UsDK2HZjmfV9yNoV1mZY5a8LLdd9UlUhck0ZfK3Fx14h1nlbCrPUApzmEVpeQdaLbp5NsGM7mZ1tV1ZdBTuOYu6S9IlQazU1RM17qXtZGVyIy4IEO41ISamJf3y5k8JiEwNah3BX3xoWf1Zn/Wz48FrVxVNTdkXDNkO1zcW1oLIV5pWozbMIg7U42bYLy8yc3TEPMbcsHdBGFeR2ukUFUDlnYO+3VbevsmDHOwTu+BiGzVAjsSpjWyvSvI8aenypzHU74R1V/Q+oYMccFOpdweCnZvEF+2DHNutgHn4OdVOz40hxV6ni8Nqss1XfOt6suvDMI/KEEE2aBDuNyNurD7E9ORNfD1dm3trl0ut0Skvg99lqMrvvnqjZsO4So33Xld1keWU1KOjUkHJzV4E5s1NaooZ0Q8XMDqhgRueiFqtMO2wNpMwZF1eDdYK3je9U3t7CbGs3lG2wA2p+nH6Tqh6S7WXTndRpTOXH1FaPe9ToqZ73lgt2yoqTPYNUe6rrxgJrvY+HvzUwaqh8I+AfB9R6Vg2ZX5SsDC7EZUKCnUbijyPneWeNKux97ebORAXUwcrGZ3ZaF4w8ugZ2LLHu0zT4bDwsGmk/V875w6p418w22DFnKzz81agc8+RtOSllx+5VE8e5+9rP7GvmE2atjflrrk2w09Z6TK/7VHdUyi77BS3NzMPOvUKswUVtDHgSWg5SQ8frQnhHeGKbCp5sgx1zvY45cLF0Y52zCXZsurHMP4PQ9o3jAi3z1QghGhD5F6kRSM8rYvLn29E0uL1Xc0Z1jbrwi2oiaa26dy8bSr18qjUwKcqDAz+qIdi2AY05ADEXnqYftc63Y7mAlxWYmjM7uWXnNA85b96r6knjzMPEN39oXXLAtnvJKwi6lnV3mWc0tlVZF1Zt9HsU7vmfGr1V1zwC1L1tN5b5Z2XbjZVbSTdW6yGqC+7GN+u+XUII0cRJsNMIvPrDXs5mG2kV6k3CqI4Xf6KiPPhzrrUb6uhadX/tvyCyq7oIm5dVKLAZ3n16u/WxeSRWy0EqU6GVzS4M1gu4eRi1pUC5LNgxd2FVVq9j1uoa1Z2lmVSXDthndsC6dlHuWSowf7aLDXYcqbJuLHOwY+6yyquiG8vFVXXBRdisaC6EEKJGJNhp4DYdPc8320+h08Gbt3fDy/0iZ3s15sKnt8HPz8DHo9UFN7lsZFSr66D9SPXYPAGe7Vw2p7dZH5tHYoW2t9bSmDM/lgt4WZaifM1O2kF1f6FJ44ZNV0W7oJZc8C83fNmc8bBdmsIs3bx2VAMs4jUHO8Zs+5odsPlMVYzGEkIIcdEk2GnAikpMvPDtHgDG94mhW3TAxZ3ImAOf3ALHN6jnmcnw7SQ1Z41vlJoEzhygmIOcqjI75pFYYe2sQYs5AKrQjWWu2TmraoDMGaDy6yeV5xsB172oHkd1r1j/YWlrJcGOucus/MrWDYE5gENTi6GCTTdWWbCTk6KWjQCZ3VcIIeqILArTgC3YkMSh1FyCvd2ZMqySgt6a+v1NOLFJZRa63KGKf/f/oPa1HKgKXssHO7aZnZQ91rocczdRaDsIKwtezF1b5buxzMFO3jnV5WS+iAfVIOvS+36VGQqvpNvOy2ZOmvKKylbbLr8qeUPg5qHWjSoptGagLAXKZYFNUa6617taa3yEEEJcEgl2GqjiUhMfrVcXxGeHt8Pf6xJmST6zU90PToDud8Ph1ZBeFrSYV/suny2xDCNHzZR8NlEN/dZKweCvAhFzZsc8saAls1NW3OsVooaSa6XWrJJ/tHUtqurodNB+ROX7zF0+BRlqpJhtsbOxLFhw977weziDhz/kFlpHjZkDQ89ANduyZlLPvUJkRJMQQtQR+de0gfp1fyrncowEe7szuluzSztZxnF1Hxyvlgi47gXrvpZlwY75oltZsAOqK8s8EiusnQpGzDU7GcegKN+61pP5XHq9NbuT9HtZG+pg9l/z+dEqrpNlzoy4N8DMDlizNebianOQqdfbfC6kC0sIIeqQZHYaqKV/qRmNb+3ZHHfXS4hJTaXW2ZEDy2Zc7jAaBjwFHn7W2hbzRdecLTEHEeZsw+lt1gUqzV1LPqEqA5GfpgKh8jU7oEZkZZ+yzolzoXqdmnBxVZmQggz13j42gYG5G6shZ3ZseZULcMwj0Hwk2BFCiLoiwU4DdDqzgHUH1YicO3pf4pIFOWdUN5TeFfzKMkQ6HQx+yf44L5tsie2kd817q/Ws9ixTEwLq3eCKSdbXhXeEpHWqK6v8aCywjsgyL+gZXAfBjvk9CjIqFimbMzsGn7p5n7pWIdix+Vl5h8A582MJdoQQoq5IN1YD9MWWE5g06BsXRMvQS7xom0f9BMRUPZEfqO4tQ9mFOP+8NbPT6jp1X1yWMen/uH12xpzlOZtYsUAZrN1YZiF10I0FlRcpm0qti2u6N5Jgx3byQrvAR4IdIYSoKw062CkpKeFf//oXcXFxeHp60rJlS1555RVMJpPlGE3TSEhIICoqCk9PTwYNGkRiYqITW31pNE3jyy0nARjXJ+YCR9eAuV4noAaLhpoLi/PPW2t2wjtaly3waw5XP23/GnOwk7Lb+hqvaoKdusrs2M5LY2buwoLG0Y2l09uPuLINcCTYEUKIOtOgg5033niD999/nzlz5rBv3z5mzpzJv//9b9555x3LMTNnzmTWrFnMmTOHzZs3ExERwZAhQ8jJyXFiyy/e8fP5nMoswN1Fz/WdIi78gvJMpbBzqbVGxpzZsV01uyq2I7Js62/aDFUX5htmVgwizMHOqa3WNbOqyuy4elq70i6VZXkFm24sc7Cjc1FDvBsi22DHM9B+xJXt8hAS7AghRJ1p0DU7f/zxBzfddBM33ngjAC1atOCzzz5jy5YtgMqCvPXWWzz//POMGaNWqV60aBHh4eEsWbKEhx56yGltv1hbj6vsSOfm/ni4VdPtVJnzR+DbR1SNjcEfphy5yGAn3b5L6sbZcM3zlU/UF9pOBULm7iM3LzWfjJm5ZgfUSKy6Gk5dXWbH3afhLpZpF+wE2e+z7caS2ZOFEKLONOjMzoABA1i9ejUHD6plBnbu3Mn69eu54YYbAEhKSiIlJYWhQ4daXmMwGBg4cCAbN26s8rxGo5Hs7Gy7W0OxNVkFOz1ja7kQZcpueO9KFeiAWs08ZRdklnVjBdakG8ucLUlVRcqgMjuu7lXPSOzmaT+cvPwF3Lw+FtRdvY5tW20LlIvKsnkNtTgZ7IMd2+AGynVjhSCEEKJuNOjMzrPPPktWVhbt2rXDxcWF0tJSXnvtNcaNGwdASopaGiA8PNzudeHh4Rw/frzK886YMYOXX37ZcQ2/BNvKMjs9YgJq98It86GkAJr1UiOvTmyC4xsvLrNz/iigqcc1Wf07vKN13SuvcsGOXWanjup1oPIC5YY+7BzKBTvlflbSjSWEEA7RoDM7n3/+OZ988glLlixh27ZtLFq0iP/85z8sWrTI7jhduS4LTdMqbLM1depUsrKyLLcTJ044pP21lVNYzIGzKjvRI6YWmR2TCQ78rB4P+ie0U5kvDq+2Tl5XkwJlc2Bz/rC6d/dVo7QuJMxmSYfyF3CvYBV8Qd3MsWPmXUlmp6HPngzVBzteEuwIIYQjNOjMzjPPPMM///lPxo4dC0Dnzp05fvw4M2bMYMKECUREqOLXlJQUIiOtGYTU1NQK2R5bBoMBg8Hg2MZfhJ0nstA0aB7oSZhfLQpsz2xX8+m4+0Dc1dYRPknr1L3Bv2YZGktmpyzY8aphwGW7flX5biy9HgLj1Dw74Z1qdr6aqDSzYw52GnI3VoD1cfmfVWALtcK7X5RamkMIIUSdaNCZnfz8fPTlClpdXFwsQ8/j4uKIiIhg5cqVlv1FRUWsW7eO/v3712tb64K5OLnW9Tr7f1L3ra9TF8nIrqpQ2LzOUmBszQp2LXUwZQFE+YtxVcKryewA3LYAbl8M4R1qdr6asBQon1crqoN9gXJDVV1mx80DHt8Kf/u5ftskhBBNXIPO7IwcOZLXXnuNmJgYOnbsyPbt25k1axYTJ04EVPfV5MmTmT59OvHx8cTHxzN9+nS8vLwYP368k1tfe7UqTj6zS2Vz4ofCgbJgp60atYarOzTvBUm/qec1KU6GigWzlQUulQmIUV1eRTmVB0gRndWtLpnbaioGY7YKIooaWzdWcMX9ru711xYhhLhMNOhg55133uGFF15g0qRJpKamEhUVxUMPPcSLL75oOWbKlCkUFBQwadIkMjIy6Nu3LytWrMDXt4EuBFkFk0lje7K5OPkCwY6mwae3QW4KtBuhlmrQuUD8EOsxsVfaBDstataI8sFNTTM7Op3K7pzYVPMA6VK5eYKbt5rZOS+tLNgpy+w06NFYftbHNf35CiGEuCQNOtjx9fXlrbfe4q233qryGJ1OR0JCAgkJCfXWLkc4fC6XnMISPN1caBdxgUAtN1UFOgD7f1D3sf3tA42YftbHNSlOhoqZhprU+Zj1vk8FG62HXPjYuuIdDJl5qisruBUYy4aeN+RuLFeDmlyxpKDyzI4QQog616Brdi4nu06qeW06N/fH1eUCX4t5mLdnoDU70OEm+2Oa97aOggqMq1kjygc3tcnSdLkdHllft3PpXEj5IuXGULMD6mekd4WgGn4vQgghLkmDzuxcTvaeVhMbdozyu8CRQNoBdd+8D4yYDcd+h0632B/j7gW97lNz7UT3qVkjXNxUd5B5QsGG3s1iW6QMjWOeHYC7lqk2l183TAghhENIsNNAJJ5WAUaHSJtgJ+esmgW59WD70VRph9R9aBvwbwZdx1Z+0htm1r4hXsH2syc3ZOVHjzWGAmUAn1B1E0IIUS+kG6sB0DSNvWfMmR2b0To/PwOf3gq7v7J/gbkbK6RN3TfGNpvT0DM7luUtygU7hsZVnC6EEMKxJNhpAE5mFJBTWIKbi47WYTb1JmcT1f2OT+xfcM6BwY5t0WxtCpSdoXw3VmOYQVkIIUS9k2CnAUgsq9dpE+6Lu2vZV6JpkHVSPT66DrLPqMfGXMgu2+7oYKemMyg7S2MtUBZCCFGvJNhpAPZWVq+Tfx5KCsueaLCnrCvLspRDiGNqarwaUTdWlQXKEuwIIYSwkmCnAbDW69gEO1nlFifd9YW6d2S9DliDHZ2L/Wy/DZEls3NO3ReZ59mRbiwhhBBWEuw0AOZurA62xcnmLqyglqB3U6OyUvdZg51QRwU7Zd1YnoE1W0/LmcxDt3POqJXfG8MMykIIIeqdBDtOlp5XxJks1V3VPtJmFJE52InobF0G4q8P4VzZHDsOy+zYBDsNnW8EoANTiQp4SovUdsnsCCGEsCHBjpOZJxOMDfbC18PNusMc7PhHQ+/71eMt8+DIGvXYUcFO9BUqm1R+ksKGyMUNvMvmqzl/yLpdanaEEELYkGDHyfaeUcXJFWZONtfs+DeH1tfBFZPUc3NdiqOCHZ9QeGI7XDPVMeeva36R6t480aKLQQVBQgghRBkJdpzsZEYBAHEh5bpeLJmd5up+8MsQ1UM9dvVQGR8BvlHq3jxKTbqwhBBClCPBjpOl56k6kyBvg/0O224sAFd3uG0BBMdDt/Ggl68OsMnslBVuS3GyEEKIcmRtLCczBzvB3u7WjSVGyD2rHttmcAJbwONb6q9xjYE5s5NmzuxIsCOEEMKepAeczBzsBNoGO9mn1L2rZ8NfjNPZzJmdrGR1L8GOEEKIciTYcbJKMzu29ToNfa4bZ/ONtH8uNTtCCCHKkWDHiTRNIyPfXLNTRbAjqucXZf9cgh0hhBDlSLDjRDnGEopLNUCCnYtWPrNj8K38OCGEEJctCXacKD1XZXW83F3wcHOx7rDMsSPDyy/Iwx/cvKzPJbMjhBCiHAl2nOh8XiVdWCCZndrQ6eyzOxLsCCGEKEeCHSfKyCsiTneGmaX/htT91h0S7NSObd2Ou3RjCSGEsCfBjhOl5xUx0eVn+hdthDWvqo2mUgl2asu8+jlIZkcIIUQFEuw40fm8ItroywKbw6uhuACO/Q7F+WrV8YBY5zawsbDtxpIZlIUQQpQjMyg7UUaekba6smLk4nw4uhYO/KSed7gJXOTrqRG7bizJ7AghhLAnV1MnKso8Q4Auz7ohcRkcWqEed7rFOY1qjOwKlCWzI4QQwp4EO07kk30IAJPOFb1WAru+ADTwCYfYK53buMbELrMjwY4QQgh7UrPjRIG5avHKtMirwSMAUBMM0mE06F2qepkoT4aeCyGEqIYEO04UXpgEQElYJ2hzvXWHdGHVjm8EULaGmGR2hBBClCPBjhNFlxwHwCWiI7QfqTb6x0Dz3k5sVSPk4gbd71Jdf4EtnN0aIYQQDYzU7DiJsbiEVqiRWF5RnSC6I4x4C6K6g15i0Fq7aY6zWyCEEKKBkmDHSbJSjhKmK6RIc8Enqo1a9qDX35zdLCGEEKLJkRSCkxSc2gNAsq4ZOlf3CxwthBBCiIt1ScFOYWFhXbXjsmNK2QfACbcWzm2IEEII0cTVOtgxmUxMmzaNZs2a4ePjw9GjRwF44YUXmDdvXp03sKlyPa8W/jznEefklgghhBBNW62DnVdffZWFCxcyc+ZM3N2t3S+dO3fmo48+qtPGNWUeWSpIzPKRYEcIIYRwpFoHO4sXL2bu3LnceeeduLhYJ77r0qUL+/fvr9PGAbRo0QKdTlfh9uijjwKgaRoJCQlERUXh6enJoEGDSExMrPN21DV9sVomwsUryMktEUIIIZq2Wgc7p06donXr1hW2m0wmiouL66RRtjZv3syZM2cst5UrVwJw2223ATBz5kxmzZrFnDlz2Lx5MxEREQwZMoScnJw6b0td0pWqeicvL5nxVwghhHCkWgc7HTt25Pfff6+w/csvv6R79+510ihboaGhREREWG4//PADrVq1YuDAgWiaxltvvcXzzz/PmDFj6NSpE4sWLSI/P58lS5ZUeU6j0Uh2drbdrb65lBoB8PaRGX+FEEIIR6r1PDsvvfQSd999N6dOncJkMvHNN99w4MABFi9ezA8//OCINloUFRXxySef8NRTT6HT6Th69CgpKSkMHTrUcozBYGDgwIFs3LiRhx56qNLzzJgxg5dfftmhbb0QV1NZsOPt69R2CCGEEE1drTM7I0eO5PPPP+enn35Cp9Px4osvsm/fPr7//nuGDBniiDZafPvtt2RmZnLvvfcCkJKSAkB4eLjdceHh4ZZ9lZk6dSpZWVmW24kTJxzW5qq4a+ZgRzI7QgghhCNd1AzKw4YNY9iwYXXdlguaN28ew4cPJyoqym67Tqeze65pWoVttgwGAwaDwSFtrBFTKW6UqLZ4ejmvHUIIIcRloNHMoHz8+HFWrVrF/fffb9kWEREBUCGLk5qaWiHb06CUWCdjdPeQAmUhhBDCkWod7Oj1elxcXKq8OcqCBQsICwvjxhtvtGyLi4sjIiLCMkILVF3PunXr6N+/v8PacsmKrcGOh6cEO0IIIYQj1boba9myZXbPi4uL2b59O4sWLXJY0a/JZGLBggVMmDABV1drk3U6HZMnT2b69OnEx8cTHx/P9OnT8fLyYvz48Q5pS50oKQCgSHPB00PWxRJCCCEcqdbBzk033VRh26233krHjh35/PPPue++++qkYbZWrVpFcnIyEydOrLBvypQpFBQUMGnSJDIyMujbty8rVqzA17fhjnIqMebjChTijqeb47JhQgghhACdpmlaXZzoyJEjdOnShby8vLo4Xb3Kzs7G39+frKws/Pz8HP5+ece3471gEOc0f3z/lYSHBDxCCCFErdX0+l0nBcoFBQW88847NG/evC5O1+QZC1VAWIg7BtdGUyMuhBBCNEq17sYKDAy0G9ataRo5OTl4eXnxySef1GnjmqriwrKaHdyrHSIvhBBCiEtX62Bn9uzZdhdovV5PaGgoffv2JTAwsE4b11QVlWV2inROnOtHCCGEuEzUOtgxz14sLl6xMV/d62UklhBCCOFoNQp2du3aVeMTdunS5aIbc7koMapurBLJ7AghhBAOV6Ngp1u3buh0Oi40cEun01FaWlonDWvKSssyOyV6CXaEEEIIR6tRsJOUlOTodlxWSotVZqfURYIdIYQQwtFqFOzExsY6uh2XFZPRHOx4OLklQgghRNN3UaueA+zdu5fk5GSKiorsto8aNeqSG9XUaWWZHU2CHSGEEMLhah3sHD16lJtvvpndu3fb1fGYh6NLzc6FmcqCHZOrBDtCCCGEo9V6+t6///3vxMXFcfbsWby8vEhMTOS3336jV69erF271gFNbILMmR0JdoQQQgiHq3Vm548//uDXX38lNDQUvV6PXq9nwIABzJgxgyeeeILt27c7op1NS0mhupdgRwghhHC4Wmd2SktL8fHxASAkJITTp08Dqoj5wIEDddu6JkpfFuzo3Dyd3BIhhBCi6at1ZqdTp07s2rWLli1b0rdvX2bOnIm7uztz586lZcuWjmhjk6MrLQt23CXYEUIIIRyt1sHOv/71L/Ly1NpOr776KiNGjOCqq64iODiYzz//vM4b2BTpy4IdvWR2hBBCCIercbDTrVs37r//fu68807Lgp8tW7Zk7969pKenV1gNXVTNpdQIgF4yO0IIIYTD1bhmp2/fvvzrX/8iKiqK8ePHs3r1asu+oKAgCXRqwcWkgh0XCXaEEEIIh6txsPPBBx+QkpLC3LlzSUlJYejQobRo0YJXXnmF5ORkR7axyXEtC3ZcDV5ObokQQgjR9NVqNJaHhwd33303v/76K4cPH+buu+9m3rx5tGzZkmHDhvHFF184qp1NipsEO0IIIUS9qfXQc7O4uDimTZvGsWPHWLp0KVu2bGHcuHF12bYmy01TS2y4e0iwI4QQQjjaRa+NBbBmzRoWLFjAN998g6urKw888EBdtatJc9dUZsdNgh0hhBDC4Wod7CQnJ7Nw4UIWLlzIsWPHuOqqq3j33Xe57bbb8PSUgtuaMFCW2ZFuLCGEEMLhahzsLFmyhAULFrBmzRrCw8O55557uO+++2jdurUj29ckGbQi0IHBy9vZTRFCCCGavBoHO/feey833ngj3377LTfccAN6/UWX+1zWiouMuOlMAHh6+Di5NUIIIUTTV+Ng5+TJk4SFhTmyLZeFgoI83MoeG7ykG0sIIYRwtBqnZyTQqRvG/FzLY6nZEUIIIRxP+qLqmbEwH4BCzQ2ddAUKIYQQDidX23pWVKgWUTXqDE5uiRBCCHF5kGCnnpkzO0U6dye3RAghhLg81DrYOXHiBCdPnrQ8/+uvv5g8eTJz586t04Y1VSUS7AghhBD1qtbBzvjx41mzZg0AKSkpDBkyhL/++ovnnnuOV155pc4b2NQUG1WwUyzdWEIIIUS9qHWws2fPHvr06QPAF198QadOndi4cSNLlixh4cKFdd2+Jqe4LLNTopdgRwghhKgPtQ52iouLMRjUhXrVqlWMGjUKgHbt2nHmzJm6bV0TVFpUAEiwI4QQQtSXWgc7HTt25P333+f3339n5cqVXH/99QCcPn2a4ODgOm9gU2MqC3ZKXTyc3BIhhBDi8lDrYOeNN97ggw8+YNCgQYwbN46uXbsC8N1331m6t0TVTEWqG8vkIpkdIYQQoj7UOtgZNGgQaWlppKWlMX/+fMv2Bx98kPfff79OGwdw6tQp7rrrLoKDg/Hy8qJbt25s3brVsl/TNBISEoiKisLT05NBgwaRmJhY5+2oK6ZildnRJNgRQggh6kWtg52CggKMRiOBgYEAHD9+nLfeeosDBw7U+ZISGRkZXHnllbi5ufHzzz+zd+9e3nzzTQICAizHzJw5k1mzZjFnzhw2b95MREQEQ4YMIScnp07bUmeKCwHQXKUbSwghhKgPNV4I1Oymm25izJgxPPzww2RmZtK3b1/c3NxIS0tj1qxZPPLII3XWuDfeeIPo6GgWLFhg2daiRQvLY03TeOutt3j++ecZM2YMAIsWLSI8PJwlS5bw0EMP1Vlb6kxJWWbH1dPJDRFCCCEuD7XO7Gzbto2rrroKgK+++orw8HCOHz/O4sWLefvtt+u0cd999x29evXitttuIywsjO7du/Phhx9a9iclJZGSksLQoUMt2wwGAwMHDmTjxo1VntdoNJKdnW13qzdlmR0ksyOEEELUi1oHO/n5+fj6+gKwYsUKxowZg16v54orruD48eN12rijR4/y3nvvER8fz/Lly3n44Yd54oknWLx4MaAmNQQIDw+3e114eLhlX2VmzJiBv7+/5RYdHV2n7a6OrkQFOzo3yewIIYQQ9aHWwU7r1q359ttvOXHiBMuXL7dkVVJTU/Hz86vTxplMJnr06MH06dPp3r07Dz30EA888ADvvfee3XE6nc7uuaZpFbbZmjp1KllZWZbbiRMn6rTd1dGVGtW9uwQ7QgghRH2odbDz4osv8vTTT9OiRQv69OlDv379AJXl6d69e502LjIykg4dOthta9++PcnJyQBEREQAVMjipKamVsj22DIYDPj5+dnd6otLqWR2hBBCiPpU62Dn1ltvJTk5mS1btrB8+XLL9uuuu47Zs2fXaeOuvPJKDhw4YLft4MGDxMbGAhAXF0dERAQrV6607C8qKmLdunX079+/TttSV/RlmR1Xdy8nt0QIIYS4PNR6NBaojEpERAQnT55Ep9PRrFkzh0wo+OSTT9K/f3+mT5/O7bffzl9//cXcuXMtK6zrdDomT57M9OnTiY+PJz4+nunTp+Pl5cX48ePrvD11wdWkMjsuBsnsCCGEEPWh1pkdk8nEK6+8gr+/P7GxscTExBAQEMC0adMwmUx12rjevXuzbNkyPvvsMzp16sS0adN46623uPPOOy3HTJkyhcmTJzNp0iR69erFqVOnWLFihaWIuqFxNZVldiTYEUIIIepFrTM7zz//PPPmzeP111/nyiuvRNM0NmzYQEJCAoWFhbz22mt12sARI0YwYsSIKvfrdDoSEhJISEio0/d1FDetCAAXg3RjCSGEEPWh1sHOokWL+OijjyyrnQN07dqVZs2aMWnSpDoPdpoad5MRdODu4e3spgghhBCXhVp3Y6Wnp9OuXbsK29u1a0d6enqdNKopc0dldtwNEuwIIYQQ9aHWwU7Xrl2ZM2dOhe1z5syxrIAuKldcasJgDnY8pBtLCCGEqA+17saaOXMmN954I6tWraJfv37odDo2btzIiRMn+OmnnxzRxiajoLgUj7Jgx+AlwY4QQghRH2qd2Rk4cCAHDx7k5ptvJjMzk/T0dMaMGcOBAwcsa2aJyhUWWYMdNylQFkIIIerFRc2zExUVVaEQ+cSJE0ycOJH58+fXScOaosKiUsJ0KtjRyarnQgghRL2odWanKunp6SxatKiuTtckGYsKrE/cZNVzIYQQoj7UWbAjLqwoP9f6xE26sYQQQoj6IMFOPSrNTQMgD09wcXNya4QQQojLgwQ79ciUp4KdLL2/k1sihBBCXD5qXKA8ZsyYavdnZmZealuaPK0s2MmRYEcIIYSoNzUOdvz9q79A+/v7c88991xyg5oyXcF5AHJdApzbECGEEOIyUuNgZ8GCBY5sx2VBXxbs5LkGOLchQgghxGVEanbqkWtZsFPgFujklgghhBCXDwl26pFbYQYAhW4Bzm2IEEIIcRmRYKceuRepVeGNhiAnt0QIIYS4fEiwU48MZcFOsUG6sYQQQoj6IsFOPfIszgSg2EMyO0IIIUR9kWCnvmgaXsWqZqfUI9jJjRFCCCEuHxLs1JeiPNw0teK5yTPEyY0RQgghLh8S7NSXfDXsvFBzw9XD28mNEUIIIS4fEuzUl3y1VMR5/PBwr/FcjkIIIYS4RBLs1Jc8ldlJ13zxcJMfuxBCCFFf5KpbX8oyO+maHx6uLk5ujBBCCHH5kGCnvuTZdGO5SbAjhBBC1BcJduqLJbPji0G6sYQQQoh6I1fd+mJXsyOZHSGEEKK+SLBTX8yZHaRmRwghhKhPEuzUlzxrN5aMxhJCCCHqj1x164t5nh1NCpSFEEKI+iTBTj3R8tWK5+kyGksIIYSoVxLs1IcSIzpjNgDnpRtLCCGEqFdy1a0PZetiFWsuZOMtBcpCCCFEPZJgpz6UFSdn4Iu7iwt6vc7JDRJCCCEuHxLs1AdLcbJMKCiEEELUN7ny1oeCTACy8cYgXVhCCCFEvWrQwU5CQgI6nc7uFhERYdmvaRoJCQlERUXh6enJoEGDSExMdGKLq1BSCECBZpDiZCGEEKKeNfgrb8eOHTlz5ozltnv3bsu+mTNnMmvWLObMmcPmzZuJiIhgyJAh5OTkOLHFlSgLdgpxl2HnQgghRD1zdXYDLsTV1dUum2OmaRpvvfUWzz//PGPGjAFg0aJFhIeHs2TJEh566KEqz2k0GjEajZbn2dnZdd9wW8Uq2DHiJpkdIYQQop41+CvvoUOHiIqKIi4ujrFjx3L06FEAkpKSSElJYejQoZZjDQYDAwcOZOPGjdWec8aMGfj7+1tu0dHRDv0MlBQAUKi5y7BzIYQQop416GCnb9++LF68mOXLl/Phhx+SkpJC//79OX/+PCkpKQCEh4fbvSY8PNyyrypTp04lKyvLcjtx4oTDPgMAJSqLVIibdGMJIYQQ9axBd2MNHz7c8rhz587069ePVq1asWjRIq644goAdDr7OWs0TauwrTyDwYDBYKj7BlelWGV2jLhLN5YQQghRzxrVldfb25vOnTtz6NAhSx1P+SxOampqhWyP01kKlN0wSGZHCCGEqFeNKtgxGo3s27ePyMhI4uLiiIiIYOXKlZb9RUVFrFu3jv79+zuxlZUoC3aMUrMjhBBC1LsG3Y319NNPM3LkSGJiYkhNTeXVV18lOzubCRMmoNPpmDx5MtOnTyc+Pp74+HimT5+Ol5cX48ePd3bT7RVbMzvSjSWEEELUrwYd7Jw8eZJx48aRlpZGaGgoV1xxBZs2bSI2NhaAKVOmUFBQwKRJk8jIyKBv376sWLECX19fJ7e8HHNmR+bZEUIIIepdgw52li5dWu1+nU5HQkICCQkJ9dOgi1VinWfHXzI7QgghRL2SK299KJZ5doQQQghnkWCnPpTNs2OUeXaEEEKIeifBTn0wz6As8+wIIYQQ9U6uvPXBPIOy5i7z7AghhBD1TIKd+mC3EKgEO0IIIUR9kmCnPth2Y7nKj1wIIYSoT3LlrQ9SoCyEEEI4jQQ79cF26LkEO0IIIUS9kmDH0UylYCoGZDSWEEII4Qxy5XW0stmTQbqxhBBCCGeQYMfRim2DHZlBWQghhKhvEuw4Wllmp0hzwYReurGEEEKIeiZXXkcrC3YKcQeQSQWFEEKIeibBjqNZVjxXwY5kdoQQQoj6JVdeR7OZPVmnA3cX+ZELIYQQ9UmuvI5WYjPHjqsLOp3OyQ0SQgghLi8S7Dhaie26WPLjFkIIIeqbXH0drdhaoCxz7AghhBD1T4IdRzOPxpKlIoQQQginkGDH0Wy6sQyy4rkQQghR7+Tq62jmRUClG0sIIYRwCgl2HK3ECEhmRwghhHAWufo6mu3Qc8nsCCGEEPVOgh1HsxmN5S6ZHSGEEKLeydXX0aRAWQghhHAqufo6ms3aWAZX6cYSQggh6psEO45mDnY0Nwwyg7IQQghR7+Tq62i2NTuyCKgQQghR7+Tq62gl1mBHMjtCCCFE/ZOrr6PZdmNJzY4QQghR7yTYcTSbbiwZjSWEEELUP7n6OpoMPRdCCCGcSq6+jlYimR0hhBDCmeTq62jmhUClZkcIIYRwCgl2HM2yEKiMxhJCCCGcoVFdfWfMmIFOp2Py5MmWbZqmkZCQQFRUFJ6engwaNIjExETnNbI880Kg0o0lhBBCOEWjufpu3ryZuXPn0qVLF7vtM2fOZNasWcyZM4fNmzcTERHBkCFDyMnJcVJLyym2FijLQqBCCCFE/WsUV9/c3FzuvPNOPvzwQwIDAy3bNU3jrbfe4vnnn2fMmDF06tSJRYsWkZ+fz5IlS5zYYhvmAmVN1sYSQgghnKFRBDuPPvooN954I4MHD7bbnpSUREpKCkOHDrVsMxgMDBw4kI0bN1Z5PqPRSHZ2tt3NIUqLQSsFoFCGngshhBBO4ersBlzI0qVL2bZtG5s3b66wLyUlBYDw8HC77eHh4Rw/frzKc86YMYOXX365bhtambKsDsiq50IIIYSzNOhUw4kTJ/j73//OJ598goeHR5XH6XQ6u+eaplXYZmvq1KlkZWVZbidOnKizNtsptg12pGZHCCGEcIYGndnZunUrqamp9OzZ07KttLSU3377jTlz5nDgwAFAZXgiIyMtx6SmplbI9tgyGAwYDAbHNdysbCSWUXMDdNKNJYQQQjhBg776XnfddezevZsdO3ZYbr169eLOO+9kx44dtGzZkoiICFauXGl5TVFREevWraN///5ObHmZsjl2CnEHkHl2hBBCCCdo0JkdX19fOnXqZLfN29ub4OBgy/bJkyczffp04uPjiY+PZ/r06Xh5eTF+/HhnNNmeefZk3ACkZkcIIYRwggYd7NTElClTKCgoYNKkSWRkZNC3b19WrFiBr6+vs5tmnT1ZMwc7ktkRQggh6lujC3bWrl1r91yn05GQkEBCQoJT2lMtm9mTASlQFkIIIZxArr6OZDN7sl4HrvqqR4gJIYQQwjEk2HEk8+zJZXPsVDccXgghhBCOIcGOI9kuFSEjsYQQQginkCuwI5XYLALqIj9qIYQQwhnkCuxIxTbdWJLZEUIIIZxCrsCOZJ5BGTeZY0cIIYRwEgl2HMkyz467zLEjhBBCOIlcgR2p2DrPjsyxI4QQQjiHXIEdyZzZwU0yO0IIIYSTyBXYkcwzKGvuUrMjhBBCOIkEO47kH02afydOEyyZHSGEEMJJ5ArsSFc9xfd9PuHL0kEY3CSzI4QQQjiDBDsOZiwxAcikgkIIIYSTyBXYwYzFKtiRSQWFEEII55ArsIMZS0oBpGZHCCGEcBK5AjtYUVk3lozGEkIIIZxDgh0Hs9TsSGZHCCGEcAq5AjuYdGMJIYQQziVXYAczWrqx5EcthBBCOINcgR3MUrMj8+wIIYQQTiHBjoNZMjsyz44QQgjhFHIFdjBLzY7MsyOEEEI4hVyBHcwyqaDU7AghhBBOIVdgBzPKPDtCCCGEU0mw42BFMhpLCCGEcCq5AjuYuWZHJhUUQgghnEOuwA4m3VhCCCGEc0mw42CWYEdGYwkhhBBOIVdgB5OaHSGEEMK55ArsYFKzI4QQQjiXXIEdqNSkUVyqAVKzI4QQQjiLBDsOZO7CAunGEkIIIZxFrsAOJMGOEEII4XxyBXYgc72OXgeushCoEEII4RRyBXYgmWNHCCGEcD4JdhxIVjwXQgghnK9BX4Xfe+89unTpgp+fH35+fvTr14+ff/7Zsl/TNBISEoiKisLT05NBgwaRmJjoxBbbM8ocO0IIIYTTNeircPPmzXn99dfZsmULW7Zs4dprr+Wmm26yBDQzZ85k1qxZzJkzh82bNxMREcGQIUPIyclxcssV6cYSQgghnK9BBzsjR47khhtuoE2bNrRp04bXXnsNHx8fNm3ahKZpvPXWWzz//POMGTOGTp06sWjRIvLz81myZEm15zUajWRnZ9vdHMFYrIIdmVBQCCGEcJ5GcxUuLS1l6dKl5OXl0a9fP5KSkkhJSWHo0KGWYwwGAwMHDmTjxo3VnmvGjBn4+/tbbtHR0Q5ps6VmR4IdIYQQwmka/FV49+7d+Pj4YDAYePjhh1m2bBkdOnQgJSUFgPDwcLvjw8PDLfuqMnXqVLKysiy3EydOOKTtUrMjhBBCOJ+rsxtwIW3btmXHjh1kZmby9ddfM2HCBNatW2fZr9Pp7I7XNK3CtvIMBgMGg8Eh7bVVJDU7QgghhNM1+JSDu7s7rVu3plevXsyYMYOuXbvyf//3f0RERABUyOKkpqZWyPY4izmzIzU7QgghhPM0uquwpmkYjUbi4uKIiIhg5cqVln1FRUWsW7eO/v37O7GFVlKzI4QQQjhfg+7Geu655xg+fDjR0dHk5OSwdOlS1q5dyy+//IJOp2Py5MlMnz6d+Ph44uPjmT59Ol5eXowfP97ZTQeso7EMbtKNJYQQQjhLgw52zp49y913382ZM2fw9/enS5cu/PLLLwwZMgSAKVOmUFBQwKRJk8jIyKBv376sWLECX19fJ7dcKSqVAmUhhBDC2Rp0sDNv3rxq9+t0OhISEkhISKifBtWSzLMjhBBCOJ9chR1IanaEEEII55OrsAPJchFCCCGE80mw40BFMqmgEEII4XRyFXYgnU4FOgY3+TELIYQQzqLTNE1zdiOcLTs7G39/f7KysvDz83N2c4QQQghRAzW9fkvKQQghhBBNmgQ7QgghhGjSJNgRQgghRJMmwY4QQgghmjQJdoQQQgjRpEmwI4QQQogmTYIdIYQQQjRpEuwIIYQQokmTYEcIIYQQTZoEO0IIIYRo0iTYEUIIIUSTJsGOEEIIIZo0CXaEEEII0aRJsCOEEEKIJs3V2Q1oCDRNA9RS8UIIIYRoHMzXbfN1vCoS7AA5OTkAREdHO7klQgghhKitnJwc/P39q9yv0y4UDl0GTCYTp0+fxtfXF51OV2fnzc7OJjo6mhMnTuDn51dn521I5DM2fk3984F8xqagqX8+kM94MTRNIycnh6ioKPT6qitzJLMD6PV6mjdv7rDz+/n5NdlfXDP5jI1fU/98IJ+xKWjqnw/kM9ZWdRkdMylQFkIIIUSTJsGOEEIIIZo0CXYcyGAw8NJLL2EwGJzdFIeRz9j4NfXPB/IZm4Km/vlAPqMjSYGyEEIIIZo0yewIIYQQokmTYEcIIYQQTZoEO0IIIYRo0iTYEUIIIUSTJsGOA7377rvExcXh4eFBz549+f33353dpIsyY8YMevfuja+vL2FhYYwePZoDBw7YHXPvvfei0+nsbldccYWTWlx7CQkJFdofERFh2a9pGgkJCURFReHp6cmgQYNITEx0Yotrr0WLFhU+o06n49FHHwUa33f422+/MXLkSKKiotDpdHz77bd2+2vynRmNRh5//HFCQkLw9vZm1KhRnDx5sh4/RfWq+4zFxcU8++yzdO7cGW9vb6Kiorjnnns4ffq03TkGDRpU4XsdO3ZsPX+Sql3oe6zJ72VD/h4v9Pkq+5vU6XT8+9//thzTkL/DmlwfGsLfogQ7DvL5558zefJknn/+ebZv385VV13F8OHDSU5OdnbTam3dunU8+uijbNq0iZUrV1JSUsLQoUPJy8uzO+7666/nzJkzlttPP/3kpBZfnI4dO9q1f/fu3ZZ9M2fOZNasWcyZM4fNmzcTERHBkCFDLOuqNQabN2+2+3wrV64E4LbbbrMc05i+w7y8PLp27cqcOXMq3V+T72zy5MksW7aMpUuXsn79enJzcxkxYgSlpaX19TGqVd1nzM/PZ9u2bbzwwgts27aNb775hoMHDzJq1KgKxz7wwAN23+sHH3xQH82vkQt9j3Dh38uG/D1e6PPZfq4zZ84wf/58dDodt9xyi91xDfU7rMn1oUH8LWrCIfr06aM9/PDDdtvatWun/fOf/3RSi+pOamqqBmjr1q2zbJswYYJ20003Oa9Rl+ill17SunbtWuk+k8mkRUREaK+//rplW2Fhoebv76+9//779dTCuvf3v/9da9WqlWYymTRNa9zfIaAtW7bM8rwm31lmZqbm5uamLV261HLMqVOnNL1er/3yyy/11vaaKv8ZK/PXX39pgHb8+HHLtoEDB2p///vfHdu4OlLZZ7zQ72Vj+h5r8h3edNNN2rXXXmu3rTF9h+WvDw3lb1EyOw5QVFTE1q1bGTp0qN32oUOHsnHjRie1qu5kZWUBEBQUZLd97dq1hIWF0aZNGx544AFSU1Od0byLdujQIaKiooiLi2Ps2LEcPXoUgKSkJFJSUuy+T4PBwMCBAxvt91lUVMQnn3zCxIkT7Ra/bezfoVlNvrOtW7dSXFxsd0xUVBSdOnVqtN9rVlYWOp2OgIAAu+2ffvopISEhdOzYkaeffrpRZSSh+t/LpvQ9nj17lh9//JH77ruvwr7G8h2Wvz40lL9FWQjUAdLS0igtLSU8PNxue3h4OCkpKU5qVd3QNI2nnnqKAQMG0KlTJ8v24cOHc9tttxEbG0tSUhIvvPAC1157LVu3bm0Us4H27duXxYsX06ZNG86ePcurr75K//79SUxMtHxnlX2fx48fd0ZzL9m3335LZmYm9957r2VbY/8ObdXkO0tJScHd3Z3AwMAKxzTGv9PCwkL++c9/Mn78eLsFFu+8807i4uKIiIhgz549TJ06lZ07d1q6MRu6C/1eNqXvcdGiRfj6+jJmzBi77Y3lO6zs+tBQ/hYl2HEg2/8xg/pFKL+tsXnsscfYtWsX69evt9t+xx13WB536tSJXr16ERsby48//ljhD7chGj58uOVx586d6devH61atWLRokWWYsim9H3OmzeP4cOHExUVZdnW2L/DylzMd9YYv9fi4mLGjh2LyWTi3Xfftdv3wAMPWB536tSJ+Ph4evXqxbZt2+jRo0d9N7XWLvb3sjF+j/Pnz+fOO+/Ew8PDbntj+Q6ruj6A8/8WpRvLAUJCQnBxcakQkaamplaIbhuTxx9/nO+++441a9bQvHnzao+NjIwkNjaWQ4cO1VPr6pa3tzedO3fm0KFDllFZTeX7PH78OKtWreL++++v9rjG/B3W5DuLiIigqKiIjIyMKo9pDIqLi7n99ttJSkpi5cqVdlmdyvTo0QM3N7dG+b1Cxd/LpvI9/v777xw4cOCCf5fQML/Dqq4PDeVvUYIdB3B3d6dnz54VUowrV66kf//+TmrVxdM0jccee4xvvvmGX3/9lbi4uAu+5vz585w4cYLIyMh6aGHdMxqN7Nu3j8jISEv62Pb7LCoqYt26dY3y+1ywYAFhYWHceOON1R7XmL/DmnxnPXv2xM3Nze6YM2fOsGfPnkbzvZoDnUOHDrFq1SqCg4Mv+JrExESKi4sb5fcKFX8vm8L3CCrb2rNnT7p27XrBYxvSd3ih60OD+VuskzJnUcHSpUs1Nzc3bd68edrevXu1yZMna97e3tqxY8ec3bRae+SRRzR/f39t7dq12pkzZyy3/Px8TdM0LScnR/vHP/6hbdy4UUtKStLWrFmj9evXT2vWrJmWnZ3t5NbXzD/+8Q9t7dq12tGjR7VNmzZpI0aM0Hx9fS3f1+uvv675+/tr33zzjbZ7925t3LhxWmRkZKP5fGalpaVaTEyM9uyzz9ptb4zfYU5OjrZ9+3Zt+/btGqDNmjVL2759u2UkUk2+s4cfflhr3ry5tmrVKm3btm3atddeq3Xt2lUrKSlx1seyU91nLC4u1kaNGqU1b95c27Fjh93fptFo1DRN0w4fPqy9/PLL2ubNm7WkpCTtxx9/1Nq1a6d17969UXzGmv5eNuTv8UK/p5qmaVlZWZqXl5f23nvvVXh9Q/8OL3R90LSG8bcowY4D/fe//9ViY2M1d3d3rUePHnZDtRsToNLbggULNE3TtPz8fG3o0KFaaGio5ubmpsXExGgTJkzQkpOTndvwWrjjjju0yMhIzc3NTYuKitLGjBmjJSYmWvabTCbtpZde0iIiIjSDwaBdffXV2u7du53Y4ouzfPlyDdAOHDhgt70xfodr1qyp9PdywoQJmqbV7DsrKCjQHnvsMS0oKEjz9PTURowY0aA+c3WfMSkpqcq/zTVr1miapmnJycna1VdfrQUFBWnu7u5aq1attCeeeEI7f/68cz+Yjeo+Y01/Lxvy93ih31NN07QPPvhA8/T01DIzMyu8vqF/hxe6Pmhaw/hb1JU1VgghhBCiSZKaHSGEEEI0aRLsCCGEEKJJk2BHCCGEEE2aBDtCCCGEaNIk2BFCCCFEkybBjhBCCCGaNAl2hBBCCNGkSbAjhBBCiCZNgh0hxGVLp9Px7bffOrsZQggHk2BHCFHv7r33XnQ6XYXb9ddf7+ym1crmzZuJiooC4PTp03h6elJUVOTkVgkhynN1dgOEEJen66+/ngULFthtMxgMTmrNxfnjjz+48sorAfj999/p1asX7u7uTm6VEKI8yewIIZzCYDAQERFhdwsMDLTs1+l0vPfeewwfPhxPT0/i4uL48ssv7c6xe/durr32Wjw9PQkODubBBx8kNzfX7pj58+fTsWNHDAYDkZGRPPbYY3b709LSuPnmm/Hy8iI+Pp7vvvuuxp9h48aNlmBn/fr1lsdCiIZFgh0hRIP1wgsvcMstt7Bz507uuusuxo0bx759+wDIz8/n+uuvJzAwkM2bN/Pll1+yatUqu2Dmvffe49FHH+XBBx9k9+7dfPfdd7Ru3druPV5++WVuv/12du3axQ033MCdd95Jenp6lW1av349AQEBBAQE8NVXX/H8888TEBDA+++/z9tvv01AQACvv/66Y34gQoiLU2frpwshRA1NmDBBc3Fx0by9ve1ur7zyiuUYQHv44YftXte3b1/tkUce0TRN0+bOnasFBgZqubm5lv0//vijptfrtZSUFE3TNC0qKkp7/vnnq2wHoP3rX/+yPM/NzdV0Op32888/V/magoICLSkpSfv555+1wMBA7ejRo9qWLVs0d3d3bd++fVpSUpKWkZFRq5+HEMKxpGZHCOEU11xzDe+9957dtqCgILvn/fr1q/B8x44dAOzbt4+uXbvi7e1t2X/llVdiMpk4cOAAOp2O06dPc91111Xbji5dulgee3t74+vrS2pqapXHe3h40KJFC7744guGDx9OXFwcGzdu5KqrrqJdu3bVvpcQwjkk2BFCOIW3t3eFLqWa0Ol0AGiaZnlc2TGenp41Op+bm1uF15pMpiqP9/HxAcBoNKLX6/nf//5HUVERmqbh4+PDVVddxc8//1yj9xZC1A+p2RFCNFibNm2q8NycPenQoQM7duwgLy/Psn/Dhg3o9XratGmDr68vLVq0YPXq1XXaph07drBlyxZcXFxYvXo1O3bsIDg4mC+++IIdO3bw0Ucf1en7CSEunWR2hBBOYTQaSUlJsdvm6upKSEiI5fmXX35Jr169GDBgAJ9++il//fUX8+bNA+DOO+/kpZdeYsKECSQkJHDu3Dkef/xx7r77bsLDwwFISEjg4YcfJiwsjOHDh5OTk8OGDRt4/PHHL7rdrVu3ZtOmTYSHhzNgwACSk5PJyclhxIgRFbJEQoiGQYIdIYRT/PLLL0RGRtpta9u2Lfv377c8f/nll1m6dCmTJk0iIiKCTz/9lA4dOgDg5eXF8uXL+fvf/07v3r3x8vLilltuYdasWZbXT5gwgcLCQmbPns3TTz9NSEgIt9566yW3fe3atVx99dUArFu3jn79+kmgI0QDptM0TXN2I4QQojydTseyZcsYPXq0s5sihGjkpGZHCCGEEE2aBDtCCCGEaNKkZkcI0SBJD7sQoq5IZkcIIYQQTZoEO0IIIYRo0iTYEUIIIUSTJsGOEEIIIZo0CXaEEEII0aRJsCOEEEKIJk2CHSGEEEI0aRLsCCGEEKJJ+3+DpYxhi4Bg5AAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_acc_hist, label='Train Accuracy(%)')\n",
        "plt.plot(test_acc_hist,label='Test Accuracy(%)')\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss Value\")\n",
        "plt.legend()\n",
        "plt.savefig(\"result.png\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}